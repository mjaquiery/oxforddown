---
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    keep_tex: false
  bookdown::word_document2: default
  bookdown::html_document2: default
documentclass: book
editor_options: 
  chunk_output_type: inline
  
bibliography: 
  - references.bib
  - bibliography/references.bib
bibliography-heading-in-pdf: Works Cited
params:
  corrections: true 
---

```{r setup 01-04, echo = F, include = F}
source('scripts_and_filters/general_setup.R')
library(parallel)

```

# Modelling advice-taking behaviour {#chapter-advice-models}
\adjustmtc 

<!-- 
Modelling work on advice-taking/source selection

Descriptive modelling of data using multivariate regression
- ML/decision-tree approach instead; could use some python for this
- how good is an unweighted model that uses the biggest predictors?

Advisor choice/advisor preference/advice usage as endpoints
- advisor choice a softmax function of advisor preference
-->

First we put all the data into the workspace.

```{r gather all data}

select_experiment('datequiz')
AdvisedTrial <- full_join(
  AdvisedTrial,
  AdvisedTrialWithConf,
  by = names(AdvisedTrial)[names(AdvisedTrial) %in% names(AdvisedTrialWithConf)]
)

rm(advisors)

select_experiment('dotstask')

```

Next we grab the data from the binary tasks. 
This includes all the dotstask data and a small portion of the datequiz data.
\mccorrect{!TODO[Later we can try to join them using an interoperable measure of influence.]}

For what follows, we standardize initial estimate confidence and final decision confidence. 
The initial estimate confidence is standardized using its own mean and standard deviation. 
The final decision confidence is first oriented to the initial decision, meaning that it can take negative values where the endorsed answer switches from one side to the other.
Increasingly negative values represent increasingly confident endorsement of the answer endorsed in the final decision.
These final decision confidence values are then standardized using the mean and standard deviation from the initial estimate confidence. 
This means that the final decision confidence readings are z-scores based on the distribution for initial estimate confidence.
It also means some of the values are much higher or lower than would be expected from the initial estimate confidence distribution because the range of possible values is wider (because negative values are possible in the final decision confidence).

```{r extract binary task data}
df <- rbind(
  AdvisedTrial %>% 
    transmute(
      pid,
      number,
      feedback = if_else(is.na(feedback), feedback, F),
      advisor = advisor0idDescription,
      study = 'dates',
      confidenceIncrease = if_else(responseAnswerSide == responseAnswerSideFinal,
                                   responseConfidenceFinal - responseConfidence,
                                   responseConfidenceFinal + responseConfidence),
      zConfidenceIncrease = scale(confidenceIncrease),
      initialConfidenceFlat = responseConfidence,
      finalConfidenceFlat = if_else(
        responseAnswerSide == responseAnswerSideFinal,
        responseConfidenceFinal,
        -(responseConfidenceFinal + responseConfidence)
      ),
      zFinalConfidence = 
        (finalConfidenceFlat - mean(responseConfidence, na.rm = T)) /
        sd(responseConfidence, na.rm = T),
      initialAnswer = responseAnswerSide,
      # standardize initial confidence. We could use scale() but keep consistency with finalConfidence above.
      zInitialConfidence = 
        (responseConfidence - mean(responseConfidence, na.rm = T)) / 
        sd(responseConfidence, na.rm = T),
      advice = advisor0adviceSide
    ),
  trials %>% 
    transmute(
      pid,
      number = id,
      feedback,
      advisor = advisor0type,
      study = 'dots',
      confidenceIncrease = if_else(initialAnswer == finalAnswer,
                                   finalConfidence - initialConfidence,
                                   finalConfidence + initialConfidence),
      zConfidenceIncrease = scale(confidenceIncrease),
      initialConfidenceFlat = initialConfidence,
      finalConfidenceFlat = if_else(
        initialAnswer == finalAnswer,
        finalConfidence,
        -(finalConfidence + initialConfidence)
      ),
      zFinalConfidence = 
        (finalConfidenceFlat - mean(initialConfidence, na.rm = T)) /
        sd(initialConfidence, na.rm = T),
      initialAnswer,
      # standardize initial confidence. We could use scale() but keep consistency with finalConfidence above.
      zInitialConfidence = 
        (initialConfidence - mean(initialConfidence, na.rm = T)) / 
        sd(initialConfidence, na.rm = T),
      advice = adviceSide
    )
) %>%
  filter(across(.cols = everything(), .fns = ~ !is.na(.)))

df <- df %>%
  mutate(
    disagree = initialAnswer != advice,
    influence = if_else(disagree, zConfidenceIncrease, -zConfidenceIncrease),
    noise = rnorm(n())
  )

df %>% pivot_longer(cols = c('zInitialConfidence', 'zFinalConfidence')) %>%
  ggplot(aes(value, fill = name)) + 
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_density(alpha = .5) +
  facet_grid(feedback~disagree, labeller = label_both)

```
The distribution of advisor influence by agreement shows that final decision confidence in the initially-chosen answer is usually lower following disagreement from the advisor.
This makes sense, and is to be expected. 
There are also a reasonable number of trials on which disagreeing advice leads to a modest _increase_ in confidence.

Following agreement by an advisor, final decision confidence sometimes increases quite dramatically, but usually changes are modest and somewhat likely to be negative. 
Very dramatic negative shifts in confidence are rare following agreement.

<!-- Feedback effect may be participants being more cautious where evidence might appear that they are wrong -->


First, we run a simple multivariate linear regression predicting the final decision confidence of advice from initial estimate confidence and whether there is agreement between advice and initial estimate.

```{r multivariate linear regression}

library(lmerTest)

m <- lmer(zFinalConfidence ~ zInitialConfidence * disagree * feedback + (1|pid), data = df)
summary(m)

```

```{r}

#' Plot the parameters of a linear model in a neat way
forestPlot <- function(model) {
  tmp <- summary(model) %>%
    .$coefficients %>%
    as.data.frame() %>%
    rownames_to_column('Parameter') %>%
    mutate(sig = if_else(`Pr(>|t|)` < .05, '*', ''))
  
  low = -ceiling(max(abs(tmp$Estimate + tmp$`Std. Error`)) * 1.1)
  
  ggplot(tmp, aes(
    y = Parameter, 
    x = Estimate, 
    xmin = Estimate - `Std. Error`,
    xmax = Estimate + `Std. Error`,
    label = sig
  )) +
    geom_vline(xintercept = 0, linetype = 'dashed') +
    geom_point() +
    geom_errorbarh() +
    geom_text(x = low) +
    scale_x_continuous(limits = c(low, -low)) +
    labs(caption = 'DV = standardized directional final decision confidence; * p < .05')
}

forestPlot(m)

```

Where the advisor disagrees with a participant, the participant's final decision confidence is substantially lower on average than the average for initial estimate confidence.
There is also a small effect of initial confidence, indicating that a participant who is initially more confident is likely to retain that increased confidence in their final decision.
The interaction between the parameters indicates that the retention of initial estimate confidence is largely cancelled out by disagreement from an advisor: there is a small net positive effect from initial confidence on final confidence even where the advisor disagrees.

The presence of feedback on a trial is better regarded as a noisy proxy for feedback on previous trials than as contributing meaningfully to the final decision on any particular trial (because feedback happens _after_ the final decision has been made).
Feedback slightly reduces the confidence of final decisions, and noticeably reduces the final confidence penalty from disagreeing advice.
Slightly more initial confidence is retained into final decision confidence in feedback trials.

Next, we can look at the effect of adding in parameters based on the historical interactions between the advisor and the participant.
The two main effects we look at are the total amount of experience a participant has had with an advisor, and the agreement rate experienced in that time.

```{r advice history predictors}

dfh <- df %>%
  nest(d = -c(pid, study)) %>%
  mutate(d = map(d, ~ nest(., x = -advisor) %>%
                   mutate(
                     x = map(x, ~ arrange(., number) %>%
                               rowid_to_column(var = 'nthWithAdvisor'))
                   ) %>%
                   unnest(cols = x))) %>%
  unnest(cols = d) 

dfh <- dfh %>% mutate(agreeRate = NA)

for (n in 1:max(dfh$nthWithAdvisor)) {
  dfh$agreeRate[dfh$nthWithAdvisor == n] <- dfh %>% 
    filter(nthWithAdvisor <= n) %>%
    nest(d = c(-pid, -advisor)) %>%
    mutate(
      x = map_dbl(d, ~ mean(!.$disagree)),
      nth = map_int(d, ~ max(.$nthWithAdvisor))
    ) %>%
    filter(nth == n) %>%
    pull(x)
}

dfh <- dfh %>%
  mutate(
    nthWithAdvisor = scale(nthWithAdvisor)
  )

```

```{r models with history}

mh <- lmer(zFinalConfidence ~ zInitialConfidence * disagree * feedback * nthWithAdvisor * agreeRate + (1|pid), data = dfh)
summary(mh)

forestPlot(mh)

```

<!-- This suggests we trust advisors less when we have more agreement experience with them. Can we explore these further? Do we trust this analysis - can we visualise what it's doing? -->

```{r}

dfh %>% filter(nthWithAdvisor > -.9) %>%
  ggplot(aes(x = agreeRate, y = zFinalConfidence, colour = disagree)) +
  geom_point(alpha = .05) + 
  geom_smooth(method = 'lm') +
  facet_wrap(~study)

```

Absent any effects, average final decision confidence is slightly higher than average initial estimate confidence in this model.
As before, there is a pronounced effect of disagreement, with final confidence being notably lower following disagreement from the advisor, and a smaller effect whereby higher initial confidence increases final confidence.

The two history parameters, total experience with advisor and experienced agreement rate, go in opposite directions. 
The more experience a participant has had with an advisor, the higher their final confidence, while the greater the experienced agreement rate the lower the confidence. 
These effects are small, but puzzling; both advisor experience and experience of agreement should go the same way, and we might expect small positive effects (to be later reversed by interaction with disagreement).
The interaction between experience of agreement and disagreement is also peculiar: the reduction in confidence from disagreement is largely cancelled out in an advisor who agrees very frequently, suggesting that disagreeing advice from a advisor who usually agrees is less rather than more influential.
The history parameters also interact with one another: the greater the experience with an advisor the greater the penalty to final confidence from increased experience of agreement.

Overall, the largest effects in the model come from some quite complex 3- and 4-way interactions, suggesting a nuanced structure to the data which is not well-captured simple effects.
This may be because the biggest changes take place under specific conditions, or it may be an effect of the law of small numbers (because specific overlaps in conditions take place much more rarely than main effects, allowing outliers to produce large coefficient estimates).
Some comfort can be taken in noting that, while the standard errors of the complex interactions with large coefficients are larger than those with smaller coefficients, they are not dramatically larger, and thus the larger coefficients are very likely to be genuinely large.

```{r}
m <- lmer(zFinalConfidence ~ zInitialConfidence * disagree * feedback + (1 | pid), data = dfh)
anova(m, mh)
```

\mccorrect{Do we want to take a descriptive model like this and use it directly as a predictive model in the network modelling (or use machine learning/decision tree/heterogeneous models)? 
Can we massage continuous dates task data to be compatible?
Can we use a multilevel model with task[participant[trial]]] structure?}

* Which parameters are important for making network agents display human-like psychology?  
  * People choose in line with their experience (mostly)  
    * Feedback is a big deal  
    * Agreement less so?  
  * Lots of variability within and between people  
    * Individual differences are interesting - but what drives them/which dimensions matter?  
  * What effects do all these have on the network structure?  
    * Does heterogeneity lead to a stable network structure?  
    * Do extremely opinionated individuals form 'echo chambers'?  
      * Do they draw in others?  
  * Other parameters such as  
    * Choice availability  
    * Hidden preference  
      * How important is agree rate?  

* Explore some of the parameters in these linear models  
  * Visualise some of the raw data and the individual effects  

* Individual differences might be able to be tracked with parameter-fitted decision-tree-type choice models between pick/average  

## Models with agents based on our experiments

Each participant's data is run through a model which has the following structure:
* $P(pick_i) = \frac{e^{\rho V_i}}{e^{\rho Vi}+e^{\rho Vj}}$ softmax logistic compression where $V$ is the trust placed in an advisor and $_i$ and $_j$ are competing advisors and $\rho$ is the temperature parameter governing the consistency of picking based on differences in preference.
* $V_i^t = V_i^{t-1} + f(I^t,A^t) - \tau$ where $I^t$ and $A^t$ are the initial estimate and advice on trial $^t$ and $\tau$ is a decay parameter indicating how much trust decays at each time step.  
* $f(I, A, F) = \lambda(agree(I,A)\beta C)$ where $\lambda$ is the trust volatility, $\beta$ is the weight given to confidence, and $C$ is the confidence in the initial decision.  
* $\text{agree}(I, A) = -1 + 2(sign(I)=sign(A))$  
* $F^t = I^t + V_i^t A^t$ where $F^t$ is the final decision on trial $^t$.  

The final confidence judgement is scaled such that it lies between 0 (completely reversed decision) and 1 (completely sure), making its error roughly compatible with the error for picking. 
The picking and confidence errors are averaged to give the prediction error on that trial. 
All parameters ($\tau$, $\lambda$, $\beta$, and the $\rho$ temperature parameter) will be fit to minimise prediction error over the available trials.

```{r}

# Load up some dots task data. Like all of it. Do need to check the ones where 
select_experiment('dotstask')
trials <- trials %>% 
  mutate(uid = factor(paste0(studyId, studyVersion, ' p', pid)))


cacheFile <- '_cache/model-fit.rda'
if (!getOption('ESM.recalculate') & file.exists(cacheFile)) {
  load(cacheFile)
} else {
  # Let's have everyone run their data through a model where they are cumulatively 
  # updating their trust in an advisor and that is driving their p(pick) and also
  # their woa. 
  
  # Take a subset for testing stuff
  d <- trials %>% 
    nest(data = -uid) %>%
    mutate(okay = map_lgl(data, ~ any(.$hasChoice, na.rm = T))) %>%
    filter(okay) %>% slice_sample(n = 16)
  
  #' Return x scaled using \link{scale}, or 0 if all values in x are identical
  scale_or_zero <- function(x, ...) {
    y <- scale(x, ...)
    if (attr(y, 'scaled:scale') == 0)
      rep(0, length(x))
    else
      y
  }
  
  # Format how the C++ code wants the data:
  # * @param trials a data frame of trials with 5 columns (names may vary):
  # * initialConfidence.z - initial confidence rating (standardized within participant)
  # * advisorIndex - index of the advisor chosen (0 or 1; NA if no choice made)
  # * choice0 - index of the first advisor offered in the choice (NA if no choice offered)
  # * choice1 - index of the second advisor offered in the choice (NA if no choice offered)
  # * advisorAgrees - whether the chosen advisor agrees (NA if no advice provided)
  # * confidenceShift.z - amount confidence shifted from initial to final judgement (standardized within participant)
  d <- d %>%
    mutate(
      data = map(data, ~select(
        .,
        initialConfidence, 
        advisorId, 
        choice0,
        choice1,
        advisorAgrees, 
        advisorInfluenceRaw
      ) %>%
        mutate(
          initialConfidence.z = scale_or_zero(initialConfidence),
          advisorIndex = as.numeric(factor(advisorId)) - 1, # C likes 0-indexing
          across(
            .cols = matches('choice[01]'), 
            ~ as.numeric(factor(., levels = levels(factor(advisorId)))) -1
          ),
          confidenceShift.z = scale_or_zero(advisorInfluenceRaw)
        ) %>% 
        select(
          initialConfidence.z,
          advisorIndex,
          choice0, 
          choice1, 
          advisorAgrees, 
          confidenceShift.z
        ))
    )
  
  #' Perform gradient descent on a list of participants' data
  doGradDesc <- function(x) {
    library(dplyr)
    Rcpp::sourceCpp('scripts_and_filters/cpp/model-fit.cpp')
    x %>% mutate(data = purrr::map(
      data, 
      ~gradientDescent(., nStartingLocations = 150L))
    )
  }
  
  # For speed we now divvy up the data into roughly equal chunks based on the 
  # number of cores available and run the model fitting in parallel on each.
  dList <- list()
  nCores <- detectCores() - 4
  nPerCore <- floor(nrow(d) / nCores)
  for (i in 1:nCores) {
    rows <- (1 + (i - 1) * nPerCore):(i * nPerCore)
    dList[[i]] <- d[rows, ] 
  }
  # Assign remainder
  for (i in 1:(nrow(d) %% nCores)) {
    dList[[i]] <- rbind(dList[[i]], d[nrow(d) - (i - 1), ])
  }
  
  cl <- makeCluster(nCores)
  clusterExport(cl, "doGradDesc")
  dList <- parLapply(cl, dList, doGradDesc)
  stopCluster(cl)
  
  # rebuild d from dList
  d <- NULL
  for (x in dList)
    d <- rbind(d, x)
  
  # Neaten up the data
  d <- d %>%
    mutate(
      MSE = map(data, ~ as_tibble(.[['MSE']])),
      parameters = map(data, ~ as_tibble(.[['parameters']])),
      data = map(data, ~ as_tibble(.[['trials']]))
    )
  
  # Write to the cache 
  save(d, file = cacheFile)
}

```

The model with the most parameters should have the lowest MSE because it should be able to (over?)fit the data better.

```{r}

MSE <- d %>% 
  unnest(cols = MSE) %>%
  pivot_longer(
    cols = starts_with('Model'), 
    names_to = 'model', 
    names_pattern = 'Model(.)', 
    values_to = 'MSE'
  ) %>% 
  arrange(desc(MSE))

MSE %>% 
  ggplot(aes(x = model, y = MSE)) +
  geom_line(aes(group = uid), alpha = .05) +
  stat_summary(geom = 'point', aes(group = 1), size = 3, fun = mean) +
  stat_summary(geom = 'errorbar', aes(group = 1), fun.data = mean_cl_normal, width = 0, colour = 'red') +
  scale_y_continuous(limits = c(0, 5))

```

We would like to know the C++ models are doing sensible things, so let's pick a participant and track their advisor preferences and how they relate to their advisor choices in the best-fitting version of the model.

```{r}

gradientDescent(trials = d[d$uid == MSE$uid[2], ]$data[[1]], nStartingLocations = 150L)$MSE

tmp <- MSE[MSE$uid == MSE$uid[1], ] %>%
  mutate(
    model = as.numeric(model),
    parameters = map2(parameters, model, ~as_tibble(.x) %>% 
                        filter(model == .y) %>%
                        select(-model)),
    data = map2(data, model, ~pivot_longer(
      .,
      cols = matches('_model[0-9]+$'),
      names_to = c('.value', 'model'),
      names_pattern = '(.+)_model([0-9]+)'
    ) %>%
      filter(model == .y) %>%
      rowid_to_column(var = "trial_id") %>%
      select(-model))
  ) %>% 
  unnest(cols = parameters)

tmp

tmp <- tmp %>% 
  unnest(cols = data) %>%
  pivot_longer(
    cols = matches('^advisorTrust[0-9]+$'),
    names_to = 'Advisor',
    names_pattern = '([0-9]+)$',
    values_to = 'Trust'
  ) %>%
  filter(Advisor != 0)

for (a in unique(tmp$Advisor)) {
  x <- tmp %>% filter(advisorIndex == a, Advisor == a)
  gg <- x %>% ggplot(aes(x = trial_id, y = Trust))
  if (!all(is.na(x$choice0))) 
    gg <- gg + geom_rect(aes(xmin = min(trial_id), xmax = max(trial_id), 
                             ymin = -Inf, ymax = Inf),
                         data = filter(x, !is.na(choice0)), fill = 'grey85')
  gg <- gg + geom_line() +
    geom_point(aes(colour = advisorAgrees)) +
    facet_grid(paste0('M', model)~., scales = 'free_y') +
    labs(title = paste0('Advisor ', a))
  print(gg)
}
```
