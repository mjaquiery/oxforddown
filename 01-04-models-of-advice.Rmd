---
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    keep_tex: false
  bookdown::word_document2: default
  bookdown::html_document2: default
documentclass: book
editor_options: 
  chunk_output_type: inline
  
bibliography: 
  - references.bib
  - bibliography/references.bib
bibliography-heading-in-pdf: Works Cited
params:
  corrections: true 
---

```{r setup 01-04, echo = F, include = F}
source('scripts_and_filters/general_setup.R')
library(parallel)

```

# Modelling advice-taking behaviour {#chapter-advice-models}
\adjustmtc 

<!-- 
Modelling work on advice-taking/source selection

Descriptive modelling of data using multivariate regression
- ML/decision-tree approach instead; could use some python for this
- how good is an unweighted model that uses the biggest predictors?

Advisor choice/advisor preference/advice usage as endpoints
- advisor choice a softmax function of advisor preference
-->

First we put all the data into the workspace.

```{r gather all data}

select_experiment('datequiz')
AdvisedTrial <- full_join(
  AdvisedTrial,
  AdvisedTrialWithConf,
  by = names(AdvisedTrial)[names(AdvisedTrial) %in% names(AdvisedTrialWithConf)]
)

rm(advisors)

select_experiment('dotstask')

```

Next we grab the data from the binary tasks. 
This includes all the dotstask data and a small portion of the datequiz data.
\mccorrect{!TODO[Later we can try to join them using an interoperable measure of influence.]}

For what follows, we standardize initial estimate confidence and final decision confidence. 
The initial estimate confidence is standardized using its own mean and standard deviation. 
The final decision confidence is first oriented to the initial decision, meaning that it can take negative values where the endorsed answer switches from one side to the other.
Increasingly negative values represent increasingly confident endorsement of the answer endorsed in the final decision.
These final decision confidence values are then standardized using the mean and standard deviation from the initial estimate confidence. 
This means that the final decision confidence readings are z-scores based on the distribution for initial estimate confidence.
It also means some of the values are much higher or lower than would be expected from the initial estimate confidence distribution because the range of possible values is wider (because negative values are possible in the final decision confidence).

```{r extract binary task data}
df <- rbind(
  AdvisedTrial %>% 
    transmute(
      pid,
      number,
      feedback = if_else(is.na(feedback), feedback, F),
      advisor = advisor0idDescription,
      study = 'dates',
      confidenceIncrease = if_else(responseAnswerSide == responseAnswerSideFinal,
                                   responseConfidenceFinal - responseConfidence,
                                   responseConfidenceFinal + responseConfidence),
      zConfidenceIncrease = scale(confidenceIncrease),
      initialConfidenceFlat = responseConfidence,
      finalConfidenceFlat = if_else(
        responseAnswerSide == responseAnswerSideFinal,
        responseConfidenceFinal,
        -(responseConfidenceFinal + responseConfidence)
      ),
      zFinalConfidence = 
        (finalConfidenceFlat - mean(responseConfidence, na.rm = T)) /
        sd(responseConfidence, na.rm = T),
      initialAnswer = responseAnswerSide,
      # standardize initial confidence. We could use scale() but keep consistency with finalConfidence above.
      zInitialConfidence = 
        (responseConfidence - mean(responseConfidence, na.rm = T)) / 
        sd(responseConfidence, na.rm = T),
      advice = advisor0adviceSide
    ),
  trials %>% 
    transmute(
      pid,
      number = id,
      feedback,
      advisor = advisor0type,
      study = 'dots',
      confidenceIncrease = if_else(initialAnswer == finalAnswer,
                                   finalConfidence - initialConfidence,
                                   finalConfidence + initialConfidence),
      zConfidenceIncrease = scale(confidenceIncrease),
      initialConfidenceFlat = initialConfidence,
      finalConfidenceFlat = if_else(
        initialAnswer == finalAnswer,
        finalConfidence,
        -(finalConfidence + initialConfidence)
      ),
      zFinalConfidence = 
        (finalConfidenceFlat - mean(initialConfidence, na.rm = T)) /
        sd(initialConfidence, na.rm = T),
      initialAnswer,
      # standardize initial confidence. We could use scale() but keep consistency with finalConfidence above.
      zInitialConfidence = 
        (initialConfidence - mean(initialConfidence, na.rm = T)) / 
        sd(initialConfidence, na.rm = T),
      advice = adviceSide
    )
) %>%
  filter(across(.cols = everything(), .fns = ~ !is.na(.)))

df <- df %>%
  mutate(
    disagree = initialAnswer != advice,
    influence = if_else(disagree, zConfidenceIncrease, -zConfidenceIncrease),
    noise = rnorm(n())
  )

df %>% pivot_longer(cols = c('zInitialConfidence', 'zFinalConfidence')) %>%
  ggplot(aes(value, fill = name)) + 
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_density(alpha = .5) +
  facet_grid(feedback~disagree, labeller = label_both)

```
The distribution of advisor influence by agreement shows that final decision confidence in the initially-chosen answer is usually lower following disagreement from the advisor.
This makes sense, and is to be expected. 
There are also a reasonable number of trials on which disagreeing advice leads to a modest _increase_ in confidence.

Following agreement by an advisor, final decision confidence sometimes increases quite dramatically, but usually changes are modest and somewhat likely to be negative. 
Very dramatic negative shifts in confidence are rare following agreement.

<!-- Feedback effect may be participants being more cautious where evidence might appear that they are wrong -->


First, we run a simple multivariate linear regression predicting the final decision confidence of advice from initial estimate confidence and whether there is agreement between advice and initial estimate.

```{r multivariate linear regression}

library(lmerTest)

m <- lmer(zFinalConfidence ~ zInitialConfidence * disagree * feedback + (1|pid), data = df)
summary(m)

```

```{r}

#' Plot the parameters of a linear model in a neat way
forestPlot <- function(model) {
  tmp <- summary(model) %>%
    .$coefficients %>%
    as.data.frame() %>%
    rownames_to_column('Parameter') %>%
    mutate(sig = if_else(`Pr(>|t|)` < .05, '*', ''))
  
  low = -ceiling(max(abs(tmp$Estimate + tmp$`Std. Error`)) * 1.1)
  
  ggplot(tmp, aes(
    y = Parameter, 
    x = Estimate, 
    xmin = Estimate - `Std. Error`,
    xmax = Estimate + `Std. Error`,
    label = sig
  )) +
    geom_vline(xintercept = 0, linetype = 'dashed') +
    geom_point() +
    geom_errorbarh() +
    geom_text(x = low) +
    scale_x_continuous(limits = c(low, -low)) +
    labs(caption = 'DV = standardized directional final decision confidence; * p < .05')
}

forestPlot(m)

```

Where the advisor disagrees with a participant, the participant's final decision confidence is substantially lower on average than the average for initial estimate confidence.
There is also a small effect of initial confidence, indicating that a participant who is initially more confident is likely to retain that increased confidence in their final decision.
The interaction between the parameters indicates that the retention of initial estimate confidence is largely cancelled out by disagreement from an advisor: there is a small net positive effect from initial confidence on final confidence even where the advisor disagrees.

The presence of feedback on a trial is better regarded as a noisy proxy for feedback on previous trials than as contributing meaningfully to the final decision on any particular trial (because feedback happens _after_ the final decision has been made).
Feedback slightly reduces the confidence of final decisions, and noticeably reduces the final confidence penalty from disagreeing advice.
Slightly more initial confidence is retained into final decision confidence in feedback trials.

Next, we can look at the effect of adding in parameters based on the historical interactions between the advisor and the participant.
The two main effects we look at are the total amount of experience a participant has had with an advisor, and the agreement rate experienced in that time.

```{r advice history predictors}

dfh <- df %>%
  nest(d = -c(pid, study)) %>%
  mutate(d = map(d, ~ nest(., x = -advisor) %>%
                   mutate(
                     x = map(x, ~ arrange(., number) %>%
                               rowid_to_column(var = 'nthWithAdvisor'))
                   ) %>%
                   unnest(cols = x))) %>%
  unnest(cols = d) 

dfh <- dfh %>% mutate(agreeRate = NA)

for (n in 1:max(dfh$nthWithAdvisor)) {
  dfh$agreeRate[dfh$nthWithAdvisor == n] <- dfh %>% 
    filter(nthWithAdvisor <= n) %>%
    nest(d = c(-pid, -advisor)) %>%
    mutate(
      x = map_dbl(d, ~ mean(!.$disagree)),
      nth = map_int(d, ~ max(.$nthWithAdvisor))
    ) %>%
    filter(nth == n) %>%
    pull(x)
}

dfh <- dfh %>%
  mutate(
    nthWithAdvisor = scale(nthWithAdvisor)
  )

```

```{r models with history}

mh <- lmer(zFinalConfidence ~ zInitialConfidence * disagree * feedback * nthWithAdvisor * agreeRate + (1|pid), data = dfh)
summary(mh)

forestPlot(mh)

```

<!-- This suggests we trust advisors less when we have more agreement experience with them. Can we explore these further? Do we trust this analysis - can we visualise what it's doing? -->

```{r plot models}

dfh %>% filter(nthWithAdvisor > -.9) %>%
  ggplot(aes(x = agreeRate, y = zFinalConfidence, colour = disagree)) +
  geom_point(alpha = .05) + 
  geom_smooth(method = 'lm') +
  facet_wrap(~study)

```

Absent any effects, average final decision confidence is slightly higher than average initial estimate confidence in this model.
As before, there is a pronounced effect of disagreement, with final confidence being notably lower following disagreement from the advisor, and a smaller effect whereby higher initial confidence increases final confidence.

The two history parameters, total experience with advisor and experienced agreement rate, go in opposite directions. 
The more experience a participant has had with an advisor, the higher their final confidence, while the greater the experienced agreement rate the lower the confidence. 
These effects are small, but puzzling; both advisor experience and experience of agreement should go the same way, and we might expect small positive effects (to be later reversed by interaction with disagreement).
The interaction between experience of agreement and disagreement is also peculiar: the reduction in confidence from disagreement is largely cancelled out in an advisor who agrees very frequently, suggesting that disagreeing advice from a advisor who usually agrees is less rather than more influential.
The history parameters also interact with one another: the greater the experience with an advisor the greater the penalty to final confidence from increased experience of agreement.

Overall, the largest effects in the model come from some quite complex 3- and 4-way interactions, suggesting a nuanced structure to the data which is not well-captured simple effects.
This may be because the biggest changes take place under specific conditions, or it may be an effect of the law of small numbers (because specific overlaps in conditions take place much more rarely than main effects, allowing outliers to produce large coefficient estimates).
Some comfort can be taken in noting that, while the standard errors of the complex interactions with large coefficients are larger than those with smaller coefficients, they are not dramatically larger, and thus the larger coefficients are very likely to be genuinely large.

```{r models anova}
m <- lmer(zFinalConfidence ~ zInitialConfidence * disagree * feedback + (1 | pid), data = dfh)
anova(m, mh)
```

\mccorrect{Do we want to take a descriptive model like this and use it directly as a predictive model in the network modelling (or use machine learning/decision tree/heterogeneous models)? 
Can we massage continuous dates task data to be compatible?
Can we use a multilevel model with task[participant[trial]]] structure?}

* Which parameters are important for making network agents display human-like psychology?  
  * People choose in line with their experience (mostly)  
    * Feedback is a big deal  
    * Agreement less so?  
  * Lots of variability within and between people  
    * Individual differences are interesting - but what drives them/which dimensions matter?  
  * What effects do all these have on the network structure?  
    * Does heterogeneity lead to a stable network structure?  
    * Do extremely opinionated individuals form 'echo chambers'?  
      * Do they draw in others?  
  * Other parameters such as  
    * Choice availability  
    * Hidden preference  
      * How important is agree rate?  

* Explore some of the parameters in these linear models  
  * Visualise some of the raw data and the individual effects  

* Individual differences might be able to be tracked with parameter-fitted decision-tree-type choice models between pick/average  

## Models with agents based on our experiments

Each participant's data is run through a model which has the following structure:
* $1/(1 + e^{(-(V_i - V_j) * \rho)})$ softmax logistic compression where $V$ is the trust placed in an advisor, $_i$ and $_j$ are competing advisors and $\rho$ is the free parameter governing the consistency of picking based on differences in preference.
* $V_i^t = (1-\lambda) \cdot V_i^{t-1} + \lambda \cdot agree(I^t, A^t)$ where $I^t$ and $A^t$ are the initial estimate and advice on trial $^t$ and $\lambda$ is the free parameter governing the model's trust update rate. $\text{agree}(I, A)$ is 1 if $I$ and $A$ indicate the same decision, 0 otherwise.

The model error is calculated on each trial. 
Final confidence judgements are calculated on current trust in the active advisor, according to a Bayesian update wherein the initial confidence is used as a marker of the subjective probability that the initial decision direction is correct. 
The trust in the advisor is taken to signify the advisor's reliability - the probability the advisor indicates the same decision direction given that decision direction was correct.
Thus, Bayes theorem yields $F = \frac{I T}{IT + (1-I)(1-T)}$, where $I$ and $F$ are the initial and final decision probabilities (scaled confidences), and T is the trust in the advisor.
The difference between the predicted final confidence and the actual final confidence is the advice taking error on that trial.

The advisor choice error is also calculated, for trials where a choice of advisor was offered.
Advisor choice error is calculated over the last (up to) 5 trials where the currently chosen advisor on the current trial was one of the choices offered.
For each of these trials, the model takes the average probability of picking the currently chosen advisor versus the other choice, and compares this figure to the average pick rate of the currently chosen advisor. 
The difference between these two numbers is the advisor choice error on that trial.

Both advisor choice and advice influence errors lie on a scale between 0 and 1, with 0 being a perfect match to the data and 1 being entirely wrong. 
To account for different distributions of errors across each dimension, the overall model error is calculated by normalising each error dimension separately and then summing them.

\mccorrect{!TODO[Should we scale participant confidence values such that their lowest confidence is always treated as completely unsure and their highest reported confidence always treated as the top of the scale? It would keep model predictions on a similar scale to answers, but it would also make the maths harder and impose artificial limits other than the ones actually introduced by the scale. We probably shouldn't do this but should explain why we didn't - it would distort participants' confidence judgements.]}

```{r model fit calculations}

#' This parameter estimation process takes a long time, and takes even longer to
#' do the checking via shuffling 9 shuffles of each participants' data.
#' To make this tractable, the process was run on the Advanced Research 
#' Computing high performance cluster. The loaded file is the result, and the
#' code run can be inspected at
#' https://github.com/oxacclab/arc_thesis-parameter-estimation

load('cache/thesis-parameter-estimation.rda')

```

### Sanity checking by shuffling agreement

The models key on the advisor agreement. 
This means that if we shuffle the advisor agreement the model's ability to fit the data collapses (provided the model's parameters are kept fixed).

```{r visualise shuffles}

scaledError <- function(err1, err2) {
  x <- scale(err1); y <- scale(err2)
  x <- x - min(x); y <- y - min(y)
  log(x + y)
}

positions <- bind_rows(
  recovered_parameters %>%
    rename(weightedSelection = ws_end, trustUpdateRate = tu_end) %>%
    mutate(shuffle_run = 0),
  shuffles
) %>%
  select(-data) %>%
  nest(data = -uid) %>%
  mutate(
    data = map(data, ~ mutate(
      .,
      combined_error = scaledError(ws_error, tu_error)
    ) %>% arrange(combined_error)),
    pos = map_int(data, ~ rowid_to_column(., 'pos') %>% 
                    filter(shuffle_run == 0) %>%
                    pull(pos) %>% 
                    .[1]
    ),
    combined_error = map_dbl(data, ~ filter(., shuffle_run == 0) %>%
                               pull(combined_error) %>% 
                               .[1]),
    tu_error = map_dbl(data, ~ filter(., shuffle_run == 0) %>% 
                         pull(tu_error) %>%
                         .[1]),
    ws_error = map_dbl(data, ~ filter(., shuffle_run == 0) %>% 
                         pull(ws_error) %>%
                         .[1]),
    sum_error = tu_error + ws_error,
    studyId = str_match(uid, '^(.+) p[0-9]+$')[,2]
  )

ggplot(positions, aes(pos)) + 
  geom_histogram(binwidth = 1) +
  scale_x_continuous(breaks = 1:10, limits = c(NA, 10)) + 
  facet_wrap(~studyId)

positions %>% 
  pivot_longer(
    c(tu_error, ws_error, sum_error), 
    names_pattern = '(.+)_error',
    names_to = 'ErrorDimension',
    values_to = 'Error'
  ) %>%
  ggplot(aes(x = pos, y = Error)) +
  geom_point(alpha = .5) +
  geom_smooth(method = 'lm', formula = y ~ x) +
  scale_x_continuous(breaks = 1:10, limits = c(NA, 10)) +
  facet_wrap(~ErrorDimension)

cor.test(positions$pos, positions$tu_error)
cor.test(positions$pos, positions$ws_error)
cor.test(positions$pos, positions$sum_error)
```

The summaries look reasonably sensible.
The experiments which yielded the best results have participants whose data is better fit by the real data than by data with the advisor agreement shuffled.
Those experiments where there were issues with how agreement was coded or recorded do not show this pattern, nor is the pattern clearly visible in experiments where we do not believe the advisors were clearly differentiated to the participants.

There is a clear pattern for participants' datasets with better scramble positions to have lower overall mean squared error, at least for the weighted selection parameter.

```{r best fit distributions}

coefs <- positions %>% 
  select(uid, data, pos) %>%
  mutate(
    data = map(data, ~ filter(., shuffle_run == 0) %>% .[1, ])
  ) %>%
  unnest(cols = data) %>%
  select(uid, pos, weightedSelection, trustUpdateRate)

ggplot(coefs, aes(weightedSelection, fill = pos < 3)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_histogram(binwidth = 1, alpha = .1, fill = 'black') +
  geom_histogram(binwidth = 1, alpha = .5, position = 'identity') +
  scale_x_continuous(limits = c(-50, 50))

ggplot(coefs, aes(trustUpdateRate, fill = pos < 3)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_histogram(binwidth = .005, alpha = .1, fill = 'black') +
  geom_histogram(binwidth = .005, alpha = .5, position = 'identity')

```

The distribution of parameters is fairly similar between those datasets with good (1-2) positions in the shuffled list and those with bad (>=3) positions.
In both cases, the mean trust update parameter is just above 0 (overall mean = `r round(mean(coefs$trustUpdateRate), 3)`), as is the weighted selection parameter (overall mean = `r  round(mean(coefs$weightedSelection), 2)`). 
The ranges of the parameters are sensible: trust updates are small, while weighted selection is larger.
The normal distribution of coefficients is expected, but the number of cases where the parameters are estimated to be below 0 is not. 

Negative values for weighted selection are somewhat understandable: they indicate that the participant seeks out disagreeing advice preferentially, which is a sound strategy in many of the experimental designs. 
Negative values of trust update rate are more concerning - they indicate that the advisor becomes more trusted (more influential and more likely to be picked given a positive weighted selection coefficient) following disagreement.


#### Verification of model fitting validity

Verification of this model fitting approach was conducted on the Advanced Research Cluster \mccorrect{!TODO[CITATION]} in two ways. 
Firstly, the coefficients derived for each participant were fitted to versions of that participant's data where the advisor agreement column values had been shuffled. 
Overwhelmingly, the fitting error on the shuffled data tended to be higher than the fit to the original data.
Secondly, the fitting error for coefficients based on the participants' data were compared to fitting errors for coefficients fitted to shuffled data. 
Error values for the shuffled data tended to be higher than values for the original data, suggesting that participants' behaviour shared some features with the model's expectations.

