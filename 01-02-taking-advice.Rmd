---
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
  bookdown::word_document2: default
  bookdown::html_document2: default
documentclass: book

bibliography: 
  - references.bib
  - bibliography/references.bib
bibliography-heading-in-pdf: Works Cited
params:
  corrections: true 
---

```{r setup 01-02, echo = F, include = F}
source('scripts_and_filters/general_setup.R')
library(BayesFactor)
library(ez)
library(magrittr) # for %$% exploding pipe operator
```

# Psychology of advice-taking {#chapter-advice-taking}
\adjustmtc 

<!-- 
Here should be a detailed introduction.
Niccolo's stuff about advice-taking, and then demonstrate similar results in my Dates Task.
-->

## \mccorrect{!TODO[Cite Niccolo's best paper on his thesis advice-use experiments]}

\mccorrect{!TODO[Describe Niccolo's stuff, especially why accuracy vs agreement might be a good target for replication with a new experimental design.]}

## Extending results to the Dates task

### Introduction {#ai-ava-i}

\mccorrect{!TODO[Why the dates task? Generalising to a new kind of task. Quicker, more realistic in some ways, more fun]}

\mccorrect{!TODO[Why agreeing vs accurate advisors? It's a distillation of the core insight from Niccolo's work.]}

```{r ai-ava-r-load-data, include = F}

rm(list = ls()); source('scripts_and_filters/general_setup.R')

# Load the study data
select_experiment(
  project = 'datequiz',
  function(x) filter(x, study == 'datesStudy', version == 'v1-0-1')
)


.dropEnv <- new.env()
tada('datequiz', package = 'esmData', envir = .dropEnv)
.dropEnv$datequiz <- .dropEnv$datequiz %>% 
  filter(study == 'datesStudy', version != 'v1-0-1')

.dropEnv$DROP_V <- paste0(unique(.dropEnv$datequiz$version), collapse = ', ')
.dropEnv$DROP_P <- sum(.dropEnv$datequiz %>% 
                         filter(table == 'AdvisedTrial') %>% 
                         pull(N))

# Version of which the analysed version is a replication
.prevStudy <- new.env()
select_experiment(
  'datequiz', 
  function(x) filter(x, study == 'datesStudy', version == 'v0-0-21'), 
  envir = .prevStudy
)

for (E in c(.GlobalEnv, .prevStudy)) {
  E$AdvisedTrial <- annotate_responses(E$AdvisedTrial)
  E$Trial <- annotate_responses(E$Trial)
}

```

#### Open scholarship practices

\indent\indent\OpenScience{prereg} `r unique(datequiz$preregistration)`

\OpenScience{data} \mccorrect{!TODO[OSFify data for these studies]}

\OpenScience{materials} https://github.com/oxacclab/ExploringSocialMetacognition/blob/f90b6f9266a901211a4ddb7b5ee1de1c74e8df57/ACv2/index.html

##### Unanalysed data

This branch of experiments was the core of the date task. 
As such, most experimentation and piloting happened on this branch, meaning that there were many versions of the study where data were collected for which analysis is not included here (`r .dropEnv$DROP_V`). 
\mccorrect{!TODO[Explain the non-analysed versions.]}
Overall, data were collected from `r .dropEnv$DROP_P` participants across all of these versions.

The study reported here is a preregistered replication of a pilot study (v0-0-21) which produced the same results.
Data, analysis script, and analysis for the pilot study are also available at \mccorrect{!TODO[OSFify pilot data/analysis stuff]}.

#### Method {#ai-ava-m}

This study used the continuous version of the Dates Task ([§Estimation (Dates Task) - Continuous](#m-p-dates-c)).

#### Results {#ai-ava-r}

##### Exclusions

```{r ai-ava-r-exclusions}
nMaxOutliers <- 2
zThresh <- 3

for (E in c(.GlobalEnv, .prevStudy)) {
  E$exclusions <- tibble(pid = unique(E$AdvisedTrial$pid)) %>%
    mutate(
      `Attention check` = map_lgl(pid, ~ 
                                    !all(E$Trial$responseCorrect[E$Trial$pid == .]) |
                                    any(E$Trial$responseMarkerWidth[E$Trial$pid == .] != 1)
      ),
      `Unfinished` = map_lgl(pid, ~ !(. %in% E$`debrief-advisors`$pid))
    )
  
  do_exclusions(E$exclusions, envir = E)
  
  E$nMaxTrials <- E$AdvisedTrial %>% 
    group_by(pid) %>% 
    summarise(n = n(), .groups = 'drop') %>% 
    pull(n) %>% 
    max()
  
  E$AdvisedTrial <- E$AdvisedTrial %>% 
    mutate(.z = scale(timeEnd)) %>%
    filter(abs(.z) < zThresh) %>% 
    select(-.z)
  
  E$exclusions <- E$exclusions %>% 
    mutate(
      `Too many outlying trials` = 
        map_lgl(pid, ~ E$AdvisedTrial %>% 
                  filter(pid == .x) %>%
                  summarise(n = n(), .groups = 'drop') %>% 
                  pull(n) < (E$nMaxTrials - nMaxOutliers)),
      # if participants were already dropped don't mark them here
      `Too many outlying trials` = if_else(!(pid %in% E$AdvisedTrial$pid),
                                           F, `Too many outlying trials`)
    )
  
  do_exclusions(E$exclusions, envir = E, backup = F)
  
  tmp <- E$AdvisedTrial %>%
    nest(d = -pid) %>%
    mutate(
      n = map_int(d, ~ group_by(., advisor0idDescription) %>%
                    filter(advisor0actualType == 'disagreeReflected') %>%
                    summarise(n = n(), .groups = 'drop') %>%
                    filter(n > 0) %>%
                    nrow())
    ) %>%
    filter(n != 2) %>%
    pull(pid)
  
  E$exclusions <- E$exclusions %>%
    mutate(`Missing offbrand trial data` = pid %in% tmp)
  
  do_exclusions(E$exclusions, envir = E, backup = F)
}

```

Participants' data could be excluded from analysis where they fail attention checks, fail to complete the entire experiment, or have more than `r nMaxOutliers` outlying trials. Outlying trials are calculated after excluding participants who failed to complete the experiment, and are defined as trials for which the total trial time was greater than `r zThresh` standard deviations away from the mean of all trials from all participants.

A browser compatibility issue in this study meant that any participants completing the study using the Safari family of browsers had to be excluded because the advice was not presented appropriately. 

```{r ai-ava-r-exclusion-table}

exclusions$`Total excluded` <- exclusions %>% select(-pid) %>% apply(1, any)
n <- ncol(exclusions)
exclusions %>% 
  summarise(across(where(is.logical), sum)) %>%
  mutate(`Total remaining` = length(unique(AdvisedTrial$pid))) %>% 
  pivot_longer(everything(), names_to = "Reason", values_to = "Participants excluded") %>%
  kable(caption = "\\label{tab:ai-ava-dates-exclusions}Participant exclusions for Dates task advice influence experiment") %>%
  row_spec((n - 1):n, bold = T)

```

##### Task performance



##### Mainpulation checks

##### \OpenScience{prereg} Hypothesis test

```{r ai-ava-r-graph, fig.caption="Dates task advisor influence for high accuracy/agreement advisors.  Shows the influence of the advice of the advisors. The shaded area and boxplots indicate the distribution of the individual participants' mean influence of advice. Individual means for each participant are shown with lines in the centre of the graph. The dashed outline shows the distribution of participant means in the original study of which this is a replication."}

for (E in c(.GlobalEnv, .prevStudy)) {
  
  E$tmp <- E$AdvisedTrial %>% 
    filter(advisor0actualType == "disagreeReflected") %>%
    group_by(pid, advisor0idDescription) %>%
    select(c(matches('(influence|woa)'), group_vars(.))) %>%
    summarise_all(mean)
  
  # Add in feedback condition
  if (identical(E, .GlobalEnv)) {
    E$tmp <- left_join(
      E$tmp,
      E$AdvisedTrial %>% 
        group_by(pid) %>% 
        summarise(feedbackCondition = case_when(any(feedback == 1) ~ 'Feedback', 
                                                T ~ 'No feedback'),
                  .groups = 'drop'),
      by = 'pid'
    )
  } else {
    E$tmp <- left_join(
      E$tmp,
      E$AdvisedTrial %>% 
        nest(d = -pid) %>% 
        mutate(
          feedbackCondition = map_lgl(d, ~ !all(is.na(.$timeFeedbackOn))),
          feedbackCondition = if_else(feedbackCondition, 'Feedback', 'No feedback')
        ) %>%
        select(-d),
      by = 'pid'
    )
  }
  
  E$tmp <- E$tmp %>% 
    ungroup() %>%
    mutate_if(is.character, factor) %>%
    mutate(`Feedback condition` = feedbackCondition)
}

bf <- tmp %>% 
  nest(d = -c(feedbackCondition, `Feedback condition`)) %>%
  mutate(
    bf = map(d, ~ select(., pid, advisor0idDescription, advisor0WOA) %>%
               pivot_wider(names_from = advisor0idDescription, values_from = advisor0WOA) %>%
               filter_if(is.numeric, ~ !is.na(.)) %$%
               ttestBF(x = Accurate, y = Agreeing, data = ., paired = T))
  ) 

bf$BF <- sapply(1:nrow(bf), function(i) bf2str(exp(bf$bf[[i]]@bayesFactor$bf)))


dw <- .1

ggplot(tmp, aes(x = advisor0idDescription, y = advisor0WOA, 
                colour = `Feedback condition`, fill = `Feedback condition`)) +
  geom_line(aes(group = pid), alpha = .25) +
  geom_split_violin(aes(x = nudge(advisor0idDescription, dw),
                        group = advisor0idDescription), width = .9,
               colour = NA) +
  geom_split_violin(aes(x = nudge(advisor0idDescription, dw),
                        group = advisor0idDescription,
                        fill = `Feedback condition`), 
                    colour = 'black', alpha = 0, width = .9, 
                    linetype = 'dashed', size = 1,
                    data = .prevStudy$tmp) +
  geom_boxplot(outlier.shape = NA, size = 1, width = dw/2,
               aes(x = nudge(advisor0idDescription, dw), 
                   group = advisor0idDescription),
               colour = 'black') +
  geom_segment(x = 1, xend = 2, y = 1.05, yend = 1.05, colour = 'black') +
  geom_label(y = 1.05, x = 1.5, colour = 'black', fill = 'white', 
             aes(label = paste0('BF = ', BF)), data = bf) +
  scale_y_continuous(limits = c(0, 1), 
                     breaks = seq(0, 1, length.out = 6), 
                     expand = expansion(add = c(0, 0.1))) +
  scale_fill_hue(direction = -1, aesthetics = c('fill', 'colour')) +
  facet_grid(~`Feedback condition`) +
  labs(x = 'Advisor advice profile', y = 'Influence of advice')

.T.fb <- tmp %>%
  filter(feedbackCondition == 'Feedback') %>%
  select(pid, advisor0idDescription, advisor0WOA) %>%
  pivot_wider(names_from = advisor0idDescription, values_from = advisor0WOA) %>%
  filter_if(is.numeric, ~ !is.na(.)) %>%
  as.data.frame() %$%
  md.ttest(x = Accurate, y = Agreeing, 
           labels = c('*M*$_{Accurate|Feedback}$', '*M*$_{Agreeing|Feedback}$'), 
           paired = T)

.T.Nfb <- tmp %>%
  filter(feedbackCondition != 'Feedback') %>%
  select(pid, advisor0idDescription, advisor0WOA) %>%
  pivot_wider(names_from = advisor0idDescription, values_from = advisor0WOA) %>%
  filter_if(is.numeric, ~ !is.na(.)) %>%
  as.data.frame() %$%
  md.ttest(x = Accurate, y = Agreeing, 
           labels = c('*M*$_{Accurate|¬Feedback}$', '*M*$_{Agreeing|¬Feedback}$'), 
           paired = T)

tmp <- rename(tmp, 
              WOA = advisor0WOA, 
              Advisor = advisor0idDescription, 
              Feedback = feedbackCondition)

.BF <- tmp %>% 
  as.data.frame() %>%
  anovaBF(WOA ~ Feedback + Advisor, 
          data = ., 
          whichRandom = "pid",
          progress = F) %>%
  marginalBF(comparisons = list(1, 2, c(4,3))) %>%
  mutate(BF.M1.M2 = bf2str(BF.M1.M2))
  
.AOV <- ezANOVA(tmp, dv = WOA, wid = pid, within = Advisor, between = Feedback)
```

```{r ai-ava-r-anovas}

kable(.BF, caption = "\\label{tab:ai-ava-dates-r-bf}Bayesian ANOVA for Dates task advice influence experiment")

kableANOVA(.AOV$ANOVA, caption = "\\label{tab:ai-ava-dates-r-aov}Frequentist ANOVA for Dates task advice influence experiment")

```

There were systematic differences in the influence of advice on the key trials where the advice itself was balanced between advisors.
Frequentist (Table \@ref(tab:ai-ava-dates-r-aov)) and Bayesian (Table \@ref(tab:ai-ava-dates-r-bf)) ANOVA analyses both indicated an interaction between the Advice profile of an advisor and the Feedback condition of the participant.
The frequentist ANOVA indicated a main effect of Advice profile, although there was no reliable evidence either way for this effect in the Bayesian test.

Within each condition, T-tests revealed that the Accurate advisor was more influential than the Agreeing advisor in the Feedback condition (`r .T.fb`). 
The advisors were equivalently influential in the No feedback condition (`r .T.Nfb`).

#### Questionnaires

### Discussion {#ai-ava-d}

The experiments show that people can learn to disregard advice which provides support but no information, provided that feedback is available.
Where feedback is unavailable, people do not appear to distinguish between useful and supportive advice.
These results are not wholly consistent with an account of advice-taking in which people use agreement to evaluate advisors in the absence of feedback.
Under such an account, we would expect participants in the No feedback condition to have shown a greater susceptibility to advice from the Agreeing advisor, but this did not happen.
The equivalence of the influence of the advisors in the No feedback condition may be a consequence of relatively high levels of advice influence overall, producing a ceiling effect (the numerical advantage for the Agreeing advisor is as predicted by the theory).
The relatively high levels of advice influence are a feature of the Dates task, a consequence of the difficulty of the questions for most participants.
Another possible explanation for the equivalence of influence between the advisors is that participants may not have had enough exposure to the advisors to properly learn about the value of their advice. 
The limitation on exposure is another feature of the Dates task: while the Dots task provides participants with many tens of trials in which to update their assessment of advisors, the Dates task provides a level of exposure more similar to normal social interaction (although not necessarily very similar).

While not wholly consistent with the agreement-as-proxy account, the results were broadly consistent with previous work: participants systematically took more advice from the more accurate advisor. \mccorrect{Is that a fair summary? Seems only true in the feedback condition. Does that mean it really is consistent?}
The distinction between agreeing and accurate advice revealed by \mccorrect{Niccolo and Nick - what distinction did they actually make?} generalises to a substantially different task which uses a different kind of decision and different level of exposure to advisors' advice.
In the experiments which follow ([§Psychology of source selection](#chapter-advisor-choice)), both the Dots and Dates tasks are used to investigate differences in source selection behaviour.

The design of this experiment deliberately violated an assumption which may be generally true in real life, that the advice is sufficiently independent as to convey at least some information regarding the correct answer.
Had we told participants that the agreeing advisor would agree with them no matter what they the participants said, the participants may have disregarded the advice.
Nevertheless, the results show that, even where they would have performed objectively better by preferring accurate over agreeing advice, participants were not able to detect the more accurate advice without objective feedback.

## Confidence-contingent advice {#ai-cca}

\indent\indent\OpenScience{data} \mccorrect{!TODO[OSFify data for these studies]}

\OpenScience{materials} https://github.com/oxacclab/ExploringSocialMetacognition/blob/master/ACv2/index.html

ACv2/withConfidence_coreAnalysis_v0.0.1

\mccorrect{!TODO[This experiment is kinda crappy and failed its manipulation.
Can we run a version which does its job properly and provide actual evidence for/against the confidence modulation?]}

### Method {#ai-cc-m}

### Results {#ai-cc-r}

### Discussion {#ai-cca-d}

Participants appeared to pay more attention to the advice than the advisor.
In other words, participants distinguished between individual pieces of advice but did not translate these distinctions into distinctions between advisors.
This study thus provided no evidence in favour of the confidence-weighting adjustement to the agreement model.
