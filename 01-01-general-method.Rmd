---
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
  bookdown::word_document2: default
  bookdown::html_document2: default
documentclass: book
---

```{r setup 01-01, echo = F, include = F}
source('scripts_and_filters/general_setup.R')
```

# Behavioural experiment method {#chapter-behavioural-experiment-method}
\adjustmtc 

<!-- The behavioural experiments all have similar methodology, so it makes sense to dedicate a chapter to detailing it and then summarise and explain deviances/specifics when reporting each individual experiment.
-->

The behavioural experiments reported in this thesis share a common structure.
This structure is detailed here to reduce repetition elsewhere in the thesis.
Individual experiments have truncated methods sections in which the specific deviations from the general method are noted.

## General method {#gm}

The experiments take place using a judge-advisor system.
Participants give an **initial estimate** for a decision-making task, receive **advice**, and then provide a **final decision**.
The advice is always computer-generated, although the specifics of the generating procedure vary between experiments.

### Participants {#m-participants}

#### Recruitment

Human participants were recruited from the online experiment participation platform Prolific (https://prolific.co).
Participants were prevented from taking the study if they had participated in one of the other studies in the thesis, or if they had an overall approval rating on Prolific of less than 95/100.

#### Payment
 
Participants were paid approximately GBP10-15/hour pro rata.
Experiments took the average participant between 10 and 30 minutes to complete.

Some participants encountered technical problems, prompting them to contact me via the Prolific platform.
These participants were thanked, and additional information about the problem sought if necessary.
These participants were paid on an ad hoc basis depending upon the time taken before the errors emerged and the detail of the error reports.

Later studies introduced attention checks which terminated the study as a consequence for failure.
It is not clear whether this technique constitutes best practice on Prolific because automatic termination means participants may return the study rather than having their participation explicitly rejected (and thus affecting their Prolific participation rating).
Participants who returned the studies were not paid.
Participants who attempted to complete the study with an invalid code after having their participation terminated for failing attention checks were also not paid, and their completion attempt was rejected on the Prolific platform.
There is an ongoing ethical debate concerning non-payment of participants who fail attention checks in online studies.
The studies in this thesis used a mixture of paying for and not paying for participation attempts with failed attention checks.
In online studies, where low-effort participation is a serious and enduring concern, platforms such as Prolific make clear to participants and researchers that payment is only expected for responses which are given with satisfactory effort.
Participants are thus fully aware of and consenting to the process of screening results for adequate effort prior to payment.
It is important to note the difference between low effort responses, for which payment may ethically be withheld, and atypical responses, which may represent genuine engagement with the task.
It is unethical, in my view, to withhold payment from participants for atypical responses within an experiment (including low or high response times, accuracy, etc).^[Data may be excluded from _analysis_ for these reasons, but the participants should still be paid.] Participants should only be denied payment for failing to provide adequate responses to explicit attention checks.

#### Demographics

Demographic information on participants, such as age and gender, was not collected.
While there is a robust case for collecting these data and conducting sex-disaggregated analyses [@criadoperezInvisibleWomenExposing2019], initial concerns over General Data Protection Regulation resulted in a cautious approach to the collection of data concerning protected characteristics of participants.
\mccorrect{!TODO[move to discussion]}Gender differences, whether due to socialisation, biological factors, or their interactions, may well alter advice-taking and expressed confidence in decisions.
I suspect, although I can offer no evidence, that gender differences in the results presented in this thesis will at most show overlapping distributions.
I do not think it highly plausible that different strategies are wholly the preserve of any particular gender, or that egocentric discounting is markedly stronger in any particular gender.

Participants were at least 18 years of age, confirmed by the requirements for possessing an account on the Prolific platform and by explicit confirmation when giving informed consent.

### Ethics {#m-ethics}

Ethical approval for the studies in the thesis was granted by the University of Oxford Medical Sciences Interdivisional Research Ethics Committee (References: R55382/RE001; R55382/RE002).

### Procedure {#m-procedure}

Participants visited the Uniform Resource Locator for the study by following a link from Prolific using their own device (Figure \@ref(fig:m-participant-experience)).
Early studies only supported computers, but later studies included support for tablets and smartphones.
After viewing an information sheet describing the study and giving their consent to participate, participants began the study proper.
The study introduced the software to the participant interactively, demonstrating the decision-making task and how responses could be made.
Next, participants were given a block of practice trials to familiarise them with the decision-making task.
Participants were then introduced to advice, and given a block of practice trials in which they received advice.
The core experimental blocks followed the practice with advice.
Finally, debrief questions were presented and feedback provided concerning the participant's performance, including a stable link to the feedback and a payment code.
The participant entered the payment code into the Prolific platform and their participantion was at an end.

```{r m-participant-experience, fig.align='center', fig.caption="Participant pathway through the studies.  Participants used their own devices to complete the study, which was presented on a website written in HTML, CSS, and JavaScript. The data were saved on the server using PHP.", out.width="100%"}
knitr::include_graphics("figures/methods/participant-experience.jpg")
```

On each trial, participants were faced with a decision-making task for which they offered an initial estimate.
They then received advice (on some trials they were able to choose which of two advisors would provide this advice).
They then made a final decision.
On feedback trials, they received feedback on their final decision.
The schematic for this trial structure is shown for the Dots Task in Figure \@ref(fig:m-trial).

```{r m-trial, fig.align='center', fig.caption="Trial structure of the Dots Task.  In the initial estimate phase, participants saw two boxes of dots presented simultaneously for 300ms. Participants then reported whether there were more dots on in the left or the right box, and how confident they were in this decision. Participants then received advice, sometimes being offered the choice of which advisor would provide the advice. The advice was displayed for 2000ms before participants could submit a final decision, again reporting which box they believe contained more dots and their confidence in their decision. On feedback trials, feedback was presented by redisplaying the correct box while showing the other box as empty.", out.width="100%"}
knitr::include_graphics("figures/methods/dots-structure.jpg")
```

\mccorrect{!TODO[Check feedback duration and style in image caption]}

#### Perceptual decision (Dots Task) {#m-p-dots}

Stimuli in the Dots Task consisted of two boxes arranged to the left and right of a fixation cross (Figure \@ref(fig:m-dots)).
These boxes were briefly and simultaneously filled with an array of non-overlapping dots, and the participant was instructed to indentify the box with the most dots.
The number of dots was exactly determined by the difficulty of the trial: the box with the \mccorrect{!TODO[check this description]} least dots had 200 - the difficulty, while the box with the most had 200 + the difficulty.
The dots did not move during the presentation of the stimulus.
There was thus an objectively correct answer to the question which, given enough time, could be precisely determined from the stimulus.

```{r m-dots, fig.align='center', fig.caption="Dots Task stimulus.", out.width="100%"}
knitr::include_graphics("figures/methods/dots.jpg")
```

The Dots Task stimuli can be customised to make the discrimination easier or more difficult.
This means that the stimuli can be adjusted to maintain a specific accuracy for each individual participant, allowing confidence to be examined in the absence of confounds with the probability of being correct.
Stimuli were continually adjusted throughout the experiment to maintain an initial estimate accuracy of around 72% using a 2-down-1-up staircase procedure.
There were a substantial number \mccorrect{!TODO[how many?]} of trials in the practice block so that participants could eliminate practice effects and thus experience a more stable objective difficulty during the core trial blocks.
After each block participants were told what percentage of the final decisions they had provided were correct and allowed to take a short, self-paced break.

##### Specific limitations

The Dots task uses a perceptual decision with a high number of trials.
This structure makes it plausible that participants respond to the advice primarily through simple reinforcement learning rather than through specific social processes thought to be at work in advice-taking and advisor evaluation.

Whether or not the Dots task taps into social processes, both the task itself and the experimental structure are very different from most advice-taking and advisor evaluation in the real world. 
Perceptual decisions are rarely the subject of advice, and thus the central task is an unusual one for joint decision-making.
The amount of exposure to advisors also greatly exceeds that which would be obtained over a far longer period of time in most real world situations.
This much greater level of exposure risks investing effects with artificial importance: while it is a strength of experimental designs to magnify the effects they aim to study we must not let such magnification blind us to the real relevance of these effects within the complex and dynamic context of real life.

#### Estimation (Dates Task)

##### Rationale

##### Continuous {#m-p-dates-c}



```{r m-dates, fig.align='center', fig.caption="Dates Task with continuous responses.", out.width="100%"}
knitr::include_graphics("figures/methods/dates.jpg")
```

##### Binary {#m-p-dates-b}



```{r m-dates-binary, fig.align='center', fig.caption="Dates Task with binary responses.", out.width="100%"}
knitr::include_graphics("figures/methods/dates-binary.jpg")
```

##### Development

\mccorrect{!TODO[In some amount of detail TBD describe the process of coming up with questions (trawling general knowledge sites, eventually settling on a few timelines inc. Wikipedia, Oxford thingy; questions 1850-1950; rounds of getting participant answers + conf intervals; questions 1900-2000; answers + conf intervals)]}

##### Specific limitations

\mccorrect{!TODO[Some discussion of limitations of this method: no control over participant error rates, difficult task, inconsistent presentation for an individual (e.g. solid knowledge of an obscure date)]}

#### General limitations

There are several limitations common to both task designs.
The most obvious limitation is that the advisors are not organically-interacting humans.
There are other ecological validity limitations in the presentation of advice, the structure of the experiment, and the absence of other cues.

\mccorrect{!TODO[Other limitations that aren't wider ecological validity ones?]}

The use of artificial advisors means that advice can be carefully specified, and the experiments can be run easily, cheaply, and quickly.
Transparently artificial advisors may, however, limit the generalisability of the experimental results in two ways.
Firstly, if different integration processes exist for social and non-social information, it is plausible that, for at least some participants, the advice information is perceived as non-social information.
While social and non-social information processing would not invalidate any findings (because such a factor would be unlikely to be systematically related to manipulations of interest), they may harm the ability of experimental results to inform us about the processes by which social information is integrated.
Secondly, artificial advisors may not trigger a number of human-centred processes such as equality bias [@mahmoodiEqualityBiasImpairs2015], meaning that effects revealed in these experiments may be much more difficult to observe in real human advice exchanges.

The advice presented to participants in the experiments is specific and impersonal.
During real-life advice-taking, advice is often provided within a discussion, with estimates accompanied by reasons and points responded interactively.
Although studies have indicated that advice-taking behaviour remains similar where discussion is allowed [@libermanNaiveRealismCapturing2012; @minsonTwoTangoEffects2011], these experiments placed discussion within the context of advice exchanges over a decision made by both dyad members individually, and not with distinct roles for the advisor and judge as used in these experiments.

Further ecological validity limitations arise from the structure of the experiment.
The task presents a series of trials sequentially, with a rapid procession through each.
This structure is intended to condense a real-world relationship with an advisor, built up over repeated interactions over time, into as narrow a time window as possible. 
It is likely that this temporal compression does not fundamentally alter the processes of advisor evaluation and trust formation, but we have little positive evidence to support this supposition.

The final major difference between real-life advice and the advice offered in these experiments is in the richness of the relationship between advisor and judge.
In a real-life relationship there would be numerous other factors at play which may alter or overwhelm any advice-taking and advisor evaluation processes revealed by these experiments. 
How well a person gets along with another could matter beyond simply increasing the perceived benevolence of the other, for example.

### Advisor advice profiles {#m-profiles}

\mccorrect{!TODO[Adjust this to talk more precisely about what agreement means in different contexts, and be a more general introduction to the idea of agreement, accuracy, and their relationship.]}

The advisers are virtual agents whose probability of agreeing with the participant’s decision varies as a function of the participant’s confidence and correctness in the initial decision phase.
Table \@ref(tab:m-advisor-profiles) illustrates how this relationship functions, and shows that the overall correctness and agreement rates of the advisers is equivalent overall.
Importantly, on largest minority of trials, the middle 40%, the advisers are exactly equivalent, meaning these trials can be compared directly without confounds arising from agreement rate and initial confidence.

```{r m-advisor-profiles-r} 
library(tidyverse)
library(kableExtra)
tmp <- tribble(
  ~` `, ~`  `, ~`Bias Sharing`, ~`Anti Bias`,
  "Participant correct", "High (top 30%)", 80, 60,
  "Participant correct", "Medium (middle 40%)", 70, 70,
  "Participant correct", "Low (bottom 30%)", 60, 80,
  "Participant incorrect", "Any", 30, 30,
  "Total agreement", "Participant correct", 70, 70,
  "Total agreement", "Participant incorrect", 30, 30
)

kable(tmp, caption = "\\label{tab:m-advisor-profiles}Confidence contingent agreement advisor advice profiles") %>%
  kable_styling() %>%
  column_spec(1, bold = T) %>%
  row_spec(0, bold = F) %>%
  collapse_rows(columns = 1, valign = "top") %>%
  add_header_above(c(" ", 
                     "Initial decision confidence", 
                     "Probability of agreement (%)" = 2))

```


### Analysis {#m-analysis}

#### Dependent variables {#m-analysis-dv}

##### Pick rate

Pick rate provides a measure of source selection behaviour. 
In most experiments there are some trials that offer participants a choice of which advisor they would like to hear from. 
There are always two choices, and a choice must always be made. 
The two choices are consistent within the experiment.
Pick rate is the proportion of choice trials in which a specified advisor was chosen.

A participant's pick rate is an aggregate over a number of trials, and expresses the observed probability of picking the specified advisor.
The mentally represented preference for that advisor is not measured directly (if such a thing even exists), and cannot be determined from the observed pick rate without knowing the mapping function for each individual participant.
Mapping functions (such as softmax \mccorrect{!TODO[cite something on this]}) produce stochastic choice behaviour from a preference marked on a continuous scale, and are typically sigmoid.
The relationship between preference and pick rate is non-linear and idiosyncratic, but it is likely monotonic for all participants: the stronger the preference the higher the pick rate.
Despite this monotonic relationship, it is important not to infer that one participant's preference for an advisor is stronger than another's on the basis of the former picking that advisor more frequently: it may be instead that, for the former participant, smaller differences in preference lead to more consistent picking behaviour.

##### Weight on Advice {#m-analysis-dv-woa}

##### Influence

##### Capped influence {#m-analysis-dv-influence}

Influence, the dependant variable in some analyses, is calculated as the extent to which the judge’s initial decision is revised in the direction of the advisor’s advice.
The initial ($C_1$) and final ($C_2$) decisions are made on a scale stretching from -55 to +55 with zero excluded, where values <0 indicate a ‘left’ decision and values >0 indicate a ‘right’ decision, and greater magnitudes indicate increased confidence.
Influence ($I$) is given for agreement trials by the shift towards the advice:

\begin{align}
I|\text{agree} = f(C_1) 
\begin{cases}
  C_2 - C_1 & C_1 > 0 \\
  -C_2 + C_1 & C_1 < 0
\end{cases}
(\#eq:m-influence-agree)
\end{align}

And by the inverse of this for disagreement trials:

\begin{align}
I|\text{disagree} = -I|\text{agree}
(\#eq:m-influence-disagree)
\end{align}

The confidence scale excludes 0, and thus the final decision can always be more extreme when moving against the direction of the initial answer than when moving further in the direction of the initial answer.
A capped measure of influence was used to minimise biases arising from the natural asymmetry of the scale.
This measure was calculated by truncating absolute influence values which were greater than the maximum influence which could have obtained had the final decision been a maximal response in the direction of the initial answer (Figure \@ref(fig:m-capping)).

```{r m-capping, fig.align='center', fig.caption="Capping influence to avoid scale bias.  In this example the judge’s initial response is 42, meaning that their final decision could be up to 13 points more confident or up to 97 points less confident. Any final decision which is more than 13 points less confident is therefore capped at 13 points less confident.", out.width="100%"}
knitr::include_graphics("figures/experiment_01_MATLAB/capping.jpg")
```

The capped influence measure $I_\text{capped}$ is obtained by:

\begin{align}
I_\text{capped} = f(C_1) 
\begin{cases}
  \text{min}(I, 2C_1 - S_\text{max}) & C_1 > 0 \\
  \text{max}(I, 2C_1 + S_\text{max}) & C_1 < 0
\end{cases}
(\#eq:ex1-influence-capping)
\end{align}

\mccorrect{!TODO[Check this equation and explain its terms]}

#### Statistics

Statistical analyses are conducted using a both Frequentist and Bayesian statistics.

##### Frequentist statistics

Frequentist statistics, often in psychology referred to simply as 'statistics', are a family of statistical tests which minimise the long-term error rates rates of the conclusions they invite.
To perform one of these tests, the user first specifies a null hypothesis which the test will seek to reject.
Next, the user selects a rate of false-positives that is acceptable, termed an alpha ($\alpha$).
In psychology $\alpha$ is almost universally .05, meaning that 5% of results for tests where there is no true effect will be positives.
The principle statistic of interest from these tests is the $p$-value: the chance that a sample equivalent to the user's sample would be as extreme or more extreme than the observed sample _assuming that the null hypothesis is true_.
The $p$-value quantifies how expected the sample observations are where the null hypothesis is the model: its complement quantifies how surprising the data are given the null hypothesis is true.
If the $p$-value is lower than the $\alpha$ the null hypothesis may be rejected on the reasoning that such an unlikely sample as the current one is more consistent with some other hypothesis.
Where the null hypothesis is rejected the sample is labelled as being _significantly different_ from that expected from the null model, and this general feature of the difference is known as statistical significance.

It is common practice when using frequentist statistics to set up experiments such that two, and only two hypotheses can possibly explain the data.
One of these is deemed the null hypothesis and tested as described above.
The other hypothesis is termed the 'alternate' hypothesis, and, if the null hypothesis is rejected, is accepted in place of the null hypothesis because there were only two possible explanations and the null hypothesis has been ruled out.

Frequentist statistics are useful, better understood by psychologists than alternatives, and capable of delivering genuine insights when used correctly.
A great many caveats surround their use and interpretation, however. 
Frequentist statistics cannot express positive evidence for the null hypothesis (because the null hypothesis is an assumption of the tests): $p$-values are uniformly distributed among samples drawn from the null hypothesis [@murdochPValuesAreRandom2008] and so no argument can be made from any particular value that it constitutes more or less evidence for the null.
Likewise, $p$-values below the chosen $\alpha$ cannot strictly be interpreted as expressing more or less evidence for the alternate hypothesis (or against the null hypothesis). 
Frequentist statistics control long-term error rates rather than capturing relative likelihoods of theories, and thus the only legitimate inferences from a frequentist test are to accept or reject the null hypothesis.
The uniform distribution of $p$-values under the null also means that repeated sampling from the null distribution without adjusting the $\alpha$ will increase the rate at which false-positive conclusions are drawn.
This repeated sampling can happen in many ways such as running related tests on the same data, using slightly different data (e.g. by adjusting exclusions), and running tests at multiple time points in data collection (especially if a significant result terminates data collection).

In this thesis a range of frequentist statistical tests are used, most frequently t-tests and analyses of variance (ANOVA).
The $\alpha$ is always set at .05 unless otherwise stated.
Null hypotheses are always the expected distribution if the effect being tested is nil.
Where the level of influence exerted by two different advisors is studied, for example, the null hypothesis would be that there were no systematic differences in influence exerted by those advisors.

##### Bayesian statistics

Bayesian statistics consist in a statistical approach that attempts to capture the (posterior) likelihood of a hypothesis being true on the basis of its (prior) plausibility and the strength of the evidence.
These tests are frequently adapted to quantify the relative likelihood of two competing hypotheses given their relative prior plausibilities and how consistent the observed evidence is with each theory.
To perform these Bayesian statistical tests the user specifies the hypotheses to be compared, their relative likelihoods, and the evidence sampled.

The principle statistic in Bayesian tests is the Bayes Factor (BF).
BF quantifies the posterior likelihood of one hypothesis over another. 
In the notation used here, BF always quantifies a more complex model over a simpler one: in the case of a Bayesian t-test it therefore quantifies the alternate hypothesis (in which there is an effect) over the null hypothesis (in which there is not an effect).
The BF takes a value between 0 and infinity.
Values below 1 indicate a greater likelihood for the simpler model, and that greater likelihood is given by 1/BF.
Where BFs below 1 are reported in this manuscript the notation 1/BF is used to allow an intuitive reading of the strength of evidence in favour of the simpler model.

Bayesian tests produce a continuous measure of relative likelihood which simply describes the data and prior beliefs. 
In order to draw categorical inferences, thresholds are placed on this continuous outcome.
Here these thresholds are 1/3 < BF < 3, meaning that a BF of less than 1/3 constitutes evidence in favour of the simpler model while a BF greater than 3 constitutes evidence in favour of the more complex model.
These values are those suggested by \mccorrect{!TODO[CITE whoever has that neat little table. Jeffreys?]} as representing \mccorrect{!TODO[which word does the paper use?]} evidence in a given direction.
Where the BF lies between these thresholds it is labelled as 'uninformative'.
An uninformative result supports neither the simpler nor the more complex model, and indicates that the data are insufficient to distinguish the hypotheses.

The Bayesian tests used here rely on the priors specified by the BayesFactor R package \mccorrect{!TODO[CITE BayesFactor]}.
These priors govern the expected distributions of observed differences between samples where there is or is not a genuine effect creating systematic differences.
The use of the same, weakly-informative priors for all tests means the approach used here is an 'objective Bayesian' approach.
This objective Bayesian approach can be contrasted with a 'subjective Bayesian' approach in which the goal is to specify the exact amount of belief one should have in one hypothesis over another.
Neither the objective nor subjective approach is clearly superior. 
The subjective approach is used here because it is simpler.
There is some risk that results will be a poor fit to reality because the priors are inappropriate, but this risk is fairly low and somewhat mitigated by the additional inclusion of frequentist statistics.

##### Integrating statistical results

In most cases, Bayesian and frequentist statistics produce the same conclusion \mccorrect{!TODO[pretty sure there's a paper which states/demonstrates this]}.
Where this is not the case, results should be interpreted very cautiously: a significant frequentist test with an uninformative or null-favouring Bayesian test can indicate that the result may be a false-positive, while clear Bayesian support for the alternate hypothesis in the absence of a significant frequentist test can indicate that the priors in the Bayesian test are inappropriate. 

Where null conclusions are to be drawn, i.e. the null hypothesis is to be retained, only Bayesian statistics can be considered informative. 
In these cases Bayesian statistics will be interpreted, with the caveat that the safeguard of using two independent approaches to draw statistical conclusions has lapsed.

##### Software

Data analysis was performed using R [@rcoreteamLanguageEnvironmentStatistical2018].
For a full list of packages and software environment information, see \mccorrect{!TODO[figure out where to include this stuff.
Appendix?
Also link to a containerized version of this.]}

## Open science approach {#open-science}

### Open science {#os-introduction}

_Nullius in verba_ ("take nobody's word for it") is written in stone above the entrance to the Royal Society's library.
This fundamental principle of science, that it proceeds on evidence rather than assertion, has frequently been forgotten in practice.
Concerns about sloppy, self-deluding, or outright fradulent science have existed since at least the time of Bacon.
The modern open science movement in psychology dates from the early 2010s.
Simmons et al.
demonstrated how easily false positive results could emerge from unconstrained researcher degrees of freedom in analysis [@simmonsFalsePositivePsychologyUndisclosed2011], Nosek and colleagues published a roadmap for improving the structure and function of academic research and publishing [@nosekScientificUtopiaOpening2012; @nosekScientificUtopiaII2012], and the Open Science Collaboration began [@collaborationEstimatingReproducibilityPsychological2015].
In the years following, a deluge of papers, movements, and practical changes have emerged.
The meaning of open science varies within each sub-discipline, and this section outlines how the experiments comprising this thesis have been conducted in a reproducible and transparent manner.

### Badges {#os-badges}

Following the Center for Open Science (https://cos.io), this thesis uses a series of badges to indicate adherence to particular aspects of open science.
Three badges, _preregistration_, _open materials_, and _open data_, are adopted directly from the Centre and used according to the Centre's rules (https://osf.io/tvyxz/wiki/1.%20View%20the%20Badges/).
Studies which qualify for a badge will have the badge displayed immediately below their title.
Each badge will contain a link to online resources which provide the content for which the badge is awarded.

#### \OpenScience{prereg} Preregistration {#os-prereg}

Preregistration of a study means that information about the study has been solidified prior to the analysis of the data.
This means that hypotheses cannot be changed to represent unanticipated or overly-specific findings as a priori predicted [@kerrHARKingHypothesizingResults1998].
In practice in this thesis, preregistration means describing in detail the design and analysis plan for an experiment and depositing the description with a reputable organisation prior to data being collected.
The links which accompany the preregistration badge will point to the preregistration document.
These measures help to prevent presenting a highly selected and biased interpretation of the data as the result of a natural analytical process.

The preregistration badge also appears within results sections to designate those statistical investigations which were included in the preregistration.
Some analyses are exploratory.
These exploratory analyses are not included in the preregistration, because they are inspired by the data themselves.
They are reported after the preregistered analyses, or are clearly designated as exploratory in the text.

#### \OpenScience{materials} Open materials {#os-materials}

A foundational principle of science is that findings can be reproduced by other people.
Open materials facilitate reproduction by making it easier to rerun an experiment.
Open materials also increase the likelihood that errors can be identified.
In the case of the behavioural experiments reported here, the open materials include computer code necessary to run the experiment.
The links accompanying the open materials badge points to this code.

#### \OpenScience{data} Open data {#os-data}

Theories are the output of science as a whole, but data are the output of any individual study.
Sharing data directly allows other scientists to check and extend the data analysis conducted, to reuse the data in meta-analyses, and to repurpose the data for other investigations.
This increases the robustness of the results, and increases the efficiency of science as a whole.
The links accompanying the open data badge point to online storage where the data can be obtained for a study, along with appropriate metadata.

### Thesis workflow

This thesis is written in RMarkdown, with the data fetched and analysed at the time the document is produced using the publically available pipeline - the entire document can be reproduced locally using the source code in an appropriate environment.
A Docker environment copying the environment used to produce this document is available at \mccorrect{!TODO[the containerisation thing]}
