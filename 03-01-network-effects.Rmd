---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::html_document2: default
  bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
documentclass: book
---

# Network effects of interaction {#chapter-network-effects}
\adjustmtc 

```{r}
source('scripts_and_filters/general_setup.R')
library(adviseR)
library(igraph)
library(BayesFactor)
library(magrittr)
library(parallel)
library(PearsonDS)

library(gifski)

# animation
saveAnimation <- function(model, saveAs) {
  p <- tempfile(pattern = 'anim_')
  dir.create(p)
  gs <- list(
    which(V(model$model$graphs[[1]])$bias <= .5),
    which(V(model$model$graphs[[1]])$bias > .5)
  )
  coords <- NULL
  for (i in 1:length(m$model$graphs)) {
    g <- model$model$graphs[[i]]
    png(filename = paste0(p, '/', sprintf('%04i', i), '.png'))
    plotGraph(model, i, activeColours = F, layout = function(x) 
      layout_with_fr(x, coords = coords, start.temp = 0.1), mark.groups = gs)
    dev.off()
    coords <- layout_with_fr(g, coords = coords, start.temp = 0.1)
  }
  gifski(
    list.files(p, full.names = T), 
    gif_file = saveAs,
    width = 480, 
    height = 480, 
    delay = .1,
    loop = F,
    progress = F
  )
  unlink(p)
}
```

<!-- We may have some modelling to present along these lines. -->

## Models with agents drawn from a distribution

### Can we reproduce the major polarisation/echo chamber effects?

First, we present the models with a variety of parameters.
This shows basic model behaviour range before we consider weighted selection.

```{r network-models}
t1 <- Sys.time()
n_reps <- 10
withr::with_seed(
  20210507,
  seeds <- c(20210507, floor(runif(n_reps - 1) * 1e8))
)
parameters <- tidyr::crossing(
  bias_volatility_mean = .01,
  bias_volatility_sd = .005,
  n_agents = 100,
  truth_sd = .5,
  sensitivity_mean = 3,
  sensitivity_sd = .5,
  random_seed = seeds,
  # starting_graph = c(function(x) {
  #   matrix(runif(nrow(x) ^ 2, .5), nrow(x), nrow(x))
  # }),
  starting_graph = .6,
  confidence_weighted = F,
  tibble(
    n_decision = c(500, 700),
    decision_flags = map(
      n_decision, 
      ~ if (. > 500) {c(rep(1, 500), rep(3, 200))} else {rep(3, .)}
    )
  ),
  tidyr::crossing(
    tibble(
      trust_volatility_mean = .0112, # estimated from our empirical data
      trust_volatility_sd = 0
    ),
    bias_sd = c(.1, .75),
    # confidence_weighted = c(T, F),
    # bias_volatility_sd = c(0, .05), 
    # feedback_proportion = c(0, .1),
    tibble(
      weighted_sampling_mean = c(0, 3.0, 10.1),
      weighted_sampling_sd = c(0, 18.1, 151)
    )
  )
)

# Drop unnecessary models
# parameters <- parameters %>% 
#   filter(!(bias_volatility_sd == 0 & n_decision > 200))

summary_fun <- function(m) {
  .cc <- function(x, ...) adviseR::.cluster_count(x, k = 1:2, ...)
  # Calculate group ratios
  m$parameters$group_ratio_original <-
    adviseR::groupRatio(m$model$graphs[[length(m$model$graphs)]])
  m$parameters$group_ratio_empirical <-
    adviseR::groupRatio(m$model$graphs[[length(m$model$graphs)]], F)
  
  lastGen <- 
    m$model$agents[m$model$agents$decision == max(m$model$agents$decision), ]
  
  # Record mean final bias to determine which way the population is going
  m$parameters$mean_final_bias <- mean(
    m$model$agents$bias[m$model$agents$decision == max(m$model$agents$decision)]
  )
  
  # Calculate proportion of extremes
  x <- lastGen$bias
  m$parameters$n_lt_20 <- mean(x < .2)
  m$parameters$n_gt_80 <- mean(x > .8)
  m$parameters$n_mid <- mean(x >= .2 & x <= .8)
  
  # Calculate the bias magnitude
  m$parameters$meanBiasMagnitude <- mean(abs(lastGen$bias - .5)) 
  
  # Calculate the compression stats
  m$parameters$cluster_count <- .cc(
    m$model$graphs[[length(m$model$graphs)]]
  )
  # Save the summary stuff for the first example of each model
  if (m$parameters$random_seed != 20210507 || m$parameters$confidence_weighted) {
    m$model <- NULL
  } else {
    library(tidyverse)
    m$model$agents <- m$model$agents %>%
      nest(d = -decision) %>%
      mutate(
        cluster_count = map_int(
          decision,
          function(i) {
            g <- m$model$graphs[[i]]
            if (all(is.na(g)))
              NA_integer_
            else
              .cc(g)
          }
        )
      ) %>%
      unnest(cols = d)
  }
  
  m
}

models <- runSimulations(
  parameters, 
  cores = min(nrow(parameters), 3),
  summaryFun = summary_fun
)

for (m in models) {
  if (!is.null(m$model)) {
    try({
      adviseR::networkGraph(
        m, 
        mark.groups = list(
          which(V(m$model$graphs[[1]])$bias <= .5),
          which(V(m$model$graphs[[1]])$bias > .5)
        )
      ) 
    })
    
    try({
      for (x in adviseR::inspectModel(m))
        print(x)
    })
    
    # Bias magnitude
    print(
      m$model$agents %>% 
        mutate(biasDeviance = abs(bias - .5)) %>%
        ggplot(aes(x = decision, y = biasDeviance)) +
        stat_summary(geom = 'ribbon', fun.data = mean_cl_normal) +
        scale_y_continuous(limits = c(0, .5))
    )
  }
}

output <- models %>%
  lapply(function(m) {
    m$parameters$truth_fun <- NULL
    m$parameters$decision_flags <- NULL
    m$parameters$starting_graph <- NULL
    as_tibble(m$parameters)
  }) %>%
  bind_rows() %>%
  mutate(burnIn = n_decisions != min(n_decisions))

output %>% select(ends_with('ratio'), everything())
Sys.time() - t1
```



```{r} 
# As above but individual runs

# used to access names of potentially varying parameters
tmp <- output %>% select( 
  burnIn, 
  bias_sd:feedback_proportion, 
  truth_sd, 
  confidence_weighted, 
  -weighted_sampling_sd, 
  -trust_volatility_sd,
  random_seed
)

for (v in names(select(tmp, -random_seed))) {
  if (length(unique(pull(output, v))) < 2)
    next()
  print(
    output %>% 
      mutate(grp = glue(
        '{',
        paste0(names(select(tmp, -!!sym(v))), collapse = "}|{"),
        '}'
      )) %>%
      rowid_to_column() %>%
      select(
        rowid, 
        meanBiasMagnitude, 
        cluster_count, 
        burnIn, 
        bias_sd:random_seed, 
        truth_sd:confidence_weighted, 
        grp
      ) %>%
      pivot_longer(c(meanBiasMagnitude, cluster_count), names_to = 'outcome') %>%
      mutate(across(-value, factor)) %>%
      ggplot(aes(y = value, x = !!sym(v))) + 
      # stat_summary(
      #   geom = 'label', 
      #   fun.data = function(x) data.frame(
      #     y = -Inf, label = glue('mean = {round(mean(x), 2)}'), vjust = 0
      #   )
      # ) + 
      geom_violin(alpha = .1, fill = 'black', colour = NA) +
      geom_line(aes(group = grp), alpha = .25, position = position_jitter(.1, .01)) +
      # geom_boxplot(outlier.colour = NA, fill = NA, width = .5) +
      # geom_label(aes(label = rowid)) +
      facet_wrap(~outcome, scales = 'free_y')
  )
}

```

Summarising the results in this way means that we can observe some notable features of the models.
Echo chambers form more readily where  
* the agents update their trust more readily  
* the agents update their biases  
* the agents start with a flat network wherein all agents have equal trust in one another  
  * this is due to the agents losing their biases at least as quickly as they build their trust networks  
* decisions are updated proportional to the judge's confidence  
* the agents' biases are more different to start with  

### Using the empirically estimated weighted selection distribution

Weighted sampling is the representation in the model of source selection behaviour.
This source selection behaviour is the primary outcome of interest in the behavioural experiments, and thus it is important to understand how this property may theoretically affect echo chamber formation within in the models.

```{r weighted sampling simple}
t1 <- Sys.time()

ws.emp <- list(
  uncoupled = c(mean = 3.0, sd = 18.1, skewness = .0067, kurtosis = 7.75),
  fixedTU = c(mean = 10.1, sd = 151, skewness = 1.40, kurtosis = 32.8)
)
ws <- lapply(ws.emp, function(x) rpearson(parameters$n_agents[1], moments = x))

# Check actual moments vs target moments
ws.emp
lapply(ws, function(x) round(empMoments(x), 3))

models.emp <- NULL
for (i in 1:nrow(parameters)) {
  p <- parameters[i, ]
  agts <- adviseR:::makeAgents(
    n_agents = p$n_agents, n_decisions = p$n_decision, 
    bias_sd = p$bias_sd, sensitivity_mean = p$sensitivity_mean,
    sensitivity_sd = p$sensitivity_sd, 
    trust_volatility_mean = p$trust_volatility_mean,
    trust_volatility_sd = p$trust_volatility_sd, 
    bias_volatility_mean = p$bias_volatility_mean,
    bias_volatility_sd = p$bias_volatility_sd,
    weighted_sampling_mean = p$weighted_sampling_mean,
    weighted_sampling_sd = p$weighted_sampling_sd,
    starting_graph = p$starting_graph
  )
  if (p$weighted_sampling_mean != 0) {
    if (p$weighted_sampling_mean == 3.0)
      agts$agents$weighted_sampling <- rep(ws$uncoupled, p$n_decision)
    if (p$weighted_sampling_mean == 10.1)
      agts$agents$weighted_sampling <- rep(ws$fixedTU, p$n_decision)
  }
  models.emp[[i]] <- list(
    n_agents = p$n_agents, n_decisions = p$n_decision, 
    bias_sd = p$bias_sd, sensitivity_mean = p$sensitivity_mean,
    sensitivity_sd = p$sensitivity_sd, 
    trust_volatility_mean = p$trust_volatility_mean,
    trust_volatility_sd = p$trust_volatility_sd, 
    bias_volatility_mean = p$bias_volatility_mean,
    bias_volatility_sd = p$bias_volatility_sd,
    weighted_sampling_mean = p$weighted_sampling_mean,
    weighted_sampling_sd = p$weighted_sampling_sd,
    starting_graph = p$starting_graph,
    decision_flags = unlist(p$decision_flags),
    confidence_weighted = p$confidence_weighted,
    random_seed = p$random_seed,
    model = agts
  )
}

models.emp <- runSimulations(
  models.emp, 
  cores = min(nrow(parameters), 3),
  summaryFun = summary_fun
)

for (m in models.emp) {
  if (!is.null(m$model)) {
    try({
      adviseR::networkGraph(
        m, 
        mark.groups = list(
          which(V(m$model$graphs[[1]])$bias <= .5),
          which(V(m$model$graphs[[1]])$bias > .5)
        )
      ) 
    })
    
    try({
      for (x in adviseR::inspectModel(m))
        print(x)
    })
    
    # Bias magnitude
    print(
      m$model$agents %>% 
        mutate(biasDeviance = abs(bias - .5)) %>%
        ggplot(aes(x = decision, y = biasDeviance)) +
        stat_summary(geom = 'ribbon', fun.data = mean_cl_normal) +
        scale_y_continuous(limits = c(0, .5))
    )
  }
}

output.emp <- models.emp %>%
  lapply(function(m) {
    m$parameters$truth_fun <- NULL
    m$parameters$decision_flags <- NULL
    m$parameters$starting_graph <- NULL
    as_tibble(m$parameters)
  }) %>%
  bind_rows() %>%
  mutate(burnIn = n_decisions == min(n_decisions))

output.emp %>% select(ends_with('ratio'), everything())
Sys.time() - t1
```

```{r}

op <- bind_rows(
  mutate(output, ws_source = 'sim'),
  mutate(output.emp, ws_source = 'emp')
)

for (v in names(select(tmp, -random_seed, -ws_source))) {
  if (length(unique(pull(output, v))) < 2)
    next()
  print(
    op %>% 
      mutate(grp = glue(
        '{',
        paste0(names(select(tmp, -!!sym(v))), collapse = "}|{"),
        '}|{ws_source}'
      )) %>%
      rowid_to_column() %>%
      select(
        rowid, 
        ws_source,
        meanBiasMagnitude, 
        cluster_count, 
        burnIn, 
        bias_sd:random_seed, 
        truth_sd:confidence_weighted, 
        grp
      ) %>%
      pivot_longer(c(meanBiasMagnitude, cluster_count), names_to = 'outcome') %>%
      mutate(across(-value, factor)) %>%
      ggplot(aes(y = value, x = !!sym(v))) + 
      # stat_summary(
      #   geom = 'label', 
      #   fun.data = function(x) data.frame(
      #     y = -Inf, label = glue('mean = {round(mean(x), 2)}'), vjust = 0
      #   )
      # ) + 
      geom_violin(alpha = .15, aes(fill = ws_source), colour = NA) +
      geom_line(aes(group = grp, colour = ws_source), alpha = .25, 
                position = position_jitter(.1, .01)) +
      # geom_boxplot(outlier.colour = NA, fill = NA, width = .5) +
      # geom_label(aes(label = rowid)) +
      facet_wrap(~outcome, scales = 'free_y')
  )
}

```

It's odd that weighted selection should universally decrease the echo chambery-ness of all the models. 
We can investigate this under conditions where it ought to increase: when the network is already set up such that shared bias correlates with trust.

```{r weighted selection increase}

# Select an appropriate model to examine
parameters[100, ]
# Set up a starting_trust function to make trust proportional to shared bias
trust_fun <- function(agents) {
  bias <- matrix(agents$bias, nrow = nrow(agents), ncol = nrow(agents))
  bias <- abs(bias - t(bias)) # bias similarity
  apply(bias, 1, function(x) {
    1 - (x / max(x) * .5)
  })
}
# Run models across weighted sampling and starting trust
m_base <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = .6,
  weighted_sampling_mean = 0
)
m_tf <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = trust_fun,
  weighted_sampling_mean = 0
)
m_base_ws_5 <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = .6,
  weighted_sampling_mean = 5
)
m_tf_ws_5 <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = trust_fun,
  weighted_sampling_mean = 5
)
m_base_ws_10 <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = .6,
  weighted_sampling_mean = 10
)
m_tf_ws_10 <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = trust_fun,
  weighted_sampling_mean = 10
)
# Explore differences in weighted selection 
bind_rows(
  mutate(adviseR:::.biasCorrelation(m_base), model = 'm_base'),
  mutate(adviseR:::.biasCorrelation(m_tf), model = 'm_tf'),
  mutate(adviseR:::.biasCorrelation(m_base_ws_10), model = 'm_base_ws_10'),
  mutate(adviseR:::.biasCorrelation(m_tf_ws_10), model = 'm_tf_ws_10'),
  mutate(adviseR:::.biasCorrelation(m_base_ws_5), model = 'm_base_ws_5'),
  mutate(adviseR:::.biasCorrelation(m_tf_ws_5), model = 'm_tf_ws_5')
) %>%
  ggplot(aes(x = decision, y = r, colour = model)) +
  geom_errorbar(aes(ymin = ciL, ymax = ciH), alpha = .5)
```

Okay, odd behaviour. 
Apparently the higher the weighted selection the _lower_ the correlation between shared bias and advice weight. 
This seems wrong. 
Even when advice weight starts as perfectly correlated with shared bias, the correlation unravels _faster_ when weighted selection is higher. 

We can explore this directly by taking the connectivity graphs and feeding them into the selectAdvisorSimple function.

```{r weighted selection investigation}
generations <- c(1, 100, 500, 1000)
g <- lapply(generations, function(i) {
  m_tf$model$graphs[[i]][attr = 'weight'] %>%
  matrix(sqrt(length(.)), sqrt(length(.)))
})

picks <- tidyr::crossing(
  i = 1:1e4, 
  weightedSelection = c(0, 1, 5, 15, 25, 50), 
  generation = 1:length(g)
) %>% mutate(
  picks = map2(weightedSelection, generation, function(ws, gen) {
    tibble(
      agent = c(1, 50, 100),
      pick = adviseR:::selectAdvisorSimple(g[[gen]], ws)[agent]
    )
  })
) %>% 
  unnest(cols = picks) %>%
  mutate(generation = generations[generation])

picks

picks %>% filter(agent == 1, generation == 1) %>%
  ggplot(aes(pick, fill = factor(weightedSelection))) +
  geom_histogram(position = 'identity', binwidth = 1) +
  facet_wrap(~weightedSelection, labeller = label_both, scales = 'free_y')

picks %>% 
  mutate(across(-pick, factor)) %>%
  ggplot(aes(pick, colour = weightedSelection)) +
  geom_density(position = 'identity') +
  facet_wrap(generation~agent, labeller = label_both)
  

```

So from the above it looks like the weighted selection function is making agents more likely to select agents they trust more.

We can next check that the mean weight of advisor is higher than the mean advisor weight (i.e. the advisor actually selected on each trial is probably more trusted than the average advisor).

```{r weighted selection average weight}
avg_vs_actual <- tibble(
  generation = generations,
  average_weight = map_dbl(generations, ~ mean(
    g[[which(generations == .)]][upper.tri(g[[which(generations == .)]])]
  )),
  average_advisor_weight_0 = m_tf$model$agents %>%
    filter(decision %in% generations) %>%
    group_by(decision) %>%
    summarise(x = mean(weight), .groups = 'drop') %>%
    pull(x),
  average_advisor_weight_5 = m_tf_ws_5$model$agents %>%
    filter(decision %in% generations) %>%
    group_by(decision) %>%
    summarise(x = mean(weight), .groups = 'drop') %>%
    pull(x),
  average_advisor_weight_10 = m_tf_ws_10$model$agents %>%
    filter(decision %in% generations) %>%
    group_by(decision) %>%
    summarise(x = mean(weight), .groups = 'drop') %>%
    pull(x)
) %>%
  pivot_longer(
    starts_with('average_advisor_weight_'), 
    names_to = 'weightedSelection', 
    names_pattern = '([0-9]+)$', 
    values_to = 'average_advisor_weight'
  )

avg_vs_actual

avg_vs_actual %>%
  mutate(across(-ends_with('_weight'), factor)) %>%
  ggplot(aes(x = weightedSelection, y = average_advisor_weight)) +
  geom_point() + 
  geom_hline(aes(yintercept = average_weight)) +
  geom_label(
    x = 1.5,
    aes(y = average_weight, label = glue('avg={round(average_weight, 2)}'))
  ) +
  scale_y_continuous(limits = c(.5, 1)) +
  facet_wrap(~generation, labeller = label_both)
```

So it looks like the weighted selection really is behaving properly in the models: when weighted selection is not active the average weight of the selected advisors is approximately equal to the average weight of all advisors; and as weighted selection is increasingly strong the average weight of selected advisors rises.

This, in combination with the previous graphs, suggests that weighted selection really is doing its job.
The next candidate for examination is trust updating: are agents decreasing their trust in one another on average following interaction (thus producing an effect whereby the more likely interaction is to occur, the more likely trust is to degrade)?

```{r trust update investigation}
g_to_mat <- function(g) {
  x <- g[attr = 'weight']
  n <- sqrt(length(x))
  matrix(x, n, n)
}

get_trust_update <- function(g1, g2) {
  apply(g2 - g1, 1, sum)
}

generations <- c(1, 99, 499, 999)

for (m in c("m_tf", "m_tf_ws_10", "m_base_ws_10")) {
  mdl <- get(m)
  tu <- mdl$model$agents %>% 
    filter(decision %in% generations) %>%
    mutate(generation = decision) %>%
    nest(d = -decision) %>%
    mutate(
      d = map(d, ~ mutate(
        .,
        trust_update = get_trust_update(
          g_to_mat(mdl$model$graphs[[.$generation[1]]]),
          g_to_mat(mdl$model$graphs[[.$generation[1] + 1]])
        )
      ))
    ) %>%
    unnest(cols = d)
  
  print(
    tu %>% 
      mutate(advisorAgrees = adviseR:::adviceAgrees(initial, advice)) %>%
      ggplot(aes(x = weight, y = trust_update, colour = advisorAgrees)) +
      geom_hline(yintercept = 0) +
      geom_vline(xintercept = .5) +
      geom_smooth(method = 'lm', formula = y ~ x) +
      geom_point(alpha = .5) +
      coord_fixed() +
      scale_y_continuous(limits = c(-.25, .25)) +
      scale_x_continuous(limits = c(0, 1)) +
      facet_wrap(~generation, labeller = label_both) +
      labs(title = m)
  )
}
```

As shown in these figures, trust updates are dependent upon the current weight (because they use a weighted average drift). 
This means that, the more trusted an advisor, the more damaging disagreement becomes and the less beneficial agreement becomes. 
This makes sense, but may be driving the phenomenon of weighted selection producing less pronounced bias-weight correlations.
More highly weighted advisors are more likely to be called on for advice, and given the disproportionate penalties for disagreement, more likely on average to decrease trust weights.
Over time, this may produce the pattern of more highly trusted advisors being less trusted.
The correlation seen in the non-weighted case might be an artefact of the more trustworthy advisors being consulted less often, and hence having less opportunity to lose their trust.

Note that disagreement is still very common even with highly rated advisors.





We focus on exploring whether weighted sampling introduces a lower threshold for asymmetric tie weights to produce echo chambers. 
We examine this over several repetitions of models, for a wider range of values for weighted_sampling and decision burn-in.
For simplicity (and tractability) we reduce each collection of model instances to a simple set of summary statistics describing the group weight ratio.

```{r}
if (!(hasName(.Options, 'ESM.skip') && getOption('ESM.skip'))) {
  t1 <- Sys.time()
  set.seed(20201126)
  
  summaryFun <- function(model) 
    groupRatio(model$model$graphs[[length(model$model$graphs)]])
  
  cacheFile <- '_cache/model-summary.rda'
  if (file.exists(cacheFile)) load(cacheFile) else runs <- NULL
  
  start_time <- Sys.time()
  
  leads <- c(0, 250, 500, 750)
  parameters <- tidyr::crossing(
    tibble(
      n_decision = map_int(leads, ~ as.integer(. + 1000)),
      decision_flags = map(leads, ~ c(rep(1, .), rep(3, 1000)))
    ),
    weighted_sampling_mean = c(0, 3, 6, 12, 24, 48),
    weighted_sampling_sd = 0,
    bias_volatility_sd = .05,
    bias_volatility_mean = 0,
    n_agents = 100,
    starting_graph = .1
  ) 
  
  # We now have a lot of different models
  max_cores <- detectCores() - 6
  n_reps <- 4 # 4 full-computer runs for each model
  
  seeds <- runif(max_cores * n_reps, 1e6, 1e9)
  
  
  for (i in 1:nrow(parameters)) {
    for (r in 1:n_reps) {
      indices <- ((r - 1) * max_cores + 1):(r * max_cores)
      rows <- slice_sample(parameters[i, ], n = max_cores, replace = T) %>%
        mutate(random_seed = seeds[indices])
      # Don't go again if we already did this and have the result saved
      if (!is.null(runs) && 
          nrow(filter(
            runs, 
            n_decision %in% rows$n_decision & 
            weighted_sampling_mean %in% rows$weighted_sampling_mean &
            random_seed %in% rows$random_seed)) > 0
      )
        next()
      runs <- rbind(
        runs,
        rows %>% mutate(
          run = indices,
          groupRatio = runSimulations(rows, max_cores, summaryFun)
        )
      )
      save(runs, file = cacheFile)
    }
  }
  
  end_time <- Sys.time()
  print(paste0(
    'Completed ', n_reps * max_cores * nrow(parameters), ' models (',
    n_reps, ' reps using ', max_cores, ' cores of ', nrow(parameters), 
    ' sets of parameters)'
  ))
  end_time - start_time
  
  
  runs %>% 
    nest(d = c(groupRatio, run, random_seed)) %>%
    mutate(
      meanGroupRatio = map_dbl(d, ~mean(.$groupRatio)),
      meanEchoCamber = map_dbl(d, ~mean(.$groupRatio > 1.5)),
      leadIn = n_decision - 1000
    ) %>%
    select(
      starts_with('mean'), 
      leadIn, 
      everything(), 
      -d, 
      -n_decision, 
      -decision_flags
    )
  
  runs %>%
    mutate(
      leadIn = factor(n_decision - 1000),
      weighted_sampling_mean = if_else(
        is.na(weighted_sampling_mean), 
        0, 
        weighted_sampling_mean
      ),
      weighted_sampling_mean = factor(weighted_sampling_mean)
    ) %>%
    ggplot(aes(x = leadIn, y = groupRatio, fill = weighted_sampling_mean)) +
    geom_hline(yintercept = 1.5, linetype = 'dashed') +
    annotate(geom = 'label', label = 'Threshold', x = "0", y = 1.5) +
    geom_boxplot(outlier.alpha = .25) +
    scale_y_continuous(limits = c(0, NA)) +
    scale_fill_brewer(palette = 'Blues')
  
  Sys.time() - t1
}

```

Weighted sampling interacts with the lead-in time.
Lead-in time is a proxy for the starting asymmetry of the connections in the network.
The figure shows that, as networks start off with more homophilic structures, weighted selection increases the effect of homophily. 
Note that sufficiently homophilic networks will continue to exhibit echo chamber structures regardless of the level of weighted sampling, and that even very strong weighted sampling does not produce echo chambers in networks which start off perfectly symmetrical. 

The key finding of interest from the models thus far is that source selection may exacerbate echo chamber formation. 
This finding is in line with similar findings from others who have produced these models \mccorrect{!TODO[citations galore]}.
We are now in a position to see whether these models continue to exhibit these features when we parametrise them based on the data we observed in our participants' behavioural data.

## Participant data

Exploration of the model space shows that the models are sensitive to effects of source selection behaviour as parametrised by weighted sampling. 
This parameter in the model can be derived from the advisor choice behaviour in the behavioural experiments.
Other model parameters can also be fit using the participants' behavioural data. 

There are two ways in which parameters can be derived for the models from behavioural data. 
Both approaches start by taking the model used in simulation and finding the parameters for that model which best describe each participant's behaviour.
This is done using a standard parameter estimation approach based on a gradient descent algorithm.

Once parameter estimates have been obtained for all the participants, those parameters must be reconnected with the simulation side of the model. 
The first approach to doing this is to simply instantiate agents with exactly the properties of our participants.
By sampling a variety of models made up of different selections of participants, we can characterise some of the network behaviour that particular mixtures of individual strategies produce, at the expense of being tied explicitly to the performances of specific participants.

The second approach is to construct distributions in parameter space from the individual participants' specific parameter estimates. 
This approach allows for the generation of an unlimited number of unique participants, and can allow for correlations between parameter estimates as well as describing the estimates' distributions. 
The drawback to this approach is that the agents produced will be more homogeneous in their overall strategies.
For example, if a minority strategy exists within the population, this strategy should be evident in the first approach but would not be so in the second - instead we would simply have the parameter estimate distributions reflective of the dominant strategy slightly modified through averaging towards the minority strategy, which may in turn produce agents whose behaviour is not reflective of any plausible real individuals.

### Model fitting

