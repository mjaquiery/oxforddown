---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::html_document2: default
  bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
documentclass: book
---

# Network effects of interaction {#chapter-network-effects}
\adjustmtc 

## Method 

### Model details

### Parameters varied between model runs

There are three parameters which are varied between runs:
* The standard deviation of the agents' biases
* Whether or not there is time for the random network weights to update before biases update
* The agents' weighted selection values

## Results

```{r setup 03-01, echo = F, include = F}
source('scripts_and_filters/general_setup.R')
library(adviseR)
library(igraph)
library(BayesFactor)
library(magrittr)
library(parallel)
```

### Validity of the model

```{r basic models run}
n_reps <- 5
withr::with_seed(
  20210507,
  seeds <- c(20210507, floor(runif(n_reps - 1) * 1e8))
)
parameters <- tidyr::crossing(
  bias_volatility_mean = .01,
  bias_volatility_sd = .005,
  n_agents = 100,
  truth_sd = .5,
  sensitivity_mean = 3,
  sensitivity_sd = .5,
  random_seed = seeds,
  starting_graph = c(function(x) {
    m <- matrix(runif(nrow(x) ^ 2, .5), nrow(x), nrow(x))
    diag(m) <- 0
    m
  }),
  # starting_graph = .6,
  confidence_weighted = F,
  tibble(
    n_decision = c(3000, 4000),
    decision_flags = map(
      n_decision, 
      ~ if (. > 3000) {c(rep(1, 3000), rep(3, . - 3000))} else {rep(3, .)}
    )
  ),
  tidyr::crossing(
    tibble(
      trust_volatility_mean = .0112, # estimated from our empirical data
      trust_volatility_sd = 0
    ),
    bias_sd = c(1, 1.5),
    tibble(
      weighted_sampling_mean = c(0, 3.0, 3.0),
      weighted_sampling_sd = c(0, 0, 18.1)
    )
  )
)

summary_fun <- function(m) {
  .cc <- function(x, ...) adviseR::.cluster_count(x, k = 1:2, ...)
  # Calculate group ratios
  m$parameters$group_ratio_original <-
    adviseR::groupRatio(m$model$graphs[[length(m$model$graphs)]])
  m$parameters$group_ratio_empirical <-
    adviseR::groupRatio(m$model$graphs[[length(m$model$graphs)]], F)
  
  lastGen <- 
    m$model$agents[m$model$agents$decision == max(m$model$agents$decision), ]
  
  # Record mean final bias to determine which way the population is going
  m$parameters$mean_final_bias <- mean(
    m$model$agents$bias[m$model$agents$decision == max(m$model$agents$decision)]
  )
  
  # Calculate proportion of extremes
  x <- lastGen$bias
  m$parameters$n_lt_20 <- mean(x < .2)
  m$parameters$n_gt_80 <- mean(x > .8)
  m$parameters$n_mid <- mean(x >= .2 & x <= .8)
  
  # Calculate the bias magnitude
  m$parameters$meanBiasMagnitude <- mean(abs(lastGen$bias - .5)) 
  
  # Calculate the compression stats
  m$parameters$cluster_count <- .cc(
    m$model$graphs[[length(m$model$graphs)]]
  )
  # Save the summary stuff for the first example of each model
  library(tidyverse)
  m$model$agents <- m$model$agents %>%
    nest(d = -decision) %>%
    mutate(
      cluster_count = map_int(
        decision,
        function(i) {
          g <- m$model$graphs[[i]]
          if (all(is.na(g)))
            NA_integer_
          else
            .cc(g)
        }
      )
    ) %>%
    unnest(cols = d)
  
  m
}

nCores <- 3
for (i in 1:ceiling(nrow(parameters) / nCores)) {
  n <- (i - 1) * nCores + 1
  n <- c(n, min(n + nCores - 1, nrow(parameters)))
  fName <- glue('_cache/network-models-cuF/100a3000d_{n[1]}-{n[2]}.rda')
  if (file.exists(fName)) next()
  
  t1 <- Sys.time()
  models <- runSimulations(
    parameters[n[1]:n[2], ], 
    cores = nCores,
    summaryFun = summary_fun
  )
  save(models, file = fName)
  print(glue('Completed run {i} ({n[1]}-{n[2]}).'))
  print(Sys.time() - t1)
}
```

```{r basic models load}
t1 <- Sys.time()
fName <- '_cache/network-models-cuF.rda'
if (file.exists(fName)) {
  load(fName)
} else {
  # Gather data and bind into an analysable data frame
  agents.cuF <- NULL
  graphs.cuF <- list()
  fList <- list.files('_cache/network-models-cuF/', pattern = '', full.names = T)
  i <- 1
  for (f in fList) {
    load(f)
    for (m in models) {
      graphs.cuF[[i]] <- list()
      for (d in 1:m$parameters$n_decisions) {
        if ((d %% 100 == 1) | d == m$parameters$n_decisions) {
          graphs.cuF[[i]][[d]] <- m$model$graphs[[d]]
        }
      }
      
      m$parameters$model_num <- i
      i <- i + 1
      m$parameters$starting_graph <- NULL
      m$parameters$truth_fun <- NULL
      m$parameters$decision_flags <- NULL
      agents.cuF <- 
        bind_rows(
          agents.cuF, 
          m$model$agents %>% 
            filter(decision %% 100 == 1) %>%
            rename(agt_cluster_count = cluster_count) %>%
            left_join(as_tibble(m$parameters), by = character())
        )
    }
  }
  rm(m, models)
  save(agents.cuF, graphs.cuF, file = fName)
}
Sys.time() - t1
```

```{r basic models graphs}
# Plot graphs for example instances of each model
ns <- agents.cuF %>% 
  filter(random_seed == min(random_seed)) %>%
  select(model_num) %>%
  unique() %>%
  pull(model_num)

for (n in ns) {
  adviseR::networkGraph(
    list(model = list(graphs = graphs.cuF[[n]])),
    mark.groups = list(
      which(V(graphs.cuF[[n]][[1]])$bias <= .5),
      which(V(graphs.cuF[[n]][[1]])$bias > .5)
    )
  )
  
  tmp <- agents.cuF %>% filter(model_num == n)
  print(
    tmp %>%
      filter(decision %in% range(decision)) %>%
      transmute(
        model_num, 
        bias_sd, 
        ws_mean = weighted_sampling_mean,
        ws_sd = weighted_sampling_sd,
        burnIn = n_decisions > 3000,
        clusters = cluster_count, 
        mean_bias = round(meanBiasMagnitude, 3), 
        group_ratio = round(group_ratio_empirical, 3)
      ) %>%
      unique()
  )
  tmp <- list(
    model = list(agents.cuF = tmp),
    # hack back in the decision flags
    parameters = list(
      decision_flags = 
        if (tmp$n_decisions[1] == 3000) {
          rep(3, 3000)
        } else {
          c(rep(1, 3000), rep(3, 1000))
        }
    )
  )
  print(biasEvolution(tmp))
  
  # Strip out decision_flags for dropped decisions
  tmp$parameters$decision_flags <- 
    tmp$parameters$decision_flags[
      1:length(tmp$parameters$decision_flags) %in% tmp$model$agents$decision
    ]
  tmp$model$graphs <- graphs.cuF[[n]]
  print(weightEvolution(tmp, decisions = function(d) c(1, 1501, 2901, 3401, 3901)))
}

```


The model reproduces key effects of interest. 
Firstly, agents reinforce one another's biases, leading to the emergence of extreme biases.
Secondly, agents develop greater trust in agents who share their bias.

#### Emergence of extreme biases

* Where there is a burn-in time, the population usually polarises quickly to opposite extremes when biases are allowed to vary.
  * Without burn-in, the tendency is for the whole network to collapse on one or other extreme.
  * (Not shown) high levels of feedback can prevent this collapse.

#### Homophily

* Burn-in time also leads to more dichotomous final networks (captured by group ratio where that number is defined). 
  * This effect is most clearly seen where networks diverge after biases are allowed to vary.

#### Effect of weighted selection

* Weighted selection increases the likelihood agents select similarly-minded agents for their advisors (or dissimilarly-minded if the coefficient is negative).
  * Its effects on bias dynamics can be chaotic. 
  * Without burn-in it allows individuals, or small clusters, to resist an emerging consensus.
  * Where burn-in allows homophily to emerge before biases update, weighted selection accelerates polarisation.
  
### Consistency of the model

```{r basic model graphs consistency}
# Plot graphs for all instances of weighted sampling models to show heterogeneity
ns <- agents.cuF %>% 
  filter(
    bias_sd == 1.5,
    weighted_sampling_mean == 3,
    n_decisions == 3000
  ) %>%
  select(model_num) %>%
  unique() %>%
  pull(model_num)

for (n in ns) {
  adviseR::networkGraph(
    list(model = list(graphs = graphs.cuF[[n]])),
    mark.groups = list(
      which(V(graphs.cuF[[n]][[1]])$bias <= .5),
      which(V(graphs.cuF[[n]][[1]])$bias > .5)
    )
  )
  
  tmp <- agents.cuF %>% filter(model_num == n)
  print(
    tmp %>%
      filter(decision %in% range(decision)) %>%
      transmute(
        model_num, 
        bias_sd, 
        ws_mean = weighted_sampling_mean,
        ws_sd = weighted_sampling_sd,
        burnIn = n_decisions > 3000,
        clusters = cluster_count, 
        mean_bias = round(meanBiasMagnitude, 3), 
        group_ratio = round(group_ratio_empirical, 3)
      ) %>%
      unique()
  )
  tmp <- list(
    model = list(agents.cuF = tmp),
    # hack back in the decision flags
    parameters = list(
      decision_flags = 
        if (tmp$n_decisions[1] == 3000) {
          rep(3, 3000)
        } else {
          c(rep(1, 3000), rep(3, 1000))
        }
    )
  )
  print(biasEvolution(tmp))
  
  # Strip out decision_flags for dropped decisions
  tmp$parameters$decision_flags <- 
    tmp$parameters$decision_flags[
      1:length(tmp$parameters$decision_flags) %in% tmp$model$agents$decision
    ]
  tmp$model$graphs <- graphs.cuF[[n]]
  print(weightEvolution(tmp, decisions = function(d) c(1, 1501, 2901, 3401, 3901)))
}

```

Each model was run `r length(unique(agents$random_seed))` times with different random seeds to check which features were consistent across runs. 
The features described above were all consistent across runs.

For parameter sets in which all agents' biases tend towards the same extreme, which extreme was favoured varied according to the random seed used. 
This stochasticity is akin to placing a ball on a gabled roof: tiny variations in the initial conditions affect which way it will roll, but it will always roll down one pitch or the other.

Similarly, in some of these models, adding in weighted selection switches which bias extreme is adopted.
This effect is not consistent between runs with different random seeds, and is an outsized effect of minor differences to early states, analogous to a breath of wind nudging the ball in the previous example one way or another.

### Using empirically estimated coefficients

Now we have a reasonable understanding of how the models behave, we can examine the effects of drawing parameter values for trust volatility and weighted selection directly from the parameter estimation approach used in \mccorrect{!TODO[link to chapter wherever we did parameter estimates]}.
Using parameters estimated from actual individuals has both strengths and weaknesses. 
The strengths are that the values represent genuine best-estimates of both trust volatility and weighted selection. 
Given that these two parameters are related to one another, with weighted selection being dependent on trust volatility, it may be important to use observations of both simultaneously to appropriately model individuals' behaviour.
The weaknesses are that the task given to human participants differed in potentially important ways from the task modelled in the agent-based model. 
The most potentially important difference in this respect is the choice of advisor: in the behavioural experiments the human participants were familiarised with two advisors and then given the choice between them; whereas in the model the agents are picking from a large number of potential advisors. 
The parameter is estimated on the basis of a sigmoid function applied to the trust difference between advisors, whereas it is used in the agent-based models in a half-sigmoid applied to the difference between each advisor and the most trusted advisor. 

These considerations mean that the model dynamics arising from using estimated coefficients may be informative, but only in an illustrative capacity. 
Too much differs between the behavioural and simulated situations to draw strong conclusions.

```{r empirical models run}
load('_cache/thesis-parameter-estimation.rda')

# Fetch the unique parameter combinations for the models we just ran
parameters.cuF.emp <- agents.cuF %>%
  select(
    n_agents:confidence_weighted,
    -starts_with('trust_volatility'),
    -starts_with('weighted_sampling'),
    -starting_graph_type
  ) %>%
  unique() %>%
  rowid_to_column() %>%
  nest(d = -rowid) %>%
  mutate(
    model = map(d, function(.) {
      withr::with_seed(
        .$.random_seed_agents,
        do.call(adviseR:::makeAgents, select(
          ., 
          -bias_update_slope, -starts_with('feedback'), 
          -contains('random_seed'), -truth_sd, -confidence_weighted
        ))
      )
    })
  ) %>%
  # Substitute trust_volatility and weighted_sampling coefficients from estimates
  mutate(
    model = map2(model, d, function(.x, .y) {
      p <- withr::with_seed(
        .y$.random_seed_agents,
        slice_sample(recovered_parameters, n = .y$n_agents, replace = T)
      )
      .x$agents <- .x$agents %>%
        mutate(
          trust_volatility = rep(p$tu_end, .y$n_decisions),
          weighted_sampling = rep(p$ws_end, .y$n_decisions)
        )
      .x
    })
  ) %>% 
  unnest(cols = d) %>%
  mutate(
    decision_flags = map(
      n_decisions,
      function(.) {
        if (. > 3000) c(rep(1, 3000), rep(3, . - 3000)) else rep(3, .)
      }
    ),
    starting_graph = parameters$starting_graph[1],
    model = map2(model, n_agents, function(.x, .y) {
      g <- parameters$starting_graph[[1]](tibble(x = 1:.y))
      .x$graphs <- list(g)
      .x
    })
  ) %>%
  select(-rowid)

nCores <- 3
for (i in 1:ceiling(nrow(parameters.cuF.emp) / nCores)) {
  n <- (i - 1) * nCores + 1
  n <- c(n, min(n + nCores - 1, nrow(parameters.cuF.emp)))
  fName <- glue('_cache/network-models-cuF-emp/100a3000d_{n[1]}-{n[2]}.rda')
  if (file.exists(fName)) next()
  
  t1 <- Sys.time()
  models <- runSimulations(
    parameters.cuF.emp[n[1]:n[2], ], 
    cores = nCores,
    summaryFun = summary_fun
  )
  save(models, file = fName)
  print(glue('Completed run {i} ({n[1]}-{n[2]}).'))
  print(Sys.time() - t1)
}
```

```{r empirical models load}
t1 <- Sys.time()
fName <- '_cache/network-models-cuF-emp.rda'
if (file.exists(fName)) {
  load(fName)
} else {
  # Gather data and bind into an analysable data frame
  agents.cuF.emp <- NULL
  graphs.cuF.emp <- list()
  fList <- list.files('_cache/network-models-cuF-emp/', pattern = '', full.names = T)
  i <- 1
  for (f in fList) {
    load(f)
    for (m in models) {
      graphs.cuF.emp[[i]] <- list()
      for (d in 1:m$parameters$n_decisions) {
        if ((d %% 100 == 1) | d == m$parameters$n_decisions) {
          graphs.cuF.emp[[i]][[d]] <- m$model$graphs[[d]]
        }
      }
      
      m$parameters$model_num <- i
      i <- i + 1
      m$parameters$starting_graph <- NULL
      m$parameters$truth_fun <- NULL
      m$parameters$decision_flags <- NULL
      agents.cuF.emp <- 
        bind_rows(
          agents.cuF.emp, 
          m$model$agents %>% 
            filter(decision %% 100 == 1) %>%
            rename(agt_cluster_count = cluster_count) %>%
            left_join(as_tibble(m$parameters), by = character())
        )
    }
  }
  rm(m, models)
  save(agents.cuF.emp, graphs.cuF.emp, file = fName)
}
Sys.time() - t1
```

```{r empirical models graphs}
# Plot graphs for example instances of each model
ns <- agents.cuF.emp %>% 
  filter(random_seed == min(random_seed)) %>%
  select(model_num) %>%
  unique() %>%
  pull(model_num)

for (n in ns) {
  adviseR::networkGraph(
    list(
      model = list(graphs = graphs.cuF.emp[[n]]), 
      parameters = list(
        n_agents = agents.cuF.emp %>% 
          filter(model_num == n) %>% 
          pull(n_agents) %>%
          .[1]
      )
    ),
    mark.groups = list(
      which(V(graphs.cuF.emp[[n]][[1]])$bias <= .5),
      which(V(graphs.cuF.emp[[n]][[1]])$bias > .5)
    )
  )
  
  tmp.emp <- agents.cuF.emp %>% filter(model_num == n)
  print(
    tmp.emp %>%
      filter(decision %in% range(decision)) %>%
      transmute(
        model_num, 
        bias_sd, 
        ws_mean = weighted_sampling_mean,
        ws_sd = weighted_sampling_sd,
        burnIn = n_decisions > 3000,
        clusters = cluster_count, 
        mean_bias = round(meanBiasMagnitude, 3), 
        group_ratio = round(group_ratio_empirical, 3)
      ) %>%
      unique()
  )
  tmp.emp <- list(
    model = list(agents = tmp.emp),
    # hack back in the decision flags
    parameters = list(
      decision_flags = 
        if (tmp.emp$n_decisions[1] == 3000) {
          rep(3, 3000)
        } else {
          c(rep(1, 3000), rep(3, 1000))
        }
    )
  )
  print(biasEvolution(tmp.emp))
  
  # Strip out decision_flags for dropped decisions
  tmp.emp$parameters$decision_flags <- 
    tmp.emp$parameters$decision_flags[
      1:length(tmp.emp$parameters$decision_flags) %in% tmp.emp$model$agents$decision
    ]
  tmp.emp$model$graphs <- graphs.cuF.emp[[n]]
  print(weightEvolution(tmp.emp, decisions = function(d) c(1, 1501, 2901, 3401, 3901)))
}

```

#### Descriptive observations of individual models

As with the simulations with heterogeneous weighted sampling values, some agents resist the consensus extreme. 
As compared to those simulations, though, more agents occupy a middle-ground, and can cross more easily from one extreme to another.
Where a burn-in period is present, a large proportion of the agents occupy unstable middle-ground positions, preventing clear polarisation or consensus in the population.

### Combined output with previous

```{r all models summary}

mdls.cuF.all <- bind_rows(
  agents.cuF %>% 
    group_by(model_num) %>%
    summarise(across(n_agents:cluster_count, unique), .groups = 'drop') %>%
    mutate(
      burnIn = n_decisions > 3000,
      weighted_sampling = glue(
        '{ifelse(nchar(weighted_sampling_mean) == 1, "0", "")}{weighted_sampling_mean}',
        ' ({weighted_sampling_sd})'
      )
    ) %>%
    select(
      model_num:starting_graph_type,
      weighted_sampling,
      burnIn,
      everything(),
      -n_decisions, 
      -weighted_sampling_mean, 
      -weighted_sampling_sd
    ),
  agents.cuF.emp %>% 
    mutate(model_num = model_num + max(agents.cuF$model_num)) %>%
    group_by(model_num) %>%
    summarise(across(n_agents:cluster_count, unique), .groups = 'drop') %>%
    mutate(
      burnIn = n_decisions > 3000,
      weighted_sampling = 'emp'
    ) %>%
    select(
      model_num:starting_graph_type,
      weighted_sampling,
      burnIn,
      everything(),
      -n_decisions, 
      -weighted_sampling_mean, 
      -weighted_sampling_sd
    )
) %>%
  select(-starts_with('trust_volatility'))

for (v in names(select(mdls.cuF.all, n_agents:burnIn))) {
  if (length(unique(pull(mdls.cuF.all, v))) < 2)
    next()
  print(
    mdls.cuF.all %>% 
      mutate(grp = glue(
        '{',
        paste0(
          names(
            select(
              mdls.cuF.all, 
              n_agents:random_seed, 
              -!!sym(v)
            )
          ), 
          collapse = "}|{"
        ),
        '}'
      )) %>%
      pivot_longer(
        c(meanBiasMagnitude, cluster_count, starts_with('group_ratio')), 
        names_to = 'outcome'
      ) %>%
      mutate(across(-value, factor)) %>%
      ggplot(aes(y = value, x = !!sym(v), colour = weighted_sampling == 'emp')) + 
      geom_violin(alpha = .1, fill = 'black', colour = NA) +
      geom_line(aes(group = grp), alpha = .25, position = position_jitter(.1, .01)) +
      facet_wrap(~outcome, scales = 'free_y')
  )
}
```

Having considered the patterns at the level of individual models, we can draw more useful conclusions at a higher level from considering them as a set. 
In this analysis we attend to two properties of primary interest: 
First, how accurate are agents over time? 
This is represented by the mean bias magnitude. 
In ideal circumstances, this should be very low, indicating that agents are largely unbiased. 
Second, to what extent are agents more likely to trust advice from agents who share their bias?
This is captured by the ratio between trust in agents with the same versus different bias. 
This value is not always well defined - where all agents eventually acquire the same bias, the mean weight of out-group advisors is undefined.

Summarising the results in this way means that we can observe some general features of the models by comparing otherwise-similar models over the different parameters.

A more varied range of biases in the population has relatively little consistent effect on mean bias magnitude, but does lead to higher group ratios. 
This indicates that models more readily split into polarised camps with echo-chambers where there are more extreme biases present in the population.

Mean bias magnitude is less pronounced where there is a burn-in time, although this is likely due to there being less time after burn-in for the biases to update. 
Burn-in time tends to decrease the homophily of the models, judging by final bias, but to increase it judging by initial bias. 
This suggests that polarisation occurs, but that many middle-ground agents change which camp they end up in.

Weighted selection has little effect where the mean is increased, but a clearer effect where variation is introduced. 
The clustering of weights suggests that there is less homophily where agents are more selective about whom they consult for advice. 

Weighted sampling interacts with the lead-in time.
Lead-in time is a proxy for the starting asymmetry of the connections in the network.
The figure shows that, as networks start off with more homophilic structures, weighted selection increases the effect of homophily. 
Note that sufficiently homophilic networks will continue to exhibit echo chamber structures regardless of the level of weighted sampling, and that even very strong weighted sampling does not produce echo chambers in networks which start off perfectly symmetrical. 

The key finding of interest from the models thus far is that source selection may exacerbate echo chamber formation. 
This finding is in line with similar findings from others who have produced these models \mccorrect{!TODO[citations galore]}.
We are now in a position to see whether these models continue to exhibit these features when we parametrise them based on the data we observed in our participants' behavioural data.

##### Bias x TrustUpdate

```{r}

agents.cuF.emp %>% 
  filter(
    decision == max(decision),
    abs(trust_volatility) < .2
  ) %>%
  ggplot(aes(x = trust_volatility, y = abs(bias - .5))) +
  geom_smooth(method = 'lm', formula = y ~ x) + 
  geom_point(alpha = .25)

agents.cuF.emp %>% 
  filter(
    decision == max(decision),
    abs(weighted_sampling) < 55
  ) %>%
  ggplot(aes(x = weighted_sampling, y = abs(bias - .5))) +
  geom_smooth(method = 'lm', formula = y ~ x) + 
  geom_point(alpha = .25)

```

## Participant data

Exploration of the model space shows that the models are sensitive to effects of source selection behaviour as parametrised by weighted sampling. 
This parameter in the model can be derived from the advisor choice behaviour in the behavioural experiments.
Other model parameters can also be fit using the participants' behavioural data. 

There are two ways in which parameters can be derived for the models from behavioural data. 
Both approaches start by taking the model used in simulation and finding the parameters for that model which best describe each participant's behaviour.
This is done using a standard parameter estimation approach based on a gradient descent algorithm.

Once parameter estimates have been obtained for all the participants, those parameters must be reconnected with the simulation side of the model. 
The first approach to doing this is to simply instantiate agents with exactly the properties of our participants.
By sampling a variety of models made up of different selections of participants, we can characterise some of the network behaviour that particular mixtures of individual strategies produce, at the expense of being tied explicitly to the performances of specific participants.

The second approach is to construct distributions in parameter space from the individual participants' specific parameter estimates. 
This approach allows for the generation of an unlimited number of unique participants, and can allow for correlations between parameter estimates as well as describing the estimates' distributions. 
The drawback to this approach is that the agents produced will be more homogeneous in their overall strategies.
For example, if a minority strategy exists within the population, this strategy should be evident in the first approach but would not be so in the second - instead we would simply have the parameter estimate distributions reflective of the dominant strategy slightly modified through averaging towards the minority strategy, which may in turn produce agents whose behaviour is not reflective of any plausible real individuals.

### Model fitting

