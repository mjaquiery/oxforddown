---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::html_document2: default
  bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
documentclass: book
---

# Network effects of interaction {#chapter-network-effects}
\adjustmtc 

```{r setup 03-01, echo = F, include = F}
source('scripts_and_filters/general_setup.R')
library(adviseR)
library(igraph)
library(BayesFactor)
library(magrittr)
library(parallel)
```

```{r basic models run}
n_reps <- 5
withr::with_seed(
  20210507,
  seeds <- c(20210507, floor(runif(n_reps - 1) * 1e8))
)
parameters <- tidyr::crossing(
  bias_volatility_mean = .01,
  bias_volatility_sd = .005,
  n_agents = 100,
  truth_sd = .5,
  sensitivity_mean = 3,
  sensitivity_sd = .5,
  random_seed = seeds,
  starting_graph = c(function(x) {
    m <- matrix(runif(nrow(x) ^ 2, .5), nrow(x), nrow(x))
    diag(m) <- 0
    m
  }),
  # starting_graph = .6,
  confidence_weighted = F,
  tibble(
    n_decision = c(3000, 4000),
    decision_flags = map(
      n_decision, 
      ~ if (. > 3000) {c(rep(1, 3000), rep(3, . - 3000))} else {rep(3, .)}
    )
  ),
  tidyr::crossing(
    tibble(
      trust_volatility_mean = .0112, # estimated from our empirical data
      trust_volatility_sd = 0
    ),
    bias_sd = c(1, 1.5),
    tibble(
      weighted_sampling_mean = c(0, 3.0, 3.0),
      weighted_sampling_sd = c(0, 0, 18.1)
    )
  )
)

summary_fun <- function(m) {
  .cc <- function(x, ...) adviseR::.cluster_count(x, k = 1:2, ...)
  # Calculate group ratios
  m$parameters$group_ratio_original <-
    adviseR::groupRatio(m$model$graphs[[length(m$model$graphs)]])
  m$parameters$group_ratio_empirical <-
    adviseR::groupRatio(m$model$graphs[[length(m$model$graphs)]], F)
  
  lastGen <- 
    m$model$agents[m$model$agents$decision == max(m$model$agents$decision), ]
  
  # Record mean final bias to determine which way the population is going
  m$parameters$mean_final_bias <- mean(
    m$model$agents$bias[m$model$agents$decision == max(m$model$agents$decision)]
  )
  
  # Calculate proportion of extremes
  x <- lastGen$bias
  m$parameters$n_lt_20 <- mean(x < .2)
  m$parameters$n_gt_80 <- mean(x > .8)
  m$parameters$n_mid <- mean(x >= .2 & x <= .8)
  
  # Calculate the bias magnitude
  m$parameters$meanBiasMagnitude <- mean(abs(lastGen$bias - .5)) 
  
  # Calculate the compression stats
  m$parameters$cluster_count <- .cc(
    m$model$graphs[[length(m$model$graphs)]]
  )
  # Save the summary stuff for the first example of each model
  library(tidyverse)
  m$model$agents <- m$model$agents %>%
    nest(d = -decision) %>%
    mutate(
      cluster_count = map_int(
        decision,
        function(i) {
          g <- m$model$graphs[[i]]
          if (all(is.na(g)))
            NA_integer_
          else
            .cc(g)
        }
      )
    ) %>%
    unnest(cols = d)
  
  m
}

nCores <- 3
for (i in 1:ceiling(nrow(parameters) / nCores)) {
  n <- (i - 1) * nCores + 1
  n <- c(n, min(n + nCores - 1, nrow(parameters)))
  fName <- glue('_cache/network-models-cuF/100a3000d_{n[1]}-{n[2]}.rda')
  if (file.exists(fName)) next()
  
  t1 <- Sys.time()
  models <- runSimulations(
    parameters[n[1]:n[2], ], 
    cores = nCores,
    summaryFun = summary_fun
  )
  save(models, file = fName)
  print(glue('Completed run {i} ({n[1]}-{n[2]}).'))
  print(Sys.time() - t1)
}
```

```{r basic models load}
t1 <- Sys.time()
fName <- '_cache/network-models-cuF.rda'
if (file.exists(fName)) {
  load(fName)
} else {
  # Gather data and bind into an analysable data frame
  agents.cuF <- NULL
  graphs.cuF <- list()
  fList <- list.files('_cache/network-models-cuF/', pattern = '', full.names = T)
  i <- 1
  for (f in fList) {
    load(f)
    for (m in models) {
      graphs.cuF[[i]] <- list()
      for (d in 1:m$parameters$n_decisions) {
        if ((d %% 100 == 1) | d == m$parameters$n_decisions) {
          graphs.cuF[[i]][[d]] <- m$model$graphs[[d]]
        }
      }
      
      m$parameters$model_num <- i
      i <- i + 1
      m$parameters$starting_graph <- NULL
      m$parameters$truth_fun <- NULL
      m$parameters$decision_flags <- NULL
      agents.cuF <- 
        bind_rows(
          agents.cuF, 
          m$model$agents %>% 
            filter(decision %% 100 == 1) %>%
            rename(agt_cluster_count = cluster_count) %>%
            left_join(as_tibble(m$parameters), by = character())
        )
    }
  }
  rm(m, models)
  save(agents.cuF, graphs.cuF, file = fName)
}
Sys.time() - t1
```

## Method 

Agent-based modelling is used to simulate the interactive effects of repeated paired decision-making.
The agents in the model perform a cycle of making an initial decision, selecting one of the other agents as an advisor, making a final decision, and updating their trust in their advisor and their internal beliefs about the world.
The task the agents face is roughly similar to the task faced by human participants in the Dots Task experiments [Â§Perceptual decision (Dots Task)](#m-p-dots), where the participants make a decision about which of two rapidly and simultaneously presented grids contained more dots.

Each model consists of a population of agents implementing the same mathematical model of decision-making, trust-updating, and advisor selection, with coefficients for parameters of that mathematical model drawn from appropriate distributions. 

### Model details

Each model is defined by a set of model parameters, which are assigned capital letters in the equations below. 
These parameters are used to generate further parameters which govern agents' tendencies, and these are assigned lower-case letters and superscripted with the agent to whom they belong. 
Selections from distributions are indicated by a subscript showing which step they belong to.

This is best illustrated with an example. 
Each agent has a sensitivity parameter, $s^a$ which governs the amount of random noise which is attached to their perception of the world.
On any given decision, the amount of this noise $\epsilon^a_t$ is determined by drawing from a normal distribution with standard deviation defined by the agent's sensitivity ($N(0, \frac{1}{s^a})$).
When the agent is created, the parameter defining its sensitivity distribution is itself drawn from a normal distribution with mean and standard deviation defined by the model settings ($N(S^\mu, S^\sigma)$).

#### Creating agents

Each agent has the following properties:

```{r agent property table}

tribble(
  ~ `Property`, ~ `Description`, ~ `Updates each step`,
  "$a$", "Agent unique identfier", "",
  "$s^a$", "Accuracy of agent's perceptions", "",
  "$c^a$", "Agent's subjective confidence scaling", "",
  "$\\tau^a$", "Size of agent's trust updates", "",
  "$\\lambda^a$", "Size of agent's bias updates", "",
  "$w^a$", "Extent of agent's preference for trusted advisors", "",
  "$b^a_t$", "Agent's prior expectation about the task answer", "Yes"
) %>%
  kable(caption = "\\label{tab:agent-properties}Agent properties")

```

An agent's values for each of these properties (except id) are created by drawing values from normal distributions defined by parameters in the model settings.

Additionally, each agent has a 'trust' in each other agent, ranging from being convinced that the other agent is always correct to being convinced that the other agent is always wrong.
These values are updated at each step and initialised by drawing from a uniform random distribution with limits $[0.5, 1]$.

#### Model step

##### Establishing the stimulus

The same stimulus is presented to each agent, in the form of a value ($v_t$) drawn from a normal distribution with mean ($V^\mu$) and standard deviation ($V^\sigma$) defined in the model settings:

$$v_t \sim N(V^\mu, V^\sigma)$$

The agents' task is to determine whether $v_t$ is greater than or less than zero.

##### Initial decision-making

Agents perform a noisy perception of the stimulus, by combining the true stimulus value with random noise ($\epsilon^a_t$), to produce a percept $q^a_t$.

$$q^a_t = v_t + \epsilon^a_t$$
Where: $\epsilon^a_t \sim N(0, \frac{1}{s^a})$

This sensory percept is then converted into a subjective probability that the stimulus was greater than zero ($p^a_t$) using a sigmoid function with a slope defined by the agent's subjective confidence scaling parameter.

$$p^a_t = \varsigma(q^a_t, c^a)$$
Where: $\varsigma(x, y) = \frac{1}{1 + e^{-xy}}$

This subjective probability is then integrated with the agent's prior expectation about whether stimuli are generally less than or greater than 0 ($b^a_t$), using Bayes' rule, to produce an initial decision, $i^a_t$.

$$i^a_t = \frac{P(v_t<0|p^a_t)}{P(v_t<0|p^a_t) \cdot P(v_t>0|p^a_t)}$$

Where: $P(v_t < 0 | p^a_t) = b^a_t \cdot z(p^a_t, 1, V^\sigma)$;
and: $P(v_t > 0 | p^a_t) = (1 - b^a_t) \cdot z(p^a_t, 0, V^\sigma)$;
with: $z(x, \mu, \sigma)$ giving the density of $N(\mu, \sigma)$ at $x$.

Note that in these initial decision equations the agents have direct access to a model property, $V^\sigma$, the standard deviation of the true values of the stimuli. 
Ideally, in an agent-based model, the agents would not have such direct access to non-observable properties, and would instead build up specific expectations about the variability of stimuli from observation, perhaps seeded with a loosely-informative prior. 
The agents are allowed to know the value here as a shorthand for such exploration. 
This makes the agents' task analogous to well-practised real-world tasks such as perceptual decision-making.
It is unlikely to seriously affect any conclusions or illustrations drawn from the models.

The initial decision contains both a discrete decision (whether the stimulus value was more likely to be less than zero, $i^a_t < 0.5$, or greater than zero $i^a_t \geq 0.5$), and how much so ($|i^a_t - 0.5|$). 
It thus represents both the agent's decision and the confidence in that decision.

##### Advisor selection

Having made an initial decision, each agent selects a single advisor from whom to receive advice.
This choice is made based on the trust in each potential advisor scaled by the strength of the agent's preference for receiving more-trusted advice.

The identity of an agent's advisor on a given trial is designated by $a'$, and the weight assigned to advisor $a'$ by agent $a$ at step $t$ by $\omega^{a,a'}_t$.
Each agent's trust value is adjusted to be relative to the most (or least) trusted advisor, depending upon whether the agent prefers trusted ($w^a > 0$) or untrusted ($w^a \leq 0$) advisors.

$$\omega' ^{a,a'} = 
\begin{cases}
w^a > 0, \omega^{a,a'}_t - \text{max}(\Omega^a_t)\\
w^a \leq 0, \omega^{a,a'}_t - \text{min}(\Omega^a_t)
\end{cases}$$

Where $\Omega^a_t$ is the set of trust weights in all potential advisors for agent $a$ at step $t$.

These relative trust values are then assigned probability weightings for selection based on a sigmoid function.
Because of the relative scaling, each probability is in fact drawn from a half sigmoid, with the probability weight assigned to the most likely candidate is 1, and other candidates' probability weights between 1 and 0.

The identity of advisor $^{a,a'}_t$ is determined by sampling at random from the advisors, weighted by the probability weights assigned.

###### Differences from parameter estimation

The approach used for fitting participant data from the behavioural experiments is subtly different from that used for advisor selection in the agent-based models.
In the behavioural experiments, rather than being presented with a choice of many different advisors whose appeals all had to be considered, participants were presented with a choice of two advisors only. 
The parameter recovery process estimated a trust update rate ($\tau^a$ in the agent-based model) which tracked the trust in each advisor, and the difference between these trust values was fed into a sigmoid governing selection.
The slope of that best-fitting sigmoid function was taken as equivalent to $w^a$ in the agent-based models.
This approach is reasonable given the differences in the advisor selection task facing the human participants and model agents, but should be noted as a caveat for drawing interpretations concerning the role of weighted selection values based on human participants' performance.

##### Final decision-making

Final decisions are made by Bayesian integration of the initial decision and advice.
Advice takes the form of a binary recommendation, and is weighted by the trust the agent has in their advisor.

First, initial decisions and advice are reoriented to the direction of the initial decision, such that initial decisions represent confidence in the initial decision and advice represents agreement with that decision.

$$i'~^a_t = 
\begin{cases}
i < 0.5, 1 - i^a_t \\
i \geq 0.5, i^a_t
\end{cases}$$

$$i'~^{a,a'}_t = 
\begin{cases}
i < 0.5, \text{round}(1 - i^{a,a'}_t) \\
i \geq 0.5, \text{round}(i^{a,a'}_t)
\end{cases}$$

The trust weight is slightly truncated to avoid very extreme values: 
$$ = $$
$$\omega'~^{a,a'}_t = 
\begin{cases}
i'~^{a,a'}_t = 0, 1 - \text{min}(0.95, \text{max}(0.05, \omega^{a,a'}_t)) \\
i'~^{a,a'}_t = 1, \text{min}(0.95, \text{max}(0.05, \omega^{a,a'}_t))
\end{cases}$$

The derived trust weight $\omega'~^{a,a'}_t$ now represents the probability that the advice will agree with the initial decision, given the initial decision is correct.

The final decision is obtained by performing Bayesian integration.
In Bayesian terms, we are trying to discover the probability of our final answer being correct given the agreement (or disagreement) observed from the advisor. 
Thus we are multiplying the initial probability of being correct (our subjective confidence, $i'~^a_t$) by the probability of the advisor agreeing if we are correct ($w'~^{a,a'}_t$).
This is divided by all of the options that could have led to the observed advice: the probability that we are correct multiplied by the probability of the advice if we are (the numerator), plus the probability that we are incorrect multiplied by the probability of the advice if we are incorrect. 
Because correctness and the advice are both mutually exclusive binaries, the probability of being incorrect is 1 - the probability of being correct, and the probability of the advice if we are incorrect is 1 - the probability of the advice if we are correct.

$$f'~^a_t = \frac{i'~a_t \cdot \omega'~^{a,a'}_t}{i'~a_t \cdot \omega'~^{a,a'}_t + (1 - i'~a_t)(1 - \omega'~^{a,a'}_t)}$$
and the final decision is taken by reversing the transformation applied earlier:
$$f~^a_t = 
\begin{cases}
i < 0.5, 1 - f'~^a_t \\
i \geq 0.5, f'~^a_t
\end{cases}$$

###### Feedback

##### Trust updating

##### Bias updating

### Model parameters

The models have a large number of parameters. 
Many of these are used to create the agents.

#### Parameters varied between model runs

There are three parameters which are varied between runs:
* The standard deviation of the agents' biases
* Whether or not there is time for the random network weights to update before biases update
* The agents' weighted selection values

## Results

### Validity of the model



```{r basic models graphs}
# Plot graphs for example instances of each model
ns <- agents.cuF %>% 
  filter(random_seed == min(random_seed)) %>%
  select(model_num) %>%
  unique() %>%
  pull(model_num)

for (n in ns) {
  adviseR::networkGraph(
    list(model = list(graphs = graphs.cuF[[n]])),
    mark.groups = list(
      which(V(graphs.cuF[[n]][[1]])$bias <= .5),
      which(V(graphs.cuF[[n]][[1]])$bias > .5)
    )
  )
  
  tmp <- agents.cuF %>% filter(model_num == n)
  print(
    tmp %>%
      filter(decision %in% range(decision)) %>%
      transmute(
        model_num, 
        bias_sd, 
        ws_mean = weighted_sampling_mean,
        ws_sd = weighted_sampling_sd,
        burnIn = n_decisions > 3000,
        clusters = cluster_count, 
        mean_bias = round(meanBiasMagnitude, 3), 
        group_ratio = round(group_ratio_empirical, 3)
      ) %>%
      unique()
  )
  tmp <- list(
    model = list(agents.cuF = tmp),
    # hack back in the decision flags
    parameters = list(
      decision_flags = 
        if (tmp$n_decisions[1] == 3000) {
          rep(3, 3000)
        } else {
          c(rep(1, 3000), rep(3, 1000))
        }
    )
  )
  print(biasEvolution(tmp))
  
  # Strip out decision_flags for dropped decisions
  tmp$parameters$decision_flags <- 
    tmp$parameters$decision_flags[
      1:length(tmp$parameters$decision_flags) %in% tmp$model$agents$decision
    ]
  tmp$model$graphs <- graphs.cuF[[n]]
  print(weightEvolution(tmp, decisions = function(d) c(1, 1501, 2901, 3401, 3901)))
}

```

The model reproduces key effects of interest. 
Firstly, agents reinforce one another's biases, leading to the emergence of extreme biases.
Secondly, agents develop greater trust in agents who share their bias.

#### Emergence of extreme biases

* Where there is a burn-in time, the population usually polarises quickly to opposite extremes when biases are allowed to vary.
  * Without burn-in, the tendency is for the whole network to collapse on one or other extreme.
  * (Not shown) high levels of feedback can prevent this collapse.

#### Homophily

* Burn-in time also leads to more dichotomous final networks (captured by group ratio where that number is defined). 
  * This effect is most clearly seen where networks diverge after biases are allowed to vary.

#### Effect of weighted selection

* Weighted selection increases the likelihood agents select similarly-minded agents for their advisors (or dissimilarly-minded if the coefficient is negative).
  * Its effects on bias dynamics can be chaotic. 
  * Without burn-in it allows individuals, or small clusters, to resist an emerging consensus.
  * Where burn-in allows homophily to emerge before biases update, weighted selection accelerates polarisation.
  
### Consistency of the model

```{r basic model graphs consistency}
# Plot graphs for all instances of weighted sampling models to show heterogeneity
ns <- agents.cuF %>% 
  filter(
    bias_sd == 1.5,
    weighted_sampling_mean == 3,
    n_decisions == 3000
  ) %>%
  select(model_num) %>%
  unique() %>%
  pull(model_num)

for (n in ns) {
  adviseR::networkGraph(
    list(model = list(graphs = graphs.cuF[[n]])),
    mark.groups = list(
      which(V(graphs.cuF[[n]][[1]])$bias <= .5),
      which(V(graphs.cuF[[n]][[1]])$bias > .5)
    )
  )
  
  tmp <- agents.cuF %>% filter(model_num == n)
  print(
    tmp %>%
      filter(decision %in% range(decision)) %>%
      transmute(
        model_num, 
        bias_sd, 
        ws_mean = weighted_sampling_mean,
        ws_sd = weighted_sampling_sd,
        burnIn = n_decisions > 3000,
        clusters = cluster_count, 
        mean_bias = round(meanBiasMagnitude, 3), 
        group_ratio = round(group_ratio_empirical, 3)
      ) %>%
      unique()
  )
  tmp <- list(
    model = list(agents.cuF = tmp),
    # hack back in the decision flags
    parameters = list(
      decision_flags = 
        if (tmp$n_decisions[1] == 3000) {
          rep(3, 3000)
        } else {
          c(rep(1, 3000), rep(3, 1000))
        }
    )
  )
  print(biasEvolution(tmp))
  
  # Strip out decision_flags for dropped decisions
  tmp$parameters$decision_flags <- 
    tmp$parameters$decision_flags[
      1:length(tmp$parameters$decision_flags) %in% tmp$model$agents$decision
    ]
  tmp$model$graphs <- graphs.cuF[[n]]
  print(weightEvolution(tmp, decisions = function(d) c(1, 1501, 2901, 3401, 3901)))
}

```

Each model was run `r length(unique(agents$random_seed))` times with different random seeds to check which features were consistent across runs. 
The features described above were all consistent across runs.

For parameter sets in which all agents' biases tend towards the same extreme, which extreme was favoured varied according to the random seed used. 
This stochasticity is akin to placing a ball on a gabled roof: tiny variations in the initial conditions affect which way it will roll, but it will always roll down one pitch or the other.

Similarly, in some of these models, adding in weighted selection switches which bias extreme is adopted.
This effect is not consistent between runs with different random seeds, and is an outsized effect of minor differences to early states, analogous to a breath of wind nudging the ball in the previous example one way or another.

### Using empirically estimated coefficients

Now we have a reasonable understanding of how the models behave, we can examine the effects of drawing parameter values for trust volatility and weighted selection directly from the parameter estimation approach used in \mccorrect{!TODO[link to chapter wherever we did parameter estimates]}.
The weighted selection values used above are taken from distributions defined by the estimated values for the population, which allows for the generation of an unlimited number of unique participants, but means that the agents produced will be more homogeneous in their overall strategies.
For example, if a minority strategy exists within the population, we would see the parameter estimate distributions reflective of the dominant strategy slightly modified through averaging towards the minority strategy, which may in turn produce agents whose behaviour is not reflective of any plausible real individuals.

Using parameters estimated from actual individuals instead of drawing from a distribution derived form those values has both strengths and weaknesses. 
The strengths are that the values represent genuine best-estimates of both trust volatility and weighted selection. 
Given that these two parameters are related to one another, with weighted selection being dependent on trust volatility, it may be important to use observations of both simultaneously to appropriately model individuals' behaviour.
The weaknesses are that the task given to human participants differed in potentially important ways from the task modelled in the agent-based model. 
The most potentially important difference in this respect is the choice of advisor: in the behavioural experiments the human participants were familiarised with two advisors and then given the choice between them; whereas in the model the agents are picking from a large number of potential advisors. 
The parameter is estimated on the basis of a sigmoid function applied to the trust difference between advisors, whereas it is used in the agent-based models in a half-sigmoid applied to the difference between each advisor and the most trusted advisor. 

These considerations mean that the model dynamics arising from using estimated coefficients may be informative, but only in an illustrative capacity. 
Too much differs between the behavioural and simulated situations to draw strong conclusions.

```{r empirical models run}
load('_cache/thesis-parameter-estimation.rda')

# Fetch the unique parameter combinations for the models we just ran
parameters.cuF.emp <- agents.cuF %>%
  select(
    n_agents:confidence_weighted,
    -starts_with('trust_volatility'),
    -starts_with('weighted_sampling'),
    -starting_graph_type
  ) %>%
  unique() %>%
  rowid_to_column() %>%
  nest(d = -rowid) %>%
  mutate(
    model = map(d, function(.) {
      withr::with_seed(
        .$.random_seed_agents,
        do.call(adviseR:::makeAgents, select(
          ., 
          -bias_update_slope, -starts_with('feedback'), 
          -contains('random_seed'), -truth_sd, -confidence_weighted
        ))
      )
    })
  ) %>%
  # Substitute trust_volatility and weighted_sampling coefficients from estimates
  mutate(
    model = map2(model, d, function(.x, .y) {
      p <- withr::with_seed(
        .y$.random_seed_agents,
        slice_sample(recovered_parameters, n = .y$n_agents, replace = T)
      )
      .x$agents <- .x$agents %>%
        mutate(
          trust_volatility = rep(p$tu_end, .y$n_decisions),
          weighted_sampling = rep(p$ws_end, .y$n_decisions)
        )
      .x
    })
  ) %>% 
  unnest(cols = d) %>%
  mutate(
    decision_flags = map(
      n_decisions,
      function(.) {
        if (. > 3000) c(rep(1, 3000), rep(3, . - 3000)) else rep(3, .)
      }
    ),
    starting_graph = parameters$starting_graph[1],
    model = map2(model, n_agents, function(.x, .y) {
      g <- parameters$starting_graph[[1]](tibble(x = 1:.y))
      .x$graphs <- list(g)
      .x
    })
  ) %>%
  select(-rowid)

nCores <- 3
for (i in 1:ceiling(nrow(parameters.cuF.emp) / nCores)) {
  n <- (i - 1) * nCores + 1
  n <- c(n, min(n + nCores - 1, nrow(parameters.cuF.emp)))
  fName <- glue('_cache/network-models-cuF-emp/100a3000d_{n[1]}-{n[2]}.rda')
  if (file.exists(fName)) next()
  
  t1 <- Sys.time()
  models <- runSimulations(
    parameters.cuF.emp[n[1]:n[2], ], 
    cores = nCores,
    summaryFun = summary_fun
  )
  save(models, file = fName)
  print(glue('Completed run {i} ({n[1]}-{n[2]}).'))
  print(Sys.time() - t1)
}
```

```{r empirical models load}
t1 <- Sys.time()
fName <- '_cache/network-models-cuF-emp.rda'
if (file.exists(fName)) {
  load(fName)
} else {
  # Gather data and bind into an analysable data frame
  agents.cuF.emp <- NULL
  graphs.cuF.emp <- list()
  fList <- list.files('_cache/network-models-cuF-emp/', pattern = '', full.names = T)
  i <- 1
  for (f in fList) {
    load(f)
    for (m in models) {
      graphs.cuF.emp[[i]] <- list()
      for (d in 1:m$parameters$n_decisions) {
        if ((d %% 100 == 1) | d == m$parameters$n_decisions) {
          graphs.cuF.emp[[i]][[d]] <- m$model$graphs[[d]]
        }
      }
      
      m$parameters$model_num <- i
      i <- i + 1
      m$parameters$starting_graph <- NULL
      m$parameters$truth_fun <- NULL
      m$parameters$decision_flags <- NULL
      agents.cuF.emp <- 
        bind_rows(
          agents.cuF.emp, 
          m$model$agents %>% 
            filter(decision %% 100 == 1) %>%
            rename(agt_cluster_count = cluster_count) %>%
            left_join(as_tibble(m$parameters), by = character())
        )
    }
  }
  rm(m, models)
  save(agents.cuF.emp, graphs.cuF.emp, file = fName)
}
Sys.time() - t1
```

```{r empirical models graphs}
# Plot graphs for example instances of each model
ns <- agents.cuF.emp %>% 
  filter(random_seed == min(random_seed)) %>%
  select(model_num) %>%
  unique() %>%
  pull(model_num)

for (n in ns) {
  adviseR::networkGraph(
    list(
      model = list(graphs = graphs.cuF.emp[[n]]), 
      parameters = list(
        n_agents = agents.cuF.emp %>% 
          filter(model_num == n) %>% 
          pull(n_agents) %>%
          .[1]
      )
    ),
    mark.groups = list(
      which(V(graphs.cuF.emp[[n]][[1]])$bias <= .5),
      which(V(graphs.cuF.emp[[n]][[1]])$bias > .5)
    )
  )
  
  tmp.emp <- agents.cuF.emp %>% filter(model_num == n)
  print(
    tmp.emp %>%
      filter(decision %in% range(decision)) %>%
      transmute(
        model_num, 
        bias_sd, 
        ws_mean = weighted_sampling_mean,
        ws_sd = weighted_sampling_sd,
        burnIn = n_decisions > 3000,
        clusters = cluster_count, 
        mean_bias = round(meanBiasMagnitude, 3), 
        group_ratio = round(group_ratio_empirical, 3)
      ) %>%
      unique()
  )
  tmp.emp <- list(
    model = list(agents = tmp.emp),
    # hack back in the decision flags
    parameters = list(
      decision_flags = 
        if (tmp.emp$n_decisions[1] == 3000) {
          rep(3, 3000)
        } else {
          c(rep(1, 3000), rep(3, 1000))
        }
    )
  )
  print(biasEvolution(tmp.emp))
  
  # Strip out decision_flags for dropped decisions
  tmp.emp$parameters$decision_flags <- 
    tmp.emp$parameters$decision_flags[
      1:length(tmp.emp$parameters$decision_flags) %in% tmp.emp$model$agents$decision
    ]
  tmp.emp$model$graphs <- graphs.cuF.emp[[n]]
  print(weightEvolution(tmp.emp, decisions = function(d) c(1, 1501, 2901, 3401, 3901)))
}

```

#### Descriptive observations of individual models

As with the simulations with heterogeneous weighted sampling values, some agents resist the consensus extreme. 
As compared to those simulations, though, more agents occupy a middle-ground, and can cross more easily from one extreme to another.
Where a burn-in period is present, a large proportion of the agents occupy unstable middle-ground positions, preventing clear polarisation or consensus in the population.

### Combined output with previous

```{r all models summary}

mdls.cuF.all <- bind_rows(
  agents.cuF %>% 
    group_by(model_num) %>%
    summarise(across(n_agents:cluster_count, unique), .groups = 'drop') %>%
    mutate(
      burnIn = n_decisions > 3000,
      weighted_sampling = glue(
        '{ifelse(nchar(weighted_sampling_mean) == 1, "0", "")}{weighted_sampling_mean}',
        ' ({weighted_sampling_sd})'
      )
    ) %>%
    select(
      model_num:starting_graph_type,
      weighted_sampling,
      burnIn,
      everything(),
      -n_decisions, 
      -weighted_sampling_mean, 
      -weighted_sampling_sd
    ),
  agents.cuF.emp %>% 
    mutate(model_num = model_num + max(agents.cuF$model_num)) %>%
    group_by(model_num) %>%
    summarise(across(n_agents:cluster_count, unique), .groups = 'drop') %>%
    mutate(
      burnIn = n_decisions > 3000,
      weighted_sampling = 'emp'
    ) %>%
    select(
      model_num:starting_graph_type,
      weighted_sampling,
      burnIn,
      everything(),
      -n_decisions, 
      -weighted_sampling_mean, 
      -weighted_sampling_sd
    )
) %>%
  select(-starts_with('trust_volatility'))

for (v in names(select(mdls.cuF.all, n_agents:burnIn))) {
  if (length(unique(pull(mdls.cuF.all, v))) < 2)
    next()
  print(
    mdls.cuF.all %>% 
      mutate(grp = glue(
        '{',
        paste0(
          names(
            select(
              mdls.cuF.all, 
              n_agents:random_seed, 
              -!!sym(v)
            )
          ), 
          collapse = "}|{"
        ),
        '}'
      )) %>%
      pivot_longer(
        c(meanBiasMagnitude, cluster_count, starts_with('group_ratio')), 
        names_to = 'outcome'
      ) %>%
      mutate(across(-value, factor)) %>%
      ggplot(aes(y = value, x = !!sym(v), colour = weighted_sampling == 'emp')) + 
      geom_violin(alpha = .1, fill = 'black', colour = NA) +
      geom_line(aes(group = grp), alpha = .25, position = position_jitter(.1, .01)) +
      facet_wrap(~outcome, scales = 'free_y')
  )
}
```

Having considered the patterns at the level of individual models, we can draw more useful conclusions at a higher level from considering them as a set. 
In this analysis we attend to two properties of primary interest: 
First, how accurate are agents over time? 
This is represented by the mean bias magnitude. 
In ideal circumstances, this should be very low, indicating that agents are largely unbiased. 
Second, to what extent are agents more likely to trust advice from agents who share their bias?
This is captured by the ratio between trust in agents with the same versus different bias. 
This value is not always well defined - where all agents eventually acquire the same bias, the mean weight of out-group advisors is undefined.

Summarising the results in this way means that we can observe some general features of the models by comparing otherwise-similar models over the different parameters.

A more varied range of biases in the population has relatively little consistent effect on mean bias magnitude, but does lead to higher group ratios. 
This indicates that models more readily split into polarised camps with echo-chambers where there are more extreme biases present in the population.

Mean bias magnitude is less pronounced where there is a burn-in time, although this is likely due to there being less time after burn-in for the biases to update. 
Burn-in time tends to decrease the homophily of the models, judging by final bias, but to increase it judging by initial bias. 
This suggests that polarisation occurs, but that many middle-ground agents change which camp they end up in.

Weighted selection has little effect where the mean is increased, but a clearer effect where variation is introduced. 
The clustering of weights suggests that there is less homophily where agents are more selective about whom they consult for advice. 

Weighted sampling interacts with the lead-in time.
Lead-in time is a proxy for the starting asymmetry of the connections in the network.
The figure shows that, as networks start off with more homophilic structures, weighted selection increases the effect of homophily. 
Note that sufficiently homophilic networks will continue to exhibit echo chamber structures regardless of the level of weighted sampling, and that even very strong weighted sampling does not produce echo chambers in networks which start off perfectly symmetrical. 

The key finding of interest from the models thus far is that source selection may exacerbate echo chamber formation. 
This finding is in line with similar findings from others who have produced these models \mccorrect{!TODO[citations galore]}.
We are now in a position to see whether these models continue to exhibit these features when we parametrise them based on the data we observed in our participants' behavioural data.

##### Bias x TrustUpdate

```{r}

agents.cuF.emp %>% 
  filter(
    decision == max(decision),
    abs(trust_volatility) < .2
  ) %>%
  ggplot(aes(x = trust_volatility, y = abs(bias - .5))) +
  geom_smooth(method = 'lm', formula = y ~ x) + 
  geom_point(alpha = .25)

agents.cuF.emp %>% 
  filter(
    decision == max(decision),
    abs(weighted_sampling) < 55
  ) %>%
  ggplot(aes(x = weighted_sampling, y = abs(bias - .5))) +
  geom_smooth(method = 'lm', formula = y ~ x) + 
  geom_point(alpha = .25)

```



