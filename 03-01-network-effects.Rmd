---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::html_document2: default
  bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
documentclass: book
---

# Network effects of interaction {#chapter-network-effects}
\adjustmtc 

```{r}
source('scripts_and_filters/general_setup.R')
library(adviseR)
library(igraph)
library(BayesFactor)
library(magrittr)
library(parallel)

library(gifski)

# animation
saveAnimation <- function(model, saveAs) {
  p <- tempfile(pattern = 'anim_')
  dir.create(p)
  gs <- list(
    which(V(model$model$graphs[[1]])$bias <= .5),
    which(V(model$model$graphs[[1]])$bias > .5)
  )
  coords <- NULL
  for (i in 1:length(m$model$graphs)) {
    g <- model$model$graphs[[i]]
    png(filename = paste0(p, '/', sprintf('%04i', i), '.png'))
    plotGraph(model, i, activeColours = F, layout = function(x) 
      layout_with_fr(x, coords = coords, start.temp = 0.1), mark.groups = gs)
    dev.off()
    coords <- layout_with_fr(g, coords = coords, start.temp = 0.1)
  }
  gifski(
    list.files(p, full.names = T), 
    gif_file = saveAs,
    width = 480, 
    height = 480, 
    delay = .1,
    loop = F,
    progress = F
  )
  unlink(p)
}
```

<!-- We may have some modelling to present along these lines. -->

## Models with agents drawn from a distribution

### Can we reproduce the major polarisation/echo chamber effects?

First, we present the models with a variety of parameters.
This shows basic model behaviour range before we consider weighted selection.

```{r network-models}
t1 <- Sys.time()
n_reps <- 20
withr::with_seed(
  20210507,
  seeds <- c(20210507, floor(runif(n_reps - 1) * 1e8))
)
parameters <- tidyr::crossing(
  tidyr::crossing(
    tibble(
      trust_volatility_mean = c(.05, .2),
      trust_volatility_sd = c(.01, .05)
    ),
    bias_sd = c(1, .25),
    confidence_weighted = c(T, F),
    bias_volatility_sd = c(0, .05), 
    weighted_sampling_mean = c(0, 50)
  ),
  tibble(
    n_decision = c(200, 300),
    decision_flags = map(
      n_decision, 
      ~ if (. > 200) {c(rep(1, 200), rep(3, 100))} else {3}
    )
  ),
  bias_volatility_mean = 0,
  n_agents = 100,
  random_seed = seeds,
  starting_graph = .6
)

# Drop unnecessary models
parameters <- parameters %>% 
  filter(!(bias_volatility_sd == 0 & n_decision > 200))

output <- parameters %>%
  mutate(groupRatio = NA_real_, weightRatio = NA_real_)

summary_fun <- function(m) {
  # Calculate group ratios
  m$parameters$group_ratio_original <-
    adviseR::groupRatio(m$model$graphs[[length(m$model$graphs)]])
  m$parameters$group_ratio_empirical <-
    adviseR::groupRatio(m$model$graphs[[length(m$model$graphs)]], F)
  
  # Calculate proportion of extremes
  x <- m$model$agents$bias[m$model$agents$decision == max(m$model$agents$decision)]
  m$parameters$n_lt_20 <- mean(x < .2)
  m$parameters$n_gt_80 <- mean(x > .8)
  m$parameters$n_mid <- mean(x >= .2 & x <= .8)
  
  # Save the summary stuff for the first example of each model
  if (m$parameters$random_seed != 20210507 || m$parameters$confidence_weighted) {
    m$model <- NULL
  }
  
  m
}

models <- runSimulations(
  parameters, 
  cores = min(nrow(parameters), 8),
  summaryFun = summary_fun
)

output <- models %>%
  lapply(function(m) {
    if (!is.null(m$model)) {
      adviseR::networkGraph(
        m, 
        mark.groups = list(
          which(V(m$model$graphs[[1]])$bias <= .5),
          which(V(m$model$graphs[[1]])$bias > .5)
        )
      ) 
      
      for (x in adviseR::inspectModel(m))
        print(x)
    }
    
    m$parameters$truth_fun <- NULL
    m$parameters$decision_flags <- NULL
    as_tibble(m$parameters)
  }) %>%
  bind_rows()

output %>% select(ends_with('ratio'), everything())
Sys.time() - t1
```

For summary purposes we now divide up the models into those which exhibit echo chamber formation and those which do not. 
To do this, we set a threshold on the group weight ratio after the final decision. 
This ratio is calculated as the mean of the weights of agents with the same initial bias direction divided by the mean of the weights of agents with different initial bias direction.
In short, it represents the trust ratio for in- as compared to out-group members.
Values for this ratio in the region of 1 indicate relatively balanced in- and out-group weight, while values around 2 or greater indicate substantially greater trust in in-group as opposed to out-group members.
We set the threshold (somewhat arbitrarily) to 1.5.

```{r}

runs <- output %>% 
  # filter(confidence_weighted == F) %>%
  mutate(
    biasUpdates = bias_volatility_sd != 0,
    flatStart = n_decisions == 200,
    ec_orig = group_ratio_original > 1.05,
    ec_emp = group_ratio_empirical > 1.05
  ) %>%
  select(
    -random_seed, -starting_graph, -bias_volatility_mean,
    -bias_volatility_sd, -n_decisions, -n_agents, 
    -starting_graph_type, -trust_volatility_sd,
    gr_orig = group_ratio_original,
    gr_emp = group_ratio_empirical,
    ws_mean = weighted_sampling_mean
  ) 

run_summary <- runs %>%
  group_by(
    trust_volatility_mean, confidence_weighted, biasUpdates, flatStart, bias_sd,
    ws_mean
  ) %>%
  summarise(
    across(.fns = list(m = mean, sd = sd), .names = '{.col}_{.fn}'),
    .groups = 'drop'
  ) %>% 
  mutate(across(matches('_(m|sd)$', perl = T), .fns = ~ round(., 4))) %>%
  arrange(gr_emp_m) %>%
  select(
    starts_with('ec_'), 
    matches('^gr_.+_m$', perl = T), 
    matches('^n_.+_m$', perl = T),
    ws_mean,
    tv_mean = trust_volatility_mean,
    conf_weighted = confidence_weighted,
    biasUpdates,
    flatStart,
    bias_sd, 
    matches('^gr_.+_sd$', perl = T),
    matches('^n_.+_sd$', perl = T),
    everything(), 
    -matches('^ec_.+_sd$', perl = T)
  )

run_summary

for (v in names(select(run_summary, ws_mean:bias_sd))) {
  print(
    run_summary %>% 
      select(starts_with('gr_e'), ws_mean:bias_sd) %>%
      mutate(across(ws_mean:bias_sd, factor)) %>%
      ggplot(aes(y = gr_emp_m, x = !!sym(v))) + 
      geom_hline(yintercept = 1.05, linetype = 'dashed') +
      geom_violin(alpha = .1, fill = 'black', colour = NA) +
      geom_boxplot(outlier.colour = NA, fill = NA, width = .2) +
      geom_point(alpha = .1, position = position_jitter(.1)) + 
      stat_summary(
        geom = 'label', 
        fun.data = function(x) data.frame(
          y = .975, label = glue('mean = {round(mean(x), 2)}')
        )
      )
  )
}

```

Summarising the results in this way means that we can observe some notable features of the models.
Echo chambers form more readily where  
* the agents update their trust more readily  
* the agents update their biases  
* the agents start with a flat network wherein all agents have equal trust in one another  
  * this is due to the agents losing their biases at least as quickly as they build their trust networks  
* decisions are updated proportional to the judge's confidence  
* the agents' biases are more different to start with  

### The effect of weighted sampling on echo chamber formation

Weighted sampling is the representation in the model of source selection behaviour.
This source selection behaviour is the primary outcome of interest in the behavioural experiments, and thus it is important to understand how this property may theoretically affect echo chamber formation within in the models.

```{r weighted sampling simple}
ws <- run_summary %>% 
  select(starts_with('gr_e'), ws_mean:bias_sd) %>%
  mutate(
    across(ws_mean:bias_sd, factor),
    model_id = glue('{tv_mean}.{conf_weighted}.{biasUpdates}.{flatStart}.{bias_sd}'),
    model_id = factor(model_id)
  )

ws %>%
  ggplot(aes(y = gr_emp_m, x = ws_mean)) + 
  geom_hline(yintercept = 1.05, linetype = 'dashed') +
  geom_violin(alpha = .1, fill = 'black', colour = NA) +
  geom_boxplot(outlier.colour = NA, fill = NA, width = .2) +
  geom_line(alpha = .2, aes(group = model_id)) + 
  stat_summary(
    geom = 'label', 
    fun.data = function(x) data.frame(
      y = .975, label = glue('mean = {round(mean(x), 2)}')
    )
  )

ws %>% 
  select(model_id, ws_mean, gr_emp_m) %>%
  pivot_wider(names_from = ws_mean, values_from = gr_emp_m) %>%
  transmute(model_id, diff = `50` - `0`) %>%
  arrange(diff)
```

It's odd that weighted selection should universally decrease the echo chambery-ness of all the models. 
We can investigate this under conditions where it ought to increase: when the network is already set up such that shared bias correlates with trust.

```{r weighted selection increase}

# Select an appropriate model to examine
parameters[100, ]
# Set up a starting_trust function to make trust proportional to shared bias
trust_fun <- function(agents) {
  bias <- matrix(agents$bias, nrow = nrow(agents), ncol = nrow(agents))
  bias <- abs(bias - t(bias)) # bias similarity
  apply(bias, 1, function(x) {
    1 - (x / max(x) * .5)
  })
}
# Run models across weighted sampling and starting trust
m_base <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = .6,
  weighted_sampling_mean = 0
)
m_tf <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = trust_fun,
  weighted_sampling_mean = 0
)
m_base_ws_50 <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = .6,
  weighted_sampling_mean = 50
)
m_tf_ws_50 <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = trust_fun,
  weighted_sampling_mean = 50
)
m_base_ws_10 <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = .6,
  weighted_sampling_mean = 10
)
m_tf_ws_10 <- runSimulation(
  n_agents = 100, n_decisions = 1000, trust_volatility_mean = .05,
  trust_volatility_sd = .01, bias_sd = 1, confidence_weighted = F,
  bias_volatility_mean = 0, random_seed = pi * 1e8, starting_graph = trust_fun,
  weighted_sampling_mean = 10
)
# Explore differences in weighted selection 
bind_rows(
  mutate(adviseR:::.biasCorrelation(m_base), model = 'm_base'),
  mutate(adviseR:::.biasCorrelation(m_tf), model = 'm_tf'),
  mutate(adviseR:::.biasCorrelation(m_base_ws_10), model = 'm_base_ws_10'),
  mutate(adviseR:::.biasCorrelation(m_tf_ws_10), model = 'm_tf_ws_10'),
  mutate(adviseR:::.biasCorrelation(m_base_ws_50), model = 'm_base_ws_50'),
  mutate(adviseR:::.biasCorrelation(m_tf_ws_50), model = 'm_tf_ws_50')
) %>%
  ggplot(aes(x = decision, y = r, colour = model)) +
  geom_errorbar(aes(ymin = ciL, ymax = ciH), alpha = .5)
```

Okay, odd behaviour. 
Apparently the higher the weighted selection the _lower_ the correlation between shared bias and advice weight. 
This seems wrong. 
Even when advice weight starts as perfectly correlated with shared bias, the correlation unravels _faster_ when weighted selection is higher. 

We can explore this directly by taking the connectivity graphs and feeding them into the selectAdvisorSimple function.

```{r weighted selection investigation}
generations <- c(1, 100, 500, 1000)
g <- lapply(generations, function(i) {
  m_tf$model$graphs[[i]][attr = 'weight'] %>%
  matrix(sqrt(length(.)), sqrt(length(.)))
})

picks <- tidyr::crossing(
  i = 1:1e4, 
  weightedSelection = c(0, 10, 50, 500), 
  generation = 1:length(g)
) %>% mutate(
  picks = map2(weightedSelection, generation, function(ws, gen) {
    tibble(
      agent = c(1, 50, 100),
      pick = adviseR:::selectAdvisorSimple(g[[gen]], ws)[agent]
    )
  })
) %>% 
  unnest(cols = picks) %>%
  mutate(generation = generations[generation])

picks

picks %>% 
  mutate(across(-pick, factor)) %>%
  ggplot(aes(pick, colour = weightedSelection)) +
  geom_density(position = 'identity') +
  facet_wrap(generation~agent, labeller = label_both)
  

```

So from the above it looks like the weighted selection function is making agents more likely to select agents they trust more.

We can next check that the mean weight of advisor is higher than the mean advisor weight (i.e. the advisor actually selected on each trial is probably more trusted than the average advisor).

```{r weighted selection average weight}
tibble(
  generation = generations,
  average_weight = map_dbl(generations, ~ mean(
    g[[which(generations == .)]][upper.tri(g[[which(generations == .)]])]
  )),
  average_advisor_weight = m_tf_ws_50$model$agents %>%
    filter(decision %in% generations) %>%
    group_by(decision) %>%
    summarise(x = mean(weight), .groups = 'drop') %>%
    pull(x),
  sensible = average_advisor_weight > average_weight
)
```

We focus on exploring whether weighted sampling introduces a lower threshold for asymmetric tie weights to produce echo chambers. 
We examine this over several repetitions of models, for a wider range of values for weighted_sampling and decision burn-in.
For simplicity (and tractability) we reduce each collection of model instances to a simple set of summary statistics describing the group weight ratio.

```{r}
if (!(hasName(.Options, 'ESM.skip') && getOption('ESM.skip'))) {
  t1 <- Sys.time()
  set.seed(20201126)
  
  summaryFun <- function(model) 
    groupRatio(model$model$graphs[[length(model$model$graphs)]])
  
  cacheFile <- '_cache/model-summary.rda'
  if (file.exists(cacheFile)) load(cacheFile) else runs <- NULL
  
  start_time <- Sys.time()
  
  leads <- c(0, 250, 500, 750)
  parameters <- tidyr::crossing(
    tibble(
      n_decision = map_int(leads, ~ as.integer(. + 1000)),
      decision_flags = map(leads, ~ c(rep(1, .), rep(3, 1000)))
    ),
    weighted_sampling_mean = c(0, 3, 6, 12, 24, 48),
    weighted_sampling_sd = 0,
    bias_volatility_sd = .05,
    bias_volatility_mean = 0,
    n_agents = 100,
    starting_graph = .1
  ) 
  
  # We now have a lot of different models
  max_cores <- detectCores() - 6
  n_reps <- 4 # 4 full-computer runs for each model
  
  seeds <- runif(max_cores * n_reps, 1e6, 1e9)
  
  
  for (i in 1:nrow(parameters)) {
    for (r in 1:n_reps) {
      indices <- ((r - 1) * max_cores + 1):(r * max_cores)
      rows <- slice_sample(parameters[i, ], n = max_cores, replace = T) %>%
        mutate(random_seed = seeds[indices])
      # Don't go again if we already did this and have the result saved
      if (!is.null(runs) && 
          nrow(filter(
            runs, 
            n_decision %in% rows$n_decision & 
            weighted_sampling_mean %in% rows$weighted_sampling_mean &
            random_seed %in% rows$random_seed)) > 0
      )
        next()
      runs <- rbind(
        runs,
        rows %>% mutate(
          run = indices,
          groupRatio = runSimulations(rows, max_cores, summaryFun)
        )
      )
      save(runs, file = cacheFile)
    }
  }
  
  end_time <- Sys.time()
  print(paste0(
    'Completed ', n_reps * max_cores * nrow(parameters), ' models (',
    n_reps, ' reps using ', max_cores, ' cores of ', nrow(parameters), 
    ' sets of parameters)'
  ))
  end_time - start_time
  
  
  runs %>% 
    nest(d = c(groupRatio, run, random_seed)) %>%
    mutate(
      meanGroupRatio = map_dbl(d, ~mean(.$groupRatio)),
      meanEchoCamber = map_dbl(d, ~mean(.$groupRatio > 1.5)),
      leadIn = n_decision - 1000
    ) %>%
    select(
      starts_with('mean'), 
      leadIn, 
      everything(), 
      -d, 
      -n_decision, 
      -decision_flags
    )
  
  runs %>%
    mutate(
      leadIn = factor(n_decision - 1000),
      weighted_sampling_mean = if_else(
        is.na(weighted_sampling_mean), 
        0, 
        weighted_sampling_mean
      ),
      weighted_sampling_mean = factor(weighted_sampling_mean)
    ) %>%
    ggplot(aes(x = leadIn, y = groupRatio, fill = weighted_sampling_mean)) +
    geom_hline(yintercept = 1.5, linetype = 'dashed') +
    annotate(geom = 'label', label = 'Threshold', x = "0", y = 1.5) +
    geom_boxplot(outlier.alpha = .25) +
    scale_y_continuous(limits = c(0, NA)) +
    scale_fill_brewer(palette = 'Blues')
  
  Sys.time() - t1
}

```

Weighted sampling interacts with the lead-in time.
Lead-in time is a proxy for the starting asymmetry of the connections in the network.
The figure shows that, as networks start off with more homophilic structures, weighted selection increases the effect of homophily. 
Note that sufficiently homophilic networks will continue to exhibit echo chamber structures regardless of the level of weighted sampling, and that even very strong weighted sampling does not produce echo chambers in networks which start off perfectly symmetrical. 

The key finding of interest from the models thus far is that source selection may exacerbate echo chamber formation. 
This finding is in line with similar findings from others who have produced these models \mccorrect{!TODO[citations galore]}.
We are now in a position to see whether these models continue to exhibit these features when we parametrise them based on the data we observed in our participants' behavioural data.

## Participant data

Exploration of the model space shows that the models are sensitive to effects of source selection behaviour as parametrised by weighted sampling. 
This parameter in the model can be derived from the advisor choice behaviour in the behavioural experiments.
Other model parameters can also be fit using the participants' behavioural data. 

There are two ways in which parameters can be derived for the models from behavioural data. 
Both approaches start by taking the model used in simulation and finding the parameters for that model which best describe each participant's behaviour.
This is done using a standard parameter estimation approach based on a gradient descent algorithm.

Once parameter estimates have been obtained for all the participants, those parameters must be reconnected with the simulation side of the model. 
The first approach to doing this is to simply instantiate agents with exactly the properties of our participants.
By sampling a variety of models made up of different selections of participants, we can characterise some of the network behaviour that particular mixtures of individual strategies produce, at the expense of being tied explicitly to the performances of specific participants.

The second approach is to construct distributions in parameter space from the individual participants' specific parameter estimates. 
This approach allows for the generation of an unlimited number of unique participants, and can allow for correlations between parameter estimates as well as describing the estimates' distributions. 
The drawback to this approach is that the agents produced will be more homogeneous in their overall strategies.
For example, if a minority strategy exists within the population, this strategy should be evident in the first approach but would not be so in the second - instead we would simply have the parameter estimate distributions reflective of the dominant strategy slightly modified through averaging towards the minority strategy, which may in turn produce agents whose behaviour is not reflective of any plausible real individuals.

### Model fitting

