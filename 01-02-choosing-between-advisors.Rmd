---
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    keep_tex: true
  bookdown::word_document2: default
  bookdown::html_document2: default
documentclass: book
---

```{r setup, echo = F, include = F}
options(
  tinytex.verbose = TRUE
  )

knitr::opts_chunk$set(
  echo = F, warnings = F, messages = F
)

library(ggplot2)

theme_set(
  theme_light() +
    theme(
      panel.background = element_blank(),
      panel.border = element_blank(),
      panel.grid.minor = element_blank(),
      panel.grid.major = element_blank(),
      axis.line = element_line(colour = "black"),
      text = element_text(size = 14),
      legend.position = "top"
    )
)

library(broom)
```

# Psychology of source selection {#chapter-advisor-choice}
\minitoc

<!-- Presentation of the experiments (may want different subsections for each experiment and an extra one for the general discussion). -->

## Experiment 1: Metacognitive-contingent advisors (Lab) {#ex1}

Pescetelli et al. [-@pescetelliRoleDecisionConfidence2018] showed that, in the absence of objective feedback, advice was more influential coming from an advisor who agrees with a participant when that participant is confident (_Bias Sharing_) than coming from an advisor who agrees with a particpant when that participant is unconfident (_Anti Bias_). This provides evidence of a metacognitive sensitivity in the tracking of advice and the updating of advisor utility. Here we investigate whether these effects show up in the domain of advisor selection.

The literature on information exposure and evaluation indicates that people evaluate more favourably information which agrees with their currently-held opinion !TODO[REF], and preferentially seek out information sources which are likely to provide information which agrees with their currently-held opinion [@garrettEchoChambersOnline2009; @searsSelectiveExposureInformation1967]. If this holds true in the context of the judge-advisor system, advice from _Bias Sharing_ advisors ought to be evaluated more favourably (influence should increase) and should be sought more frequently. Given the evidence in favour of the first of these propositions, we here investigate the latter: given a choice, will judges prefer to receive advice from a _Bias Sharing_ advisor over receiving advice from an advisor who does not share the judge’s bias?

Pescetelli et al. [-@pescetelliRoleDecisionConfidence2018] used a judge-advisor system to demonstrate that judges are influenced to a greater extent by advisors who share their biases. Participants played the role of judge in a judge-advisor system, while the advisors were virtual agents whose advice-giving was dependent upon the confidence and correctness of the judges' initial decisions. The advisors were balanced for overall agreement with the judge and objective correctness of advice. We place participants in a similar paradigm in which they are given a choice between advisors, and hypothesise that they will more frequently seek advice from the _Bias Sharing_ advisor than from the _Anti Bias_ advisor.

### Open science {#ex1-os}

#### Preregistration {#ex1-os-prereg}

The study was preregistered using AsPredicted.org. The preregistration document can be found at [https://aspredicted.org/ze3tn.pdf](https://aspredicted.org/ze3tn.pdf). The analyses are presented following the structure of the preregistered document where possible. All preregistered analyses are reported. Where analyses do not form part of the preregistration, they are reported after the preregistered analyses, or are clearly designated as exploratory in the text.

#### Open materials {#ex1-os-materials}

Experimental materials, including scripts required to run the experiments in MATLAB and scripts required to analyse the data in R, are available from the [GitHub repository](https://github.com/mjaquiery/nofeedback_trust). 

The experimental design was adapted from Pescetelli et al. [-@pescetelliUseMetacognitiveSignals2017], and the major work in the design, as well as the experimental scripts, is due to Niccolo. The full list of changes to the final design can be seen in the commits to the project repository, which began as a fork of [Niccolo's work](https://github.com/chri4354/nofeedback_trust).

#### Open data {#ex1-os-data}

Anonymised study data can be found at !TODO[Use a sensible archive format for this study data, archive on OSF, and produce data dictionary]. A preloaded version of the data formatted appropriately for the R scripts is included in the [GitHub repository](https://github.com/mjaquiery/nofeedback_trust).

### Method {#ex1-m}

```{r load exp1 data, include = F}

# Run the analysis script for experiment 1 from the symlinked repository.
exDirs <- c("repos/nofeedback_trust/AdvisorChoice/analysis/")
wd <- setwd(exDirs[1])
source("AdvisorChoice.R")
setwd(wd)

library(kableExtra)
library(tibble)

```

#### Participants {#ex1-m-particpants}

`r nrow(participants)` participants (M~age~ = `r round(mean(participants$age))` ±SD `r round(sd(participants$age), 1)`, `r sum(tolower(participants$gender) == 'm')` male, `r sum(tolower(participants$gender) == 'f')` female, `r sum(tolower(participants$gender) != 'm' & tolower(participants$gender) != 'f')` other) recruited from the University of Oxford participant recruitment platforms took part in the experiment. An additional 2 participants attended experimental sessions but their data were not analysed. Participants were compensated for their time with either course credit for a psychology degree, or 10GBP. 

#### Ethics {#ex1-m-ethics}

Ethical approval for the study was granted by the University of Oxford Medical Sciences Interdivisional Research Ethics Committee (Ref: R55382/RE001).

#### Procedure {#ex1-m-procedure}

The experiment consisted of a judge-advisor system with a perceptual decision task (Figure \@ref(fig:ex1-paradigm)). Participants played the role of the judge, and the advisors were played by virtual agents whose answers depended upon the confidence with which the judge reported the initial decision. In the majority of trials (92%), participants were offered advice from virtual advisors. In one third of these trials (‘choice trials’), participants chose which advisor to receive advice from by clicking on their respective portraits appearing at the top and bottom of the screen. On the remaining two thirds of trials (‘forced trials’), participants were forced to take advice from one of the two advisors (equiprobably).  On these trials, the forced advisor’s portrait appeared at the top or bottom of the screen, with a red cross appearing in the other location, which was not selectable. On the remaining 8% of trials, participants received no advice and were given no opportunity to revise their initial decision. These ‘catch trials’ were included to encourage participants to attend to the initial decisions.

```{r ex1-paradigm, fig.align='center', fig.cap="Experiment 1 procedure: The task began with a blank screen containing only a fixation cross and progress bar. Momentarily prior to the onset of the stimuli the fixation cross flickered. The stimuli, two rectangles containing approximately 200 dots each, appeared for 0.16s, one on either side of the fixation cross. Once the stimuli disappeared, a response-collection screen appeared and prompted the participant to indicate their initial decision and its confidence by selecting a point within one of two regions. The left region indicated a decision that the target was on the left, and increasingly-leftwards points within that region indicated increasing confidence in that decision. The right region indicated a decision that the target was on the right, and increasingly-rightwards points within that region indicated increasing confidence in that decision.\\\nNext, the participant was presented with a choice screen. The choice screen displayed two images, one at the top of the screen and one at the bottom. The images were one of the following: an advisor portrait, a silhouette, or a red cross. The red cross was not selectable, forcing participants to choose the other option. The silhouette offered no advice, and was only ever offered as a forced choice. Selecting an advisor image provided the participant with the opinion of that advisor on the trial.\\\nHaving heard the advice, the participant was again presented with the response-collection screen, with a yellow indicator marking their original response. A second (final) judgement was collected using this screen (except on catch trials), and the trial concluded.", out.width="100%"}
knitr::include_graphics("figures/experiment_01_MATLAB/paradigm.jpg")
```

Each participant completed 363 trials (51 practice trials over 2 blocks + 12 x 26-trial experimental blocks) in which they had to identify the box with the most dots (Figure \@ref(fig:ex1-paradigm)). The difficulty of the task was continually adjusted throughout the experiment using a 2-down, 1-up staircase procedure to keep the participant’s initial decision accuracy at 72%. At the end of each block, participants were notified as to their (final decision) accuracy in the block and given the opportunity to rest for as long as they wished. Throughout the experiment a progress bar provided a graphical indication of the number of trials remaining in the experiment. 

After each block participants were told what percentage of the (final) answers they had provided were correct and allowed to take a short, self-paced break. Prior to the first experimental block, after the final experimental block, and after the 4th and 8th experimental blocks, participants were presented with a questionnaire (Figure \@ref(fig:ex1-questionnaire)). The questionnaire contained 4 questions for each advisor. The questions asked for the judge’s assessment of the advisor’s likeability, trustworthiness, influence, and ability to do the task. The questions presented before the first experimental block were worded prospectively (e.g. ‘How much are you going to like this person?’ as opposed to ‘How much do you like this person?’). Answers were provided by moving a sliding scale below the advisor’s portrait towards the right for more favourable responses (marked ‘extremely’) or towards the left for less favourable responses (marked ‘not at all’).

```{r ex1-questionnaire, fig.align='center', fig.cap="Experiment 1 advisor questionnaire: Participants rated advisors on a number of different dimensions.", out.width="100%"}
knitr::include_graphics("figures/experiment_01_MATLAB/questionnaire.jpg")
```

Each participant attended the experiment individually, was welcomed and briefed on the experimental procedure, and had their informed consent recorded, before the experiment began. They were seated a comfortable distance in front of a 24’ (1440x900 resolution) computer screen in a small, quiet, and dimly-lit room. The experiment took place wholly on the computer, and lasted around 45 minutes.

The experiment was programmed in MATLAB R2017b [@MATLAB2017] using the Psychtoolbox-3 package [@kleiner2007s]. 

#### Advisor advice profiles {#ex1-m-profiles}

The advisors are virtual agents whose probability of agreeing with the participant’s decision varies as a function of the participant’s confidence and correctness in the initial decision phase. Table \@ref(tab:ex1-advisor-profiles) illustrates how this relationship functions, and shows that the overall correctness and agreement rates of the advisors is equivalent overall. Importantly, on largest minority of trials, the middle 40%, the advisors are exactly equivalent, meaning these trials can be compared directly without confounds arising from agreement rate and initial confidence.

```{r ex1-advisor-profiles-r} 

tmp <- tribble(
  ~` `, ~`  `, ~`Bias Sharing`, ~`Anti Bias`,
  "Participant correct", "High (top 30%)", 80, 60,
  "Participant correct", "Medium (middle 40%)", 70, 70,
  "Participant correct", "Low (bottom 30%)", 60, 80,
  "Participant incorrect", "Any", 30, 30,
  "Total agreement", "Participant correct", 70, 70,
  "Total agreement", "Participant incorrect", 30, 30
)

kable(tmp, caption = "Table \\label{tab:ex1-advisor-profiles}: Experiment 1 advisor advice profiles") %>%
  kable_styling() %>%
  column_spec(1, bold = T) %>%
  row_spec(0, bold = F) %>%
  collapse_rows(columns = 1, valign = "top") %>%
  add_header_above(c(" ", 
                     "Initial decision confidence", 
                     "Probability of agreement (%)" = 2))

```

#### Analysis {#ex1-m-analysis}

Data analysis was performed using R [@rcoreteamLanguageEnvironmentStatistical2018]. For a full list of packages and software environment information, see !TODO[figure out where to include this stuff. Appendix? Also link to a containerized version of this.]

Bayes Factors (BF) are presented alongside p values and test statistics. A BF < 0.33 indicates decisive evidence in favour of the null hypothesis over the alternative hypothesis (with lower values being increasingly clear), BF > 3 indicates decisive evidence of the alterative over the null (with higher values being increasingly clear), and 0.33 <= BF <= 3 indicates there is insufficient evidence to reach a conclusion.

Influence, the dependant variable in some analyses, is calculated as the extent to which the judge’s initial decision is revised in the direction of the advisor’s advice. The initial ($C_1$) and final ($C_2$) decisions are made on a scale stretching from -55 to +55 with zero excluded, where values <0 indicate a ‘left’ decision and values >0 indicate a ‘right’ decision, and greater magnitudes indicate increased confidence. Influence ($I$) is given for agreement trials by the shift towards the advice:

\begin{align}
I|\text{agree} = f(C_1) 
\begin{cases}
  C_2 - C_1 & C_1 > 0 \\
  -C_2 + C_1 & C_1 < 0
\end{cases}
(\#eq:ex1-influence-agree)
\end{align}

And by the inverse of this for disagreement trials:

\begin{align}
I|\text{disagree} = -I|\text{agree}
(\#eq:ex1-influence-disagree)
\end{align}

The confidence scale excludes 0, and thus the final decision can always be more extreme when moving against the direction of the initial answer than when moving further in the direction of the initial answer. A capped measure of influence was used to minimise biases arising from the natural asymmetry of the scale. This measure was calculated by truncating absolute influence values which were greater than the maximum influence which could have obtained had the final decision been a maximal response in the direction of the initial answer (Figure \@ref(fig:ex1-capping)).

```{r ex1-capping, fig.align='center', fig.cap="Capping influence to avoid scale bias: In this example the judge’s initial response is 42, meaning that their final decision could be up to 13 points more confident or up to 97 points less confident. Any final decision which is more than 13 points less confident is therefore capped at 13 points less confident.", out.width="100%"}
knitr::include_graphics("figures/experiment_01_MATLAB/capping.jpg")
```

The capped influence measure $I_\text{capped}$ is obtained by:

\begin{align}
I_\text{capped} = f(C_1) 
\begin{cases}
  \text{min}(I, 2C_1 - 55) & C_1 > 0 \\
  \text{max}(I, 2C_1 + 55) & C_1 < 0
\end{cases}
(\#eq:ex1-influence-capping)
\end{align}

The explicit measure of trust is obtained using questionnaires. The questionnaires are delivered at 4 time points, and consist of 4 questions per advisor which are answered on a 1-100 scale.

### Result {#ex1-r}

#### Descriptive statistics {#ex1-r-descriptives}

`r nrow(participants) + 2` participants took part in the study. One participant was unable to complete the experiment due to technical difficulties. [Pre-registration](#ex1-os-prereg) of the study analysis stated that data would be collected from 24 participants, so the final (overbooked) subject tested was excluded from analysis. Descriptive statistics for the `r nrow(participants)` participants included in the analysis are presented in Table \@ref(tab:ex1-descriptives).

```{r ex1-descriptives-r} 

tmp.conf <- bind_rows(
  # Mean confidence
  # Initial judgement
  trials %>% 
    group_by(cor1, participantId) %>%
    summarise(cj1 = mean(abs(cj1), na.rm = T)) %>%
    group_modify(~ mean_cl_normal(.$cj1)) %>%
    ungroup() %>%
    mutate(cor1 = if_else(cor1 == 1, "Correct", "Wrong")) %>%
    rename(condition = cor1) %>%
    # Combined correct and wrong
    bind_rows(
      cbind(
        tibble(condition = "Both"),
        trials %>%
          group_by(participantId) %>%
          summarise(cj1 = mean(abs(cj1), na.rm = T)) %>%
          group_modify(~ mean_cl_normal(.$cj1))
        )
      ) %>%
    mutate(stat = "Mean initial confidence",
           context = "Initial judgement",
           target = NA),
  
  # Final decision
  trials %>% 
    dplyr::filter(!is.nan(cj2)) %>%
    group_by(cor2, participantId) %>%
    summarise(cj2 = mean(abs(cj2), na.rm = T)) %>%
    group_modify(~ mean_cl_normal(.$cj2)) %>%
    ungroup() %>%
    mutate(cor2 = if_else(cor2 == 1, "Correct", "Wrong")) %>%
    rename(condition = cor2) %>%
    # Combined correct and wrong
    bind_rows(
      cbind(
        tibble(condition = "Both"),
        trials %>%
          group_by(participantId) %>%
          summarise(cj2 = mean(abs(cj2), na.rm = T)) %>%
          group_modify(~ mean_cl_normal(.$cj2))
        )
      ) %>%
    mutate(stat = "Mean final confidence",
           context = "Final decision",
           target = NA),
  
  # Agreement
  trials %>% 
    dplyr::filter(agree == T, !is.nan(cj2)) %>%
    group_by(advisorId, participantId) %>%
    summarise(cj2 = mean(abs(cj2), na.rm = T)) %>%
    group_modify(~ mean_cl_normal(.$cj2)) %>%
    ungroup() %>%
    mutate(advisorId = if_else(advisorId == 1, "Bias Sharing", "Anti Bias")) %>%
    rename(condition = advisorId) %>%
    # Combined correct and wrong
    bind_rows(
      cbind(
        tibble(condition = "Both"),
        trials %>%
          group_by(participantId) %>%
          summarise(cj2 = mean(abs(cj2), na.rm = T)) %>%
          group_modify(~ mean_cl_normal(.$cj2))
        )
      ) %>%
    mutate(stat = "Mean final confidence",
           context = "Advisor agrees",
           target = NA),
  
  # Disagreement
  trials %>% 
    dplyr::filter(agree == F, !is.nan(cj2)) %>%
    group_by(advisorId, participantId) %>%
    summarise(cj2 = mean(abs(cj2), na.rm = T)) %>%
    group_modify(~ mean_cl_normal(.$cj2)) %>%
    ungroup() %>%
    mutate(advisorId = if_else(advisorId == 1, "Bias Sharing", "Anti Bias")) %>%
    rename(condition = advisorId) %>%
    # Combined correct and wrong
    bind_rows(
      cbind(
        tibble(condition = "Both"),
        trials %>%
          group_by(participantId) %>%
          summarise(cj2 = mean(abs(cj2), na.rm = T)) %>%
          group_modify(~ mean_cl_normal(.$cj2))
        )
      ) %>%
    mutate(stat = "Mean final confidence",
           context = "Advisor disagrees",
           target = NA)
) %>%
  num2str.tibble(precision = 1)

tmp <- bind_rows(
  
  # Initial correct by advisor
  trials %>%
    group_by(advisorId, participantId) %>%
    summarise(cor1 = mean(cor1, na.rm = T)) %>%
    group_modify(~ mean_cl_normal(.$cor1)) %>%
    ungroup() %>%
    dplyr::filter(!is.nan(advisorId)) %>%
    mutate(advisorId = if_else(advisorId == 1, "Bias Sharing", "Anti Bias")) %>%
    # Combined average
    bind_rows(
      cbind(
        tibble(advisorId = "Both"),
        trials %>% 
          dplyr::filter(!is.nan(advisorId)) %>%
          group_by(participantId) %>%
          summarise(cor1 = mean(cor1, na.rm = T)) %>%
          group_modify(~ mean_cl_normal(.$cor1))
        )
  ) %>%
    rename(condition = advisorId) %>%
    mutate(stat = "Participant proportion correct",
           context = "Initial estimate",
           target = .71),
  
  # Final correct by advisor
  trials %>%
    group_by(advisorId, participantId) %>%
    summarise(cor2 = mean(cor2, na.rm = T)) %>%
    group_modify(~ mean_cl_normal(.$cor2)) %>%
    ungroup() %>%
    dplyr::filter(!is.nan(advisorId)) %>%
    mutate(advisorId = if_else(advisorId == 1, "Bias Sharing", "Anti Bias")) %>%
    # Combined average
    bind_rows(
      cbind(
        tibble(advisorId = "Both"),
        trials %>% 
          dplyr::filter(!is.nan(advisorId)) %>%
          group_by(participantId) %>%
          summarise(cor2 = mean(cor2, na.rm = T)) %>%
          group_modify(~ mean_cl_normal(.$cor2))
        )
  ) %>%
    rename(condition = advisorId) %>%
    mutate(stat = "Participant proportion correct",
           context = "Final decision",
           target = NA),
  
  # Judge-advisor agreement rate for Bias Sharing
  # By confidence
  trials %>%
    mutate(step = unlist(step)) %>%
    dplyr::filter(advisorId == 1, !is.nan(step)) %>%
    group_by(step, participantId) %>%
    summarise(agree = mean(agree, na.rm = T)) %>%
    group_modify(~ mean_cl_normal(.$agree)) %>%
    ungroup() %>%
    mutate(step = factor(step, labels = c("Low confidence",
                                          "Medium confidence",
                                          "High confidence")),
           step = as.character(step)) %>%
    rename(condition = step) %>%
    # By correctness 
    bind_rows(
      trials %>%
        dplyr::filter(advisorId == 1, !is.nan(cor1)) %>%
        group_by(cor1, participantId) %>%
        summarise(agree = mean(agree, na.rm = T)) %>%
        group_modify(~ mean_cl_normal(.$agree)) %>%
        ungroup() %>%
        mutate(cor1 = if_else(cor1 == 1, "Initial correct", "Initial wrong")) %>%
        rename(condition = cor1)
    ) %>%
    # Combined average
    bind_rows(
      cbind(
        tibble(condition = "All"),
        trials %>% 
          dplyr::filter(advisorId == 1) %>%
          group_by(participantId) %>%
          summarise(agree = mean(agree, na.rm = T)) %>%
          group_modify(~ mean_cl_normal(.$agree))
        )
  ) %>%
    mutate(stat = "Advisor-participant agreement rate",
           context = "Bias Sharing advisor",
           target = c(.8, .7, .6, .7, .3, NA)),
  
  # Judge-advisor agreement rate for Anti Bias
  # By confidence
  trials %>%
    mutate(step = unlist(step)) %>%
    dplyr::filter(advisorId == 2, !is.nan(step)) %>%
    group_by(step, participantId) %>%
    summarise(agree = mean(agree, na.rm = T)) %>%
    group_modify(~ mean_cl_normal(.$agree)) %>%
    ungroup() %>%
    mutate(step = factor(step, labels = c("Low confidence",
                                          "Medium confidence",
                                          "High confidence")),
           step = as.character(step)) %>%
    rename(condition = step) %>%
    # By correctness 
    bind_rows(
      trials %>%
        dplyr::filter(advisorId == 2, !is.nan(cor1)) %>%
        group_by(cor1, participantId) %>%
        summarise(agree = mean(agree, na.rm = T)) %>%
        group_modify(~ mean_cl_normal(.$agree)) %>%
        ungroup() %>%
        mutate(cor1 = if_else(cor1 == 2, "Initial correct", "Initial wrong")) %>%
        rename(condition = cor1)
    ) %>%
    # Combined average
    bind_rows(
      cbind(
        tibble(condition = "All"),
        trials %>% 
          dplyr::filter(advisorId == 2) %>%
          group_by(participantId) %>%
          summarise(agree = mean(agree, na.rm = T)) %>%
          group_modify(~ mean_cl_normal(.$agree))
        )
      ) %>%
    mutate(stat = "Advisor-participant agreement rate",
           context = "Anti Bias advisor",
           target = c(.6, .7, .8, .7, .3, NA))
) %>%
  num2str.tibble(isProportion = T) %>% 
  bind_rows(tmp.conf)

tmp %>%
  mutate_all(~ if_else(is.na(.) | . == "NA", "-", .)) %>%
  mutate_at(vars(-(stat:context), -target), 
            ~ if_else(tmp$condition %in% c("Both", "All"), 
                      cell_spec(., bold = T), .)) %>%
  mutate(target = cell_spec(target, 
                            align = if_else(target == "-", 'c', 'r'),
                            bold = if_else(tmp$condition %in% c("Both", "All"),
                                           T, F))) %>%
  select(stat, context, condition, target, everything()) %>%
  rename(` ` = stat, `  ` = context, `   ` = condition,
         Target = target, Mean = y, Low = ymin, High = ymax) %>%
  kable(caption = "Table \\label{tab:ex1-descriptives}: Descriptive statistics for Experiment 1",
        align = c('l', 'l', 'l', 'r', 'r', 'r', 'r'), 
        escape = F) %>%
  column_spec(1:2, bold = T, width = "3.25cm") %>%
  collapse_rows(columns = 1:2, valign = "top") %>%
  add_header_above(c(" " = 5, "95% CI" = 2)) %>%
  kable_styling()

```

The descriptive statistics demonstrate the contingent agreement of the advisors, with the Bias Sharing and Anti Bias advisors both agreeing at close to the target rate for most participants. While the ranges overall patterns are as designed, the variation in individual experience means some participants may have experienced by chance an advisor profile which contradicted the generative pattern (e.g. a Bias Sharing advisor who agreed on fewer high-confidence trials than mid-confidence trials). This is especially likely to be true of contingencies with fewer trials per participant, such as the incorrect trials. Overall, however, both the mean and 95% confidence intervals suggest the pattern was as desired for most participants most of the time.

Participants’ revisions to their confidence were mostly in the direction of the advice, symmetrical across left and right responses, and usually relatively small, especially in agreement trials (Figure \@ref(fig:ex1-confidence)).

```{r ex1-confidence, fig.align='center', fig.cap="Initial vs final confidence: Influence of the advisors is evident in the deviation from the dashed y = x line. Points lying below the line indicate a more leftward response from initial to final judgement. Points above the line indicate a more rightward response in the final judgement. The further away from the y = x line, the greater the change from initial to final judgement. Separate plots show agreement vs disagreement trials (between the advisor and judge), and separate colours indicate whether the judge's final decision was correct or incorrect.\\\n The shaded area indicates the boundary for the symmetrical influence measure. Points outside this area are truncated by moving them vertically until they meet the grey area.", out.width="100%"}

graph.confidence +
  labs(title = NULL, subtitle = NULL) +
  scale_y_continuous(limits = c(-60, 60), expand = c(0, 0)) +
  scale_x_continuous(limits = c(-60, 60), expand = c(0, 0))

```

#### Advisor selection

We hypothesised that the participants would display different pick rates for the Bias Sharing advisor versus the Anti Bias advisor. This hypothesis was evaluated by calculating the proportion of choice trials on which each participant picked the Bias Sharing advisor, and then testing these values as a one-sample t-test against the null hypothesis that the pick rates would be 0.5. No support was found for this hypothesis (`r cat(md.ttest(participants$aicPickRate, mu = .5, labels = "M~P(BiasSharing)~"))`; Figure \@ref(fig:ex1-selection)), although the Bayesian test indicated that the data were not sufficient to conclude that no effect was present. There was considerable variability across participants in the overall pick rate for the Bias Sharing advisor (range = [`r range(participants$aicPickRate) %>% num2str(isProportion = T) %>% paste(collapse = ", ")`]).

```{r ex1-selection, fig.align='center', fig.cap="Advisor selection: Proportion of the time each participant picked the Bias Sharing advisor. Faint lines and dots indicate data from individual participants, while the large dot indicates the mean proportion across all participants. The dashed reference line indicates picking both advisors equally, as would be expected by chance. Error bars give 95\\% confidence intervals.", out.width="100%"}

melt(participants[,c("participantId","aicPickRate.lowConf","aicPickRate.medConf","aicPickRate.highConf")],
            id.vars = 'participantId') %>%
  mutate(variable = recode(variable, 
                           "aicPickRate.lowConf" = 'Low', 
                           "aicPickRate.medConf" = 'Medium', 
                           "aicPickRate.highConf" = 'High'),
         participantId = as.factor(participantId)) %>%
  ggplot(aes(x = variable, y = value)) +
  geom_hline(linetype = "dashed", color = "black", yintercept = .5, size = 1) +
  # Confidence categories
  geom_line(aes(group = factor(participantId)), alpha = 0.25) +
  stat_summary(geom = "errorbar", fun.data = "mean_cl_boot", width = 0) +
  stat_summary(geom = "point", fun.y = "mean", fill = "black", size = 4) +
  # Overall
  geom_violin(data = participants, fill = "lightgrey", color = NA, alpha = 0.25, 
              aes(x = "Overall", y = aicPickRate), width = 1.75) +
  geom_point(position = position_nudge(x = .03), alpha = .25,
             aes(x = "Overall", y = aicPickRate), data = participants) +
  stat_summary(geom = "errorbar", fun.data = mean_cl_normal, width = 0,
               aes(x = "Overall", y = aicPickRate), data = participants,
               position = position_nudge(x = -.03)) +
  stat_summary(geom = "point", fun.y = mean, fill = "black", 
               size = 4, data = participants,
               aes(x = "Overall", y = aicPickRate),
               position = position_nudge(x = -.03)) +
  scale_y_continuous(limits = c(0,1), expand = c(0,0)) +
  scale_x_discrete(expand = c(0,0.5), limits = c('Low', 'Medium',
                                               'High', 'Overall'),
                   labels = c("Low", "Medium", "High", "All")) +
  theme(panel.border = element_blank()) +
  labs(x = "Confidence category",
       y = "P(Bias Sharing selected)") 

```

##### Advisor selection on medium-confidence trials

The advisors differed in their advice-giving as a function of the judge’s initial confidence. In trials where the judge’s initial decision was made with medium confidence, however, the advisors were equal on judge confidence and agreement rate. Comparing selection rates for these trials alone revealed a clear preference for the Bias Sharing advisor (`r cat(md.ttest(participants$aicPickRate.medConf, mu = .5, labels = "M~P(BiasSharing)~"))`; Figure \@ref(fig:ex1-selection) "Medium" confidence category), although the Bayesian analysis again indicated an insensitive result, albeit in the hypothesised direction.

#### Advisor influence

Previous work in our lab demonstrated that the agree-in-confidence advisor exerted greater influence on the judges’ final decisions than the agree-in-uncertainty advisor [@pescetelliUseMetacognitiveSignals2017]. Influence was examined with a 2x2x2 (Bias Sharing versus Anti Bias advisor; choice versus forced trials; agreement versus disagreement trials) ANOVA (Figure \@ref(fig:ex1-influence)). No main effect was found for advisor !TODO[stats], meaning that the previous finding was not replicated. As shown in Table \@ref(tab:ex1-influence), the only statistically significant effect was the main effect of agreement, with disagreement producing higher influence than agreement !TODO[marginal means].

```{r ex1-influence, fig.align='center', fig.cap="Advisor influence: Influence of advice from each advisor by advisor, agreement, and trial type. Faint lines and indicate data from individual participants, while the dots indicate the mean proportion across all participants. Error bars give 95\\% confidence intervals. Note: vertical axis is truncated to show group differences more clearly, the theoretical maximum influence given the scale is 110. The minimum is 0 as shown.", out.width="100%"}

trials %>% 
  select(participantId, advisorId, cor1, choice, agree, influence) %>%
  filter(!is.nan(cor1), !is.nan(influence), !is.nan(advisorId)) %>%
  mutate(advisor = if_else(advisorId == 1, "Bias Sharing", "Anti Bias"),
         choice = sapply(choice, sum) == 3,
         agree = if_else(agree == 0, "Disagree", "Agree"),
         influence = abs(influence),
         participantId = factor(participantId)) %>%
  group_by(participantId, advisor, choice, agree) %>%
  summarise_if(is.numeric, mean) %>%
  mutate(g = paste(participantId, advisor, sep = ":")) %>%
  ggplot(aes(x = agree, y = influence, colour = advisor, group = g)) +
  geom_line(alpha = 0.25) +
  stat_summary(aes(group = advisor), geom = "line", fun.y = mean,
               position = position_dodge(width = .25), size = 1) + 
  stat_summary(aes(group = advisor), geom = "errorbar", fun.data = mean_cl_normal, 
               width = 0, size = 1, position = position_dodge(width = .25)) +
  stat_summary(aes(group = advisor), geom = "point", fun.y = mean, 
               size = 4, position = position_dodge(width = .25)) +
  scale_y_continuous(limits = c(0, NA)) +
  facet_grid(~choice, labeller = label_both) +
  labs(x = "Agreement between advisor and participant",
       y = "Influence of the advice") 

```

```{r ex1-influence-anova} 
kableANOVA(anova.influence$ANOVA, 
           caption = "Table \\label{tab:ex1-influence}: ANOVA of Advisor influence in Experiment 1")
```

##### Advisor influence on medium confidence trials

The agree-in-confidence and agree-in-uncertainty advisors differed by design in the frequency with which they agree with the participant as a function of the participant’s confidence in their initial estimate. To control for the effects of initial confidence on influence, the above analysis was repeated using only those trials on which the initial estimate was correct and given with medium confidence. Two participants were missing data in this analysis: one participant had zero mid-confidence choice trials in which the agree-in-confidence advisor disagreed with them; and the other had zero mid-confidence choice trials in which the agree-in-uncertainty advisor disagreed with them. These participants were removed from this analysis. As before, the only effect which was statistically significant was agreement. 

The low number of trials in some of the intersections meant the some participants had to be dropped. The analysis was rerun with forced trials only, !TODO[stats] 

```{r ex1-influence-medconf-anova}
dropIds <- participants.influence.medConf %>%
  filter(is.nan(value)) %>% 
  pull(participantId)

tmp <- participants.influence.medConf %>% 
  filter(!(participantId %in% dropIds)) %>%
  mutate(participantId = factor(participantId))

tmp.aov <- ezANOVA(data = tmp,
                   dv = value, 
                   wid = participantId,
                   within = c('AiC', 'agree', 'hasChoice'),
                   return_aov = T)

tmp.aov$ANOVA %>%
  kableANOVA(caption = "Table \\label{tab:ex1-influence-medconf}: ANOVA of Advisor influence on medium confidence trials in Experiment 1")
```

#### Subjective assessment of advisors

Participants answered questionnaires about their trust in the advisors at four time points during the experiment. We hypothesised that this subjective trust measure would change over the course of the experiment, with the agree-in-confidence advisor becoming more preferred over time. As indicated by Table \@ref{tab:ex1-subjective-assessment}, however, no such effects were found: subjective assessments of the advisors did not differ post-experiment.

```{r ex1-subjective-assessment}

tmp <- questionnaires %>% 
  mutate_if(is.list, unlist) %>%
  filter(timePoint %in% range(timePoint)) %>%
  group_by(questionTextShort, adviceType, timePoint, questionText) 
  
x <- tmp %>%
  do(
    y = mean_cl_normal(.$answer)
  ) %>%
  unnest(y)

tmp <- tmp %>%
  group_by(questionTextShort, timePoint) %>% 
  group_nest() %>%
  mutate(
    t = map(data, ~ t.test(answer ~ adviceType, data = .x, paired = T)),
    b = map(data, ~ ttestBF(formula = answer ~ adviceType, 
                            data = as.data.frame(.x))),
    test = map(t, ~ tidy(.x)),
    bf = map(b, ~ exp(.x@bayesFactor$bf))
  ) %>%
  unnest(test, bf) %>%
  select(-data, -t) %>%
  left_join(x, by = c('questionTextShort', 'timePoint'))
  
tmp <- tmp %>%
  filter(adviceType == min(adviceType)) %>%
  rename(y.bs = y, ymin.bs = ymin, ymax.bs = ymax) %>%
  select(-adviceType) %>%
  left_join(tmp %>% filter(adviceType != min(adviceType)),
            by = c("questionTextShort", "timePoint", "estimate", "bf",
                   "statistic", "p.value", "parameter", "conf.low", 
                   "conf.high", "method", "alternative", "questionText"))

# Print table 
tmp.print <- tmp %>%
  mutate(timePoint = factor(timePoint, labels = c("Pre-experiment",
                                                  "Post-experiment")),
         sig = p.value < .05,
         questionText = str_replace(questionText,
                                    '(accurate|like|influenced|trustworthy)',
                                    '\\\\textbf{\\1}')) %>%
  select(timePoint, questionText, y.bs, ymin.bs, ymax.bs, 
         y, ymin, ymax, bf, estimate, p.value, sig) %>%
  mutate(
    p.value = num2str(p.value, precision = 3, isProportion = T),
    bf = num2str(bf),
    sig = if_else(sig == T, "$*$", "")
  ) %>%
  mutate_if(is.numeric, num2str, precision = 1) %>%
  rename(
    ` ` = timePoint,
    Question = questionText,
    Mean = y.bs, Low = ymin.bs, High = ymax.bs,
    ` Mean ` = y, ` Low ` = ymin, ` High ` = ymax,
    BF = bf, `$t$` = estimate, `$p$` = p.value, `  ` = sig
  )

names(tmp.print)[str_detect(names(tmp.print), '$t$')] <- 
  paste0('$t$(', tmp$parameter[1], ')')
names(tmp.print)[str_detect(names(tmp.print), 'Question')] <-
  paste0('Question', footnote_marker_alphabet(1))


tmp.print %>%
  arrange(` `) %>%
  mutate(
    ` ` = cell_spec(` `, angle = 90)
  ) %>%
  kable(caption = "Table \\label{tab:ex1-subjective-assessment-t}: Questionnaire responses pre- and post-experiment",
        align = c('l', 'l', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'c'),
        escape = F) %>%
  collapse_rows(columns = 1, valign = 'top') %>%
  column_spec(2, width = "7em") %>%
  add_header_above(c(" " = 3, "95% CI" = 2, " " = 1, "95% CI" = 2, " " = 4)) %>%
  add_header_above(c(" " = 2, "Bias sharing" = 3, "Anti bias" = 3, " " = 4)) %>%
  footnote(alphabet_title = "", alphabet = "Emphasis added.")
  
```

### Discussion {#ex1-d}

## Online experiments {#ex-online}

The remaining experiments in this thesis were conducted online. The general methodology and background information is detailed here, and deviations from it for specific experiments are highlighted in the relevant sections for the reports of each of the individual experiments. 

### Open science {#ex-online-os}

#### Preregistration {#ex-online-os-prereg}

#### Open materials {#ex-online-os-materials}

#### Open data {#ex-online-os-data}

### Method {#ex-online-m}

#### Participant recruitment {#ex-online-m-participants}

#### Ethics {#ex-online-m-ethics}

#### Procedure {#ex-online-m-procedure}

#### Analysis {#ex-online-m-analysis}

## Experiment 2: Metacognitive-contingent advisors {#ex2}

### Method {#ex2-m}

### Result {#ex2-r}

### Discussion {#ex2-d}

## Experiment 3: Advisor accuracy {#ex3}

### Method {#ex3-m}

### Result {#ex3-r}

### Discussion {#ex3-d}

## Experiment 4: Advisor agreement {#ex4}

### Method {#ex4-m}

### Result {#ex4-r}

### Discussion {#ex4-d}

## Experiment 5: Accuracy vs agreement (Date estimation) {#ex5}

### Method {#ex5-m}

### Result {#ex5-r}

### Discussion {#ex5-d}

## General discussion